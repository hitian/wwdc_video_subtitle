1
00:00:17,417 --> 0:00:22,122
（ARKit 2新特性

2
00:00:26,527 --> 0:00:27,961
早上好

3
00:00:29,930 --> 0:00:32,698
欢迎参加我们的演讲

4
00:00:34,768 --> 0:00:37,838
我叫Arsalan

5
00:00:41,074 --> 0:00:46,046
去年 我们非常高兴能将ARKit

6
00:00:47,314 --> 0:00:49,149
作为iOS 11更新

7
00:00:51,118 --> 0:00:55,556
ARKit已部署到数亿台设备

8
00:00:56,023 --> 0:00:59,893
使iOS成为最大

9
00:01:03,230 --> 0:01:07,534
ARKit为你提供了强大功能集的

10
00:01:10,103 --> 0:01:13,740
我们对你们到目前为止

11
00:01:13,807 --> 0:01:14,975
感到非常惊讶

12
00:01:15,409 --> 0:01:18,011
让我们看看App Store中的

13
00:01:20,214 --> 0:01:25,686
Civilizations是一款AR app

14
00:01:26,253 --> 0:01:28,689
你可以从各个角度查看它们

15
00:01:28,755 --> 0:01:33,861
你还可以启用X射线模式

16
00:01:35,128 --> 0:01:37,965
你可以把它们放到你的后院

17
00:01:39,333 --> 0:01:41,001
你甚至可以将它们放到现实背景中

18
00:01:41,068 --> 0:01:43,904
就跟几百年前它们的样子一样

19
00:01:45,005 --> 0:01:48,141
因此这是一个浏览历史文物的好工具

20
00:01:50,811 --> 0:01:55,916
Boulevard AR是一款AR app

21
00:01:55,983 --> 0:01:58,318
浏览艺术作品

22
00:01:58,685 --> 0:02:01,255
你可以将它们放在地上或墙上

23
00:01:58,685 --> 0:02:01,255
你可以将它们放在地上或墙上

24
00:02:01,321 --> 0:02:05,459
你可以靠近它们来查看所有的细节

25
00:02:06,026 --> 0:02:09,795
这是一个讲述艺术故事的好方法

26
00:02:16,336 --> 0:02:19,439
ARKit也是一种

27
00:02:20,007 --> 0:02:24,878
Free Reverse是一款

28
00:02:25,546 --> 0:02:29,082
你可以跟随河流穿过整个地貌

29
00:02:29,149 --> 0:02:31,485
并看到群体和野生动物

30
00:02:32,019 --> 0:02:34,121
你可以看到人类活动

31
00:02:34,955 --> 0:02:40,360
如何通过建筑对这些群体

32
00:02:42,296 --> 0:02:44,431
所以这是教育每个人

33
00:02:44,498 --> 0:02:49,102
关于环保和可持续发展的好方法

34
00:02:50,103 --> 0:02:52,139
以上就是一些这样的例子

35
00:02:52,206 --> 0:02:55,108
你可以在App Store中

36
00:02:57,744 --> 0:03:00,280
你们中有些人可能刚接触ARKit

37
00:02:57,744 --> 0:03:00,280
你们中有些人可能刚接触ARKit

38
00:03:00,347 --> 0:03:03,717
所以让我简单概述一下

39
00:03:08,555 --> 0:03:11,124
跟踪是ARKit的核心组件

40
00:03:11,525 --> 0:03:15,562
它为你提供设备在真实世界中的

41
00:03:18,999 --> 0:03:23,303
它还可以跟踪对象

42
00:03:26,139 --> 0:03:27,541
场景理解

43
00:03:28,442 --> 0:03:31,778
通过获取有关环境的更多属性

44
00:03:33,046 --> 0:03:38,819
我们可以检测水平面

45
00:03:39,720 --> 0:03:41,688
我们还可以检测垂直平面

46
00:03:41,755 --> 0:03:44,925
这让你可以将虚拟对象放置在场景中

47
00:03:51,098 --> 0:03:56,770
场景理解还可以

48
00:03:57,905 --> 0:04:02,576
因此你可以使用光照在虚拟场景中

49
00:03:57,905 --> 0:04:02,576
因此你可以使用光照在虚拟场景中

50
00:04:02,643 --> 0:04:04,044
准确反映真实环境

51
00:04:04,111 --> 0:04:07,314
从而使你的物体看起来

52
00:04:10,017 --> 0:04:13,453
渲染是用户实际在设备上看到的内容

53
00:04:13,520 --> 0:04:15,222
并与增强现实场景进行交互的过程

54
00:04:16,356 --> 0:04:19,358
ARKit API让你能够轻松

55
00:04:19,426 --> 0:04:22,329
集成你选择的任何渲染引擎

56
00:04:26,900 --> 0:04:30,871
ARKit为SceneKit

57
00:04:32,606 --> 0:04:35,909
在Xcode中

58
00:04:35,976 --> 0:04:39,947
来让你快速开始自己的增强现实体验

59
00:04:41,615 --> 0:04:47,187
注意 Unity和Unreal

60
00:04:47,254 --> 0:04:48,789
集成到它们热门的游戏引擎中

61
00:04:49,389 --> 0:04:52,492
因此你可以通过使用

62
00:04:52,926 --> 0:04:55,062
来开始使用ARKit

63
00:04:58,999 --> 0:05:01,768
让我们看看

64
00:04:58,999 --> 0:05:01,768
让我们看看

65
00:05:07,875 --> 0:05:10,210
我们现在可以保存和加载映射

66
00:05:10,277 --> 0:05:16,350
它支持着持久性和多用户体验的

67
00:05:19,753 --> 0:05:22,289
我们也为你提供环境纹理

68
00:05:22,356 --> 0:05:25,559
以便你可以逼真地渲染增强现实场景

69
00:05:27,728 --> 0:05:30,998
ARKit现在可以

70
00:05:33,400 --> 0:05:37,604
我们不仅限于2D

71
00:05:41,508 --> 0:05:44,978
最后 我们为面部跟踪提供了一些

72
00:05:48,215 --> 0:05:50,851
让我们从保存和加载映射开始吧

73
00:05:56,590 --> 0:05:59,860
保存和加载映射是世界跟踪的一部分

74
00:06:00,327 --> 0:06:03,931
世界跟踪可为你提供现实世界中

75
00:06:04,631 --> 0:06:07,701
设备的六自由度位置和方向

76
00:06:08,135 --> 0:06:11,972
这可以让你在场景中放置对象

77
00:06:12,639 --> 0:06:16,376
比如你在此视频中看到的这些桌椅

78
00:06:18,645 --> 0:06:22,549
世界跟踪还可以为你提供

79
00:06:22,616 --> 0:06:25,652
以便你可以正确的比例放置对象

80
00:06:26,119 --> 0:06:29,122
因此你的对象看起来不会太大或太小

81
00:06:34,061 --> 0:06:39,233
这也可用于实现精确测量

82
00:06:39,299 --> 0:06:41,335
如昨天看到的

83
00:06:44,605 --> 0:06:49,443
世界跟踪还为你提供3D特征点

84
00:06:49,510 --> 0:06:51,712
以便你了解环境的一些物理结构

85
00:06:52,012 --> 0:06:56,617
这可用于在场景中放置对象时

86
00:07:00,387 --> 0:07:04,024
在iOS 11.3中

87
00:07:04,725 --> 0:07:08,228
此功能可让你在AR会话中断后

88
00:07:08,295 --> 0:07:11,465
恢复跟踪状态

89
00:07:12,866 --> 0:07:15,469
这种情况可能发生在

90
00:07:16,003 --> 0:07:19,139
或者你在iPad的图片模式上

91
00:07:24,044 --> 0:07:30,017
重定位可以与

92
00:07:30,384 --> 0:07:32,686
一起使用

93
00:07:33,954 --> 0:07:37,824
我们越是绕着环境进行移动

94
00:07:38,392 --> 0:07:40,561
它就越能够扩展并学习越多

95
00:07:40,627 --> 0:07:43,430
关于环境的不同特征

96
00:07:48,135 --> 0:07:53,774
该映射过去只有在你的AR会话

97
00:07:54,875 --> 0:07:57,945
但现在我们为你提供此映射

98
00:08:00,214 --> 0:08:05,519
ARKit API中 此映射作为

99
00:08:14,228 --> 0:08:17,865
ARWorldMap

100
00:08:18,498 --> 0:08:23,637
类似于我们在右侧视图中看到的那样

101
00:08:25,873 --> 0:08:29,610
我们也知道锚是物理空间中的重要点

102
00:08:30,210 --> 0:08:33,947
它们是你想要放置虚拟对象的位置

103
00:08:34,481 --> 0:08:39,419
我们在ARWorldMap中

104
00:08:40,988 --> 0:08:45,092
此外你还可以将自定义锚

105
00:08:45,158 --> 0:08:46,793
因为这是一个可变列表

106
00:08:47,160 --> 0:08:51,431
你可以在场景中创建自定义锚

107
00:08:57,504 --> 0:09:03,143
对可视化和调试 WorldMap

108
00:08:57,504 --> 0:09:03,143
对可视化和调试 WorldMap

109
00:09:03,210 --> 0:09:09,283
以便你了解刚扫描的真实物理空间

110
00:09:12,753 --> 0:09:16,690
更重要的是 WorldMap

111
00:09:17,291 --> 0:09:21,195
因此它可以序列化为

112
00:09:21,261 --> 0:09:26,600
例如本地文件系统上的文件

113
00:09:29,536 --> 0:09:31,972
这个ARWorldMap对象

114
00:09:32,239 --> 0:09:37,344
支持着ARKit中的

115
00:09:39,546 --> 0:09:41,849
首先是持久性

116
00:09:44,284 --> 0:09:46,787
这是向你展示它是如何工作的

117
00:09:48,288 --> 0:09:54,127
我们有一个用户开始进行世界跟踪

118
00:09:54,494 --> 0:09:56,296
将一个对象放在场景中

119
00:09:57,564 --> 0:10:04,104
在离开场景之前

120
00:09:57,564 --> 0:10:04,104
在离开场景之前

121
00:10:08,876 --> 0:10:12,346
过了一段时间

122
00:10:12,913 --> 0:10:14,314
该用户回来了

123
00:10:15,048 --> 0:10:18,051
他可以加载相同的WorldMap

124
00:10:18,452 --> 0:10:22,222
并找回相同的增强现实体验

125
00:10:22,623 --> 0:10:27,394
他可以多次重复这种体验

126
00:10:27,461 --> 0:10:31,198
每次开始体验时

127
00:10:31,398 --> 0:10:34,968
这就是世界跟踪的持久性

128
00:10:37,771 --> 0:10:38,605
谢谢

129
00:10:44,311 --> 0:10:47,614
ARWorldMap

130
00:10:49,249 --> 0:10:51,251
现在你的增强现实体验

131
00:10:51,318 --> 0:10:53,887
不仅限于单个设备或单个用户

132
00:10:54,888 --> 0:10:57,524
它可以与许多用户共享

133
00:11:00,294 --> 0:11:06,800
一个用户可以创建WorldMap

134
00:11:08,335 --> 0:11:14,474
请注意 WorldMap代表

135
00:11:14,942 --> 0:11:19,646
这意味着所有用户将共享

136
00:11:20,347 --> 0:11:23,917
他们能够从不同的角度体验到

137
00:11:23,984 --> 0:11:25,519
相同的增强现实体验

138
00:11:27,621 --> 0:11:29,256
这是一个很棒的特性

139
00:11:29,723 --> 0:11:32,593
你可以使用WorldMap

140
00:11:32,659 --> 0:11:37,497
实现多用户游戏

141
00:11:39,800 --> 0:11:45,906
我们还可使用ARWorldMap

142
00:11:50,143 --> 0:11:55,382
注意我们现将ARWorldMap

143
00:11:55,449 --> 0:12:00,587
因此你可以自由选择

144
00:11:55,449 --> 0:12:00,587
因此你可以自由选择

145
00:12:02,422 --> 0:12:07,227
比如对于共享 你可以使用

146
00:12:07,294 --> 0:12:10,664
它们依赖于本地蓝牙

147
00:12:11,999 --> 0:12:14,968
这意味着你不需要联网

148
00:12:15,035 --> 0:12:16,537
就可以使用此功能

149
00:12:22,643 --> 0:12:25,312
让我们看看

150
00:12:25,379 --> 0:12:28,148
轻松实现获取

151
00:12:31,018 --> 0:12:32,653
在你的AR会话对象上

152
00:12:33,320 --> 0:12:36,290
你可以在任何时间点

153
00:12:36,356 --> 0:12:38,091
调用getCurrentWorldMap

154
00:12:39,793 --> 0:12:42,229
此方法带有一个完成处理程序

155
00:12:42,729 --> 0:12:46,166
它将返回一个

156
00:12:48,869 --> 0:12:51,738
另外注意它也可能返回错误

157
00:12:51,805 --> 0:12:53,941
如果WorldMap不可用的话

158
00:12:54,174 --> 0:12:57,611
因此在app代码中

159
00:12:59,746 --> 0:13:01,715
一旦你得到了ARWorldMap

160
00:12:59,746 --> 0:13:01,715
一旦你得到了ARWorldMap

161
00:13:02,916 --> 0:13:04,084
你可以简单地

162
00:13:07,554 --> 0:13:11,458
在世界跟踪配置中设置

163
00:13:12,025 --> 0:13:14,127
并运行你的会话

164
00:13:15,996 --> 0:13:18,732
注意这也可以动态更改

165
00:13:18,799 --> 0:13:23,203
因此你始终可以通过运行新配置

166
00:13:25,539 --> 0:13:28,876
一旦用ARWorldMap

167
00:13:30,143 --> 0:13:32,579
它将遵循与iOS 11.3中

168
00:13:32,980 --> 0:13:36,483
完全相同的行为

169
00:13:44,558 --> 0:13:50,831
重定位的可靠性

170
00:13:52,466 --> 0:13:55,869
因此获取好的WorldMap

171
00:13:56,303 --> 0:13:58,572
注意你可以随时调用

172
00:13:58,639 --> 0:14:00,274
getCurrentWorldMap

173
00:13:58,639 --> 0:14:00,274
getCurrentWorldMap

174
00:14:02,442 --> 0:14:06,213
从多个不同视角扫描你的物理空间

175
00:14:06,747 --> 0:14:08,482
非常重要

176
00:14:08,549 --> 0:14:11,385
这样跟踪系统才可以真正理解

177
00:14:11,451 --> 0:14:13,587
周围环境的物理结构

178
00:14:16,290 --> 0:14:21,428
环境应该是静态的并且纹理清晰

179
00:14:21,495 --> 0:14:24,464
并了解有关环境的更多信息

180
00:14:27,835 --> 0:14:32,039
而且在映射上有密集的特征点很重要

181
00:14:32,206 --> 0:14:35,075
这样它就可以可靠地重定位

182
00:14:37,344 --> 0:14:40,480
但你不必担心所有这些问题

183
00:14:41,582 --> 0:14:45,552
在ARKit中 其API通过向你提供

184
00:14:45,619 --> 0:14:48,422
来大大简化你的工作

185
00:14:49,323 --> 0:14:54,194
WorldMappingStatus在每个

186
00:14:54,261 --> 0:14:56,463
并且可以通过

187
00:14:57,064 --> 0:14:58,599
让我们看看这是如何工作的

188
00:15:01,568 --> 0:15:04,938
当我们刚开始世界跟踪时

189
00:15:05,005 --> 0:15:07,407
当我们开始扫描物理空间时

190
00:15:07,474 --> 0:15:08,909
它会变为受限制

191
00:15:10,577 --> 0:15:13,447
我们在真实世界中移动得越多

192
00:15:14,014 --> 0:15:16,750
世界跟踪将继续扩展该映射

193
00:15:19,553 --> 0:15:24,291
如果我们从当前的角度

194
00:15:24,358 --> 0:15:26,360
WorldMappingStatus将变为已映射

195
00:15:34,568 --> 0:15:39,306
注意如果你从已映射的物理空间移开

196
00:15:39,873 --> 0:15:42,609
WorldMappingStatus可能会

197
00:15:43,076 --> 0:15:44,711
并将开始学习更多

198
00:15:44,778 --> 0:15:48,382
关于我们所看到的新环境的信息

199
00:15:51,285 --> 0:15:54,388
那么如何在你的app代码中

200
00:15:56,423 --> 0:16:01,028
假设你有一个app可以让你

201
00:15:56,423 --> 0:16:01,028
假设你有一个app可以让你

202
00:16:01,762 --> 0:16:05,432
并且你的用户界面上有一个

203
00:16:07,401 --> 0:16:10,637
当WorldMappingStatus

204
00:16:10,704 --> 0:16:12,773
禁用此按钮是一种好实践

205
00:16:14,474 --> 0:16:17,010
当WorldMappingStatus为扩展时

206
00:16:18,212 --> 0:16:21,481
你可能希望在UI上显示活动指示器

207
00:16:21,882 --> 0:16:26,486
这会鼓励你的终端用户

208
00:16:26,553 --> 0:16:29,857
并继续扫描以扩展映射

209
00:16:30,457 --> 0:16:32,426
因为你需要它来进行重定位

210
00:16:36,530 --> 0:16:39,233
一旦WorldMappingStatus

211
00:16:40,234 --> 0:16:45,472
你就可以启用共享映射按钮

212
00:16:45,939 --> 0:16:48,575
这将让你的用户可以分享该映射

213
00:16:53,614 --> 0:16:56,717
让我们来看一个

214
00:17:06,627 --> 0:17:07,493
好的

215
00:17:09,229 --> 0:17:11,164
我们可以切换到AR 1吗？

216
00:17:13,200 --> 0:17:14,034
好的

217
00:17:14,601 --> 0:17:17,171
对于这个演示 我有两个app

218
00:17:17,570 --> 0:17:22,709
第一个app中 我将获取

219
00:17:23,277 --> 0:17:28,682
第二个app中 我将加载

220
00:17:28,749 --> 0:17:31,418
同样的增强现实体验

221
00:17:31,485 --> 0:17:32,386
让我们开始吧

222
00:17:35,856 --> 0:17:39,726
如你所见 WorldMappingStatus

223
00:17:39,793 --> 0:17:41,261
它为不可用

224
00:17:41,728 --> 0:17:44,531
一旦我开始在环境中移动

225
00:17:45,032 --> 0:17:48,068
它就开始扩展我的WorldMap

226
00:17:48,368 --> 0:17:52,005
如果我继续在该环境中移动

227
00:17:53,207 --> 0:17:56,243
WorldMappingStatus

228
00:17:57,077 --> 0:18:01,815
这意味着它已经从这个角度

229
00:17:57,077 --> 0:18:01,815
这意味着它已经从这个角度

230
00:18:02,049 --> 0:18:03,617
来进行重定位

231
00:18:03,684 --> 0:18:08,488
现在是获取和序列化

232
00:18:10,190 --> 0:18:14,528
但先让我们放置一个自定义锚

233
00:18:14,595 --> 0:18:16,296
来让这个增强现实场景更有趣

234
00:18:17,064 --> 0:18:20,767
通过碰撞测试

235
00:18:21,168 --> 0:18:25,172
并将这个对象叠加上去

236
00:18:25,239 --> 0:18:27,307
这是一台旧电视

237
00:18:27,808 --> 0:18:30,244
我想你们大多数人以前可能见过它

238
00:18:33,413 --> 0:18:37,217
当然 我仍然可以继续映射世界

239
00:18:37,784 --> 0:18:40,387
让我们保存该WorldMap

240
00:18:41,221 --> 0:18:43,490
当我保存WorldMap时

241
00:18:43,924 --> 0:18:47,694
它还可以显示属于

242
00:18:47,761 --> 0:18:52,466
你看到的那些蓝点

243
00:18:55,402 --> 0:18:57,237
另外作为一个好实践

244
00:18:58,639 --> 0:19:03,977
在保存WorldMap时

245
00:18:58,639 --> 0:19:03,977
在保存WorldMap时

246
00:19:07,347 --> 0:19:11,318
现在我们已将WorldMap

247
00:19:12,119 --> 0:19:15,189
我们现在可以在另一个app中

248
00:19:15,255 --> 0:19:16,456
恢复这个增强现实体验

249
00:19:16,924 --> 0:19:18,225
让我们试试吧

250
00:19:18,859 --> 0:19:21,495
我将从一个不同的位置启动此app

251
00:19:24,298 --> 0:19:28,936
你可以看到这是我的世界原点

252
00:19:29,469 --> 0:19:32,606
并且我的世界跟踪

253
00:19:33,140 --> 0:19:35,676
这是与我们在iOS 11.3中

254
00:19:35,742 --> 0:19:37,611
开放重定位行为

255
00:19:38,512 --> 0:19:44,852
让我把设备指向我刚才

256
00:19:44,918 --> 0:19:47,054
创建WorldMap的物理位置

257
00:19:48,722 --> 0:19:53,627
当我指向该空间时

258
00:19:54,428 --> 0:19:55,996
恢复到原来的位置

259
00:19:56,496 --> 0:20:00,868
同时它还恢复了我的自定义锚

260
00:19:56,496 --> 0:20:00,868
同时它还恢复了我的自定义锚

261
00:20:00,934 --> 0:20:03,237
因此我得到了完全相同的AR体验

262
00:20:09,543 --> 0:20:10,444
谢谢

263
00:20:11,445 --> 0:20:15,449
请注意我可以多次启动此app

264
00:20:15,682 --> 0:20:18,986
并且每次启动时它都会

265
00:20:19,453 --> 0:20:20,888
这就是持久性

266
00:20:22,389 --> 0:20:25,392
当然 这也可以与其它设备共享

267
00:20:27,427 --> 0:20:28,929
回到幻灯片

268
00:20:34,501 --> 0:20:37,304
以上就是保存和加载映射

269
00:20:37,938 --> 0:20:40,741
这是ARKit 2中强大的新功能

270
00:20:40,807 --> 0:20:45,879
即支持持久性和多用户共享体验

271
00:20:49,716 --> 0:20:53,921
在ARKit 2中 我们有更快的

272
00:20:56,857 --> 0:20:59,159
世界跟踪现在更加鲁棒

273
00:20:59,226 --> 0:21:02,329
我们可以在更复杂的环境中检测平面

274
00:20:59,226 --> 0:21:02,329
我们可以在更复杂的环境中检测平面

275
00:21:07,668 --> 0:21:12,206
水平和垂直平面

276
00:21:12,673 --> 0:21:16,009
这意味着你可以准确地将对象

277
00:21:20,214 --> 0:21:24,551
在iOS 11.3中

278
00:21:24,618 --> 0:21:26,920
引入了连续自动对焦功能

279
00:21:28,589 --> 0:21:32,593
iOS 12专门针对增强现实体验

280
00:21:32,659 --> 0:21:35,362
进行了更多优化

281
00:21:39,066 --> 0:21:44,304
我们还在ARKit中引入了

282
00:21:47,140 --> 0:21:53,013
4:3是一种广角视频格式

283
00:21:53,080 --> 0:21:58,018
因为iPad也有4:3的

284
00:21:59,720 --> 0:22:04,558
请注意4:3视频格式将是

285
00:21:59,720 --> 0:22:04,558
请注意4:3视频格式将是

286
00:22:06,760 --> 0:22:11,698
所有这些增强特性都将应用于

287
00:22:11,765 --> 0:22:15,369
但4:3视频格式除外

288
00:22:15,435 --> 0:22:19,640
为此你必须使用新的SDK

289
00:22:23,310 --> 0:22:28,315
回到改善终端用户体验的话题

290
00:22:30,817 --> 0:22:32,953
我们引入了环境纹理

291
00:22:33,987 --> 0:22:39,026
这极大增强了终端用户的渲染体验

292
00:22:41,261 --> 0:22:44,998
假设你的设计师很努力地

293
00:22:45,065 --> 0:22:49,269
为你的增强现实场景

294
00:22:50,904 --> 0:22:52,172
这看起来真的很棒

295
00:22:52,806 --> 0:22:57,311
但你需要为增强现实场景做更多事情

296
00:22:59,646 --> 0:23:06,153
你需要在AR场景中保持

297
00:22:59,646 --> 0:23:06,153
你需要在AR场景中保持

298
00:23:06,653 --> 0:23:11,391
这样对象看起来就像是

299
00:23:13,694 --> 0:23:16,864
保证比例正确也很重要

300
00:23:16,930 --> 0:23:18,765
这样你的对象才不会太大或太小

301
00:23:19,166 --> 0:23:24,771
ARKit会在世界跟踪中

302
00:23:28,775 --> 0:23:30,010
对于逼真的渲染

303
00:23:30,077 --> 0:23:33,614
考虑环境中的光照也很重要

304
00:23:37,284 --> 0:23:41,221
ARKit为你提供了环境光估计器

305
00:23:41,855 --> 0:23:45,158
来纠正虚拟对象的亮度

306
00:23:45,359 --> 0:23:48,762
以便你的物体看起来不会太亮或太暗

307
00:23:49,363 --> 0:23:51,298
它们会直接融入环境中

308
00:23:54,968 --> 0:23:58,305
如果要将物体放在物理表面上

309
00:23:58,372 --> 0:23:59,740
如水平面上

310
00:24:00,541 --> 0:24:04,011
为对象添加阴影也很重要

311
00:24:04,411 --> 0:24:07,915
这极大改善了人类的视觉感知

312
00:24:07,981 --> 0:24:10,651
他们会真正感知到物体就在表面上

313
00:24:13,420 --> 0:24:17,457
最后 如果是反光物体

314
00:24:18,725 --> 0:24:20,194
人们希望

315
00:24:21,028 --> 0:24:24,831
从虚拟对象的表面看到环境的倒影

316
00:24:25,766 --> 0:24:29,303
这就是环境纹理所能达到的效果

317
00:24:31,538 --> 0:24:35,676
让我们看看这个对象

318
00:24:38,111 --> 0:24:40,514
昨天晚上在我准备这个演讲时

319
00:24:40,581 --> 0:24:42,783
我创造了这个场景

320
00:24:44,718 --> 0:24:49,890
在吃这些水果的同时

321
00:24:51,792 --> 0:24:58,131
你可以看到比例是正确的

322
00:24:58,632 --> 0:25:01,201
你可以在对象中看到环境的倒影

323
00:24:58,632 --> 0:25:01,201
你可以在对象中看到环境的倒影

324
00:25:02,436 --> 0:25:04,872
在此对象的右侧

325
00:25:05,239 --> 0:25:10,244
你可以在右边看到这些水果的

326
00:25:10,944 --> 0:25:14,915
而在左侧你可以注意到

327
00:25:16,049 --> 0:25:19,753
你也可以在中间看到长凳表面的倒影

328
00:25:21,555 --> 0:25:25,492
这是通过ARKit 2中的

329
00:25:29,329 --> 0:25:30,163
谢谢

330
00:25:34,034 --> 0:25:37,538
环境纹理会收集场景的纹理信息

331
00:25:40,774 --> 0:25:43,877
通常它表示为立方体贴图

332
00:25:43,944 --> 0:25:46,046
但也有其它表示形式

333
00:25:49,116 --> 0:25:52,152
环境纹理或这个立方体贴图

334
00:25:52,219 --> 0:25:55,589
可用作渲染引擎中的反射探头

335
00:25:58,926 --> 0:26:03,163
该反射探头可以将其应用为

336
00:25:58,926 --> 0:26:03,163
该反射探头可以将其应用为

337
00:26:03,230 --> 0:26:06,567
例如我们在上一张幻灯片中

338
00:26:07,768 --> 0:26:13,707
因此它极大改善了

339
00:26:15,042 --> 0:26:19,513
让我们看看这个视频短片中

340
00:26:22,382 --> 0:26:26,553
ARKit在运行世界跟踪

341
00:26:26,920 --> 0:26:29,189
继续了解有关环境的更多信息

342
00:26:30,224 --> 0:26:31,959
通过使用计算机视觉技术

343
00:26:32,025 --> 0:26:37,865
它可以提取纹理信息

344
00:26:39,600 --> 0:26:42,870
而该立方体贴图被准确放置在场景中

345
00:26:44,771 --> 0:26:47,541
注意此立方体贴图只是部分填充

346
00:26:49,076 --> 0:26:51,545
为了设置反射探头

347
00:26:51,612 --> 0:26:54,114
我们需要一个完全立方体贴图

348
00:26:57,584 --> 0:26:59,286
要获得完全立方体贴图

349
00:26:59,353 --> 0:27:03,490
你需要扫描整个物理空间

350
00:26:59,353 --> 0:27:03,490
你需要扫描整个物理空间

351
00:27:03,557 --> 0:27:07,294
就像使用全景装置

352
00:27:08,562 --> 0:27:11,798
但这对终端用户来说并不现实

353
00:27:13,600 --> 0:27:19,673
ARKit通过使用先进的

354
00:27:19,740 --> 0:27:22,209
来让该过程变得更简单

355
00:27:26,713 --> 0:27:27,548
谢谢

356
00:27:31,485 --> 0:27:33,954
另外注意所有这些处理

357
00:27:34,021 --> 0:27:36,890
都实时发生在你的本地设备上

358
00:27:40,060 --> 0:27:43,497
一旦我们有了立方体贴图

359
00:27:43,864 --> 0:27:47,301
并且只要我们在场景中放置虚拟对象

360
00:27:47,367 --> 0:27:49,636
它们就会开始反射真实环境

361
00:27:51,271 --> 0:27:52,573
这是对环境纹理化过程

362
00:27:52,639 --> 0:27:55,409
工作原理的快速概述

363
00:27:57,611 --> 0:28:02,883
让我们看看ARKit API

364
00:27:57,611 --> 0:28:02,883
让我们看看ARKit API

365
00:28:07,821 --> 0:28:11,758
你需要在世界跟踪配置中

366
00:28:12,392 --> 0:28:18,298
就是将environmentTexturing属性

367
00:28:19,166 --> 0:28:20,667
就这么简单

368
00:28:29,643 --> 0:28:33,247
AR会话将在后台自动运行

369
00:28:33,313 --> 0:28:37,284
并将环境纹理作为环境探头锚

370
00:28:37,985 --> 0:28:39,753
提供给你

371
00:28:41,188 --> 0:28:45,359
AREnvironmentProbeAnchor

372
00:28:45,425 --> 0:28:49,162
这意味着它具有六自由度的

373
00:28:50,998 --> 0:28:54,668
此外 它有个MTLTexture

374
00:28:58,405 --> 0:29:02,709
ARKit还为你提供了

375
00:28:58,405 --> 0:29:02,709
ARKit还为你提供了

376
00:29:02,776 --> 0:29:07,347
它是反射探头所影响的区域

377
00:29:07,915 --> 0:29:12,653
并且渲染代理可以使用它来校正并行

378
00:29:12,753 --> 0:29:17,324
因此如果你的对象在场景中移动

379
00:29:17,391 --> 0:29:22,229
并且环境中的新纹理将被反射出来

380
00:29:24,965 --> 0:29:28,836
请注意这与其它锚的生命周期相同

381
00:29:28,902 --> 0:29:31,772
例如ARPlaneAnchor

382
00:29:36,944 --> 0:29:40,681
此外 它被完全集成到

383
00:29:40,981 --> 0:29:44,852
因此如果你使用SceneKit

384
00:29:45,752 --> 0:29:50,557
你只需在世界跟踪配置中启用此功能

385
00:29:50,991 --> 0:29:54,194
其余部分将由

386
00:30:00,434 --> 0:30:04,471
注意对于高级用例

387
00:30:04,538 --> 0:30:07,274
环境探测锚

388
00:30:11,545 --> 0:30:15,549
为此你需要将environmentTexturing模式

389
00:30:16,550 --> 0:30:20,587
然后你就可以在你想要的位置和方向

390
00:30:21,054 --> 0:30:25,526
并将它们添加到AR会话对象

391
00:30:28,996 --> 0:30:33,000
注意这只允许你将探测锚

392
00:30:33,800 --> 0:30:37,804
AR会话一旦获得

393
00:30:37,871 --> 0:30:40,107
它就会自动更新其纹理

394
00:30:42,309 --> 0:30:48,115
因此当你的增强现实场景

395
00:30:48,182 --> 0:30:49,149
就可以使用此模式

396
00:30:49,216 --> 0:30:53,620
你不希望使用太多环境探头锚

397
00:30:57,224 --> 0:31:00,227
让我们看一个环境纹理的快速演示

398
00:30:57,224 --> 0:31:00,227
让我们看一个环境纹理的快速演示

399
00:31:00,294 --> 0:31:04,798
并看看我们如何逼真地渲染

400
00:31:16,176 --> 0:31:18,712
我们切换到AR 1

401
00:31:23,550 --> 0:31:24,384
好的

402
00:31:24,451 --> 0:31:28,655
对于这个演示

403
00:31:29,456 --> 0:31:32,092
未启用环境纹理功能

404
00:31:33,427 --> 0:31:38,732
正如你在底部开关控制器上

405
00:31:38,799 --> 0:31:41,168
它只使用了环境光估计

406
00:31:41,735 --> 0:31:45,973
让我们放置之前见过的那个对象

407
00:31:47,074 --> 0:31:48,308
你可以看到

408
00:31:50,544 --> 0:31:53,947
这看起来没问题

409
00:31:54,014 --> 0:31:58,385
你可以看到它的阴影

410
00:31:59,553 --> 0:32:04,725
但它不能反射桌子的木质表面

411
00:31:59,553 --> 0:32:04,725
但它不能反射桌子的木质表面

412
00:32:06,760 --> 0:32:09,763
而且如果我在场景中放置一些东西

413
00:32:10,998 --> 0:32:12,533
如真实的水果

414
00:32:16,136 --> 0:32:20,774
我们在虚拟对象中看不到它的倒影

415
00:32:21,942 --> 0:32:24,244
现在让我们启用环境纹理

416
00:32:24,311 --> 0:32:29,650
并看看它如何逼真地显示这种纹理

417
00:32:31,185 --> 0:32:33,887
正如你所看到的 我一启用环境纹理

418
00:32:34,488 --> 0:32:38,892
对象就开始反射桌子的木质表面

419
00:32:38,959 --> 0:32:42,262
以及此香蕉的纹理

420
00:32:50,871 --> 0:32:51,839
谢谢

421
00:32:52,573 --> 0:32:57,177
这极大改善了你的增强现实场景

422
00:32:57,244 --> 0:33:02,416
它看起来非常真实

423
00:32:57,244 --> 0:33:02,416
它看起来非常真实

424
00:33:04,751 --> 0:33:06,253
好的 回到幻灯片

425
00:33:12,693 --> 0:33:14,761
这就是环境纹理

426
00:33:14,828 --> 0:33:20,501
它是ARKit 2中强大的新功能

427
00:33:20,567 --> 0:33:23,804
你的增强现实场景

428
00:33:25,506 --> 0:33:29,977
现在 为了继续介绍其它新特性

429
00:33:30,043 --> 0:33:32,479
我想邀请Reinhard上台来

430
00:33:38,986 --> 0:33:40,320
Reinhard

431
00:33:40,387 --> 0:33:41,221
开始了

432
00:33:42,222 --> 0:33:43,090
这能用吗？

433
00:33:43,156 --> 0:33:43,991
哦 好的 太好了

434
00:33:45,993 --> 0:33:50,397
早上好 我叫Reinhard

435
00:33:50,631 --> 0:33:53,000
接下来 我们将讨论图像跟踪

436
00:33:54,101 --> 0:33:58,639
在iOS 11.3中 我们引入了

437
00:33:59,606 --> 0:34:03,177
图像检测会搜索场景中

438
00:33:59,606 --> 0:34:03,177
图像检测会搜索场景中

439
00:34:04,311 --> 0:34:07,714
这里的术语“检测”

440
00:34:07,781 --> 0:34:09,583
因此它们应该不会移动

441
00:34:10,250 --> 0:34:14,888
这些图像的例子包括电影海报

442
00:34:16,356 --> 0:34:20,293
一旦检测到图像

443
00:34:20,360 --> 0:34:23,664
ARKit将估算

444
00:34:24,431 --> 0:34:28,068
该姿态可用于触发

445
00:34:29,570 --> 0:34:30,704
正如我前面提到的

446
00:34:31,103 --> 0:34:33,607
所有这些都完全集成在世界跟踪中

447
00:34:34,007 --> 0:34:37,143
你所要做的只是在你的属性中

448
00:34:38,978 --> 0:34:42,114
为了加载用于图像检测的图像

449
00:34:42,181 --> 0:34:45,886
你可以从文件加载它们

450
00:34:45,953 --> 0:34:49,121
它还可以为你提供图像的检测质量

451
00:34:50,357 --> 0:34:54,795
图像检测现在已经很棒了

452
00:34:54,862 --> 0:34:57,130
让我们来谈谈图像跟踪

453
00:34:57,798 --> 0:35:02,369
图像跟踪是图像检测的一种扩展

454
00:34:57,798 --> 0:35:02,369
图像跟踪是图像检测的一种扩展

455
00:35:02,436 --> 0:35:05,672
但它的优势在于图像

456
00:35:07,574 --> 0:35:11,512
ARKit现将以每秒60帧的速度

457
00:35:11,578 --> 0:35:13,080
估算每一帧的位置和方向

458
00:35:13,647 --> 0:35:20,454
这可以让你准确地增强2D图像

459
00:35:20,521 --> 0:35:23,490
或任何具有真实图像特征的东西

460
00:35:25,158 --> 0:35:29,129
且ARKit也可同时跟踪多个图像

461
00:35:30,831 --> 0:35:34,301
默认情况下它只选择一个

462
00:35:34,368 --> 0:35:38,906
比如杂志的封面

463
00:35:38,972 --> 0:35:42,609
或者如果是杂志内的双页杂志

464
00:35:42,676 --> 0:35:43,844
你可以把它设置为2

465
00:35:45,279 --> 0:35:50,717
在iOS 12的ARKit 2中

466
00:35:50,784 --> 0:35:53,120
即ARImageTrackingConfiguration

467
00:35:53,587 --> 0:35:56,657
它可让你进行独立的图像跟踪

468
00:35:57,157 --> 0:35:58,792
让我们看看如何设置它

469
00:35:59,960 --> 0:36:02,229
我们首先加载一组

470
00:35:59,960 --> 0:36:02,229
我们首先加载一组

471
00:36:02,296 --> 0:36:04,565
其可以来自文件或素材目录

472
00:36:05,732 --> 0:36:08,335
加载完这组

473
00:36:08,669 --> 0:36:13,507
我通过它指定

474
00:36:13,574 --> 0:36:17,277
来将会话设为

475
00:36:17,344 --> 0:36:19,213
或通过指定trackingImages属性

476
00:36:19,279 --> 0:36:21,081
将其设为ARImageTracking

477
00:36:22,349 --> 0:36:27,054
完成配置后 我用它来运行我的会话

478
00:36:28,355 --> 0:36:31,291
和往常一样 一旦会话开始运行

479
00:36:31,358 --> 0:36:33,727
我会在每次更新时

480
00:36:34,428 --> 0:36:39,032
一旦检测到图像

481
00:36:39,099 --> 0:36:41,034
ARImageAnchor类型

482
00:36:42,436 --> 0:36:45,138
这个ARImageAnchor

483
00:36:45,205 --> 0:36:48,242
我可以通过遵守

484
00:36:48,709 --> 0:36:52,613
其中有一个

485
00:36:53,113 --> 0:36:56,550
它可以通知你图像的跟踪状态

486
00:36:56,850 --> 0:36:59,419
如果对象已跟踪则为true

487
00:37:00,387 --> 0:37:04,291
它还会通知你检测到的图像及其位置

488
00:37:04,591 --> 0:37:09,396
其位置和方向以4*4矩阵表示

489
00:37:11,665 --> 0:37:15,802
为了获得这样的图像锚

490
00:37:15,869 --> 0:37:19,173
这很好 让我们来看看

491
00:37:19,740 --> 0:37:24,344
这张图片可以在儿童书中找到

492
00:37:24,411 --> 0:37:26,213
事实上 它非常适合图像跟踪

493
00:37:26,713 --> 0:37:28,649
它有很多不同的视觉特征

494
00:37:28,715 --> 0:37:31,318
它的纹理清晰

495
00:37:32,686 --> 0:37:35,956
另一方面 在儿童书中也可以

496
00:37:36,023 --> 0:37:40,093
但这种图像并不合适

497
00:37:40,694 --> 0:37:44,064
它有很多重复的结构

498
00:37:44,131 --> 0:37:47,201
并且一旦将其转换为灰度值

499
00:37:48,335 --> 0:37:50,737
但你不必自己识别这些统计信息

500
00:37:50,804 --> 0:37:52,206
因为Xcode可为你提供帮助

501
00:37:52,873 --> 0:37:55,242
如果我将这两个图像导入Xcode

502
00:37:55,976 --> 0:37:59,112
我们可以看到海洋生物图像

503
00:37:59,179 --> 0:38:02,249
这意味着它是推荐使用的

504
00:37:59,179 --> 0:38:02,249
这意味着它是推荐使用的

505
00:38:02,316 --> 0:38:04,885
显示警告图标以表示不建议使用

506
00:38:06,019 --> 0:38:10,457
如果我点击此图标

507
00:38:10,524 --> 0:38:13,760
不适合用于图像跟踪的精确描述

508
00:38:14,194 --> 0:38:16,430
我可以得到的信息包括直方图

509
00:38:16,496 --> 0:38:19,499
一致的颜色区域以及直方图

510
00:38:21,435 --> 0:38:23,704
一旦我加载完图片

511
00:38:23,770 --> 0:38:26,273
我有两种配置选择

512
00:38:26,707 --> 0:38:29,443
首先是

513
00:38:29,510 --> 0:38:30,511
我们来谈谈这个

514
00:38:33,347 --> 0:38:35,883
当我们使用世界跟踪

515
00:38:35,949 --> 0:38:38,886
图像锚在世界坐标系中表示

516
00:38:39,253 --> 0:38:43,190
这意味着图像锚可选择平面锚

517
00:38:44,291 --> 0:38:49,363
相机和世界原点本身

518
00:38:49,730 --> 0:38:52,666
这使它们的互动非常简单直观

519
00:38:54,067 --> 0:38:56,370
而作为iOS 12中的新功能

520
00:38:56,436 --> 0:39:00,407
对于以前只能检测到的图像

521
00:38:56,436 --> 0:39:00,407
对于以前只能检测到的图像

522
00:39:02,276 --> 0:39:05,679
我们有了一个新的配置

523
00:39:05,746 --> 0:39:08,048
它可以执行独立的图像跟踪

524
00:39:08,949 --> 0:39:12,586
这意味着它独立于世界跟踪

525
00:39:12,653 --> 0:39:15,522
而不依赖运动传感器来进行跟踪

526
00:39:16,123 --> 0:39:20,227
这意味着此配置在开始识别图像之前

527
00:39:20,561 --> 0:39:25,165
并且可以在无法进行世界跟踪的

528
00:39:25,232 --> 0:39:26,733
场景中成功识别

529
00:39:26,800 --> 0:39:30,170
如电梯或火车等移动平台

530
00:39:31,438 --> 0:39:35,843
在这种情况下 ARKit将以

531
00:39:35,909 --> 0:39:38,178
估算每帧的位置和方向

532
00:39:39,313 --> 0:39:43,116
并且只需四行简单代码

533
00:39:43,951 --> 0:39:46,987
你需要做的是首先创建一个

534
00:39:47,054 --> 0:39:50,457
类型为ARImageTracking

535
00:39:50,524 --> 0:39:52,626
并指定想要跟踪的一组图像

536
00:39:53,093 --> 0:39:57,297
在这个例子中

537
00:39:59,466 --> 0:40:02,569
我告诉配置我要跟踪的图像数量

538
00:39:59,466 --> 0:40:02,569
我告诉配置我要跟踪的图像数量

539
00:40:03,003 --> 0:40:04,972
在这个例子中 我将其指定为2

540
00:40:06,106 --> 0:40:09,910
在我的用例中

541
00:40:09,977 --> 0:40:11,712
而不会同时出现三个

542
00:40:12,479 --> 0:40:16,917
注意如果我正在跟踪两个图像

543
00:40:17,618 --> 0:40:21,855
它将不会被跟踪

544
00:40:23,190 --> 0:40:26,126
然后我使用此配置来运行我的会话

545
00:40:27,728 --> 0:40:31,698
正如我之前提到的

546
00:40:31,765 --> 0:40:33,934
并使用世界跟踪来做到这点

547
00:40:35,169 --> 0:40:38,038
图像检测和跟踪之间的唯一区别

548
00:40:38,105 --> 0:40:39,940
是跟踪图像的最大数量

549
00:40:40,340 --> 0:40:45,412
如果你有一个使用图像检测的app

550
00:40:45,746 --> 0:40:49,950
你可以简单地添加它并重新编译

551
00:40:50,884 --> 0:40:55,255
为了向你展示这是多么容易

552
00:41:03,130 --> 0:41:06,767
我们可以切换到AR 2吗？

553
00:41:06,834 --> 0:41:10,404
我想创建一个AR相框

554
00:41:10,470 --> 0:41:13,140
为此我带了一张我家的猫的照片

555
00:41:13,774 --> 0:41:15,409
让我们使用Xcode构建它

556
00:41:16,610 --> 0:41:22,749
我先用Xcode

557
00:41:22,983 --> 0:41:25,485
如你所见 它现在是空的

558
00:41:26,720 --> 0:41:29,890
接下来我需要指定要附加的图像

559
00:41:30,557 --> 0:41:33,093
为此我导入了

560
00:41:33,160 --> 0:41:34,394
让我们在这里打开她

561
00:41:36,997 --> 0:41:37,831
这就是我的猫

562
00:41:40,400 --> 0:41:41,969
我需要指定一个名字

563
00:41:42,436 --> 0:41:44,838
我将其命名为Daisy

564
00:41:44,905 --> 0:41:49,710
我在这里指定了在现实世界中

565
00:41:49,776 --> 0:41:50,911
即我的相框宽度

566
00:41:52,246 --> 0:41:55,282
我也载入了一段我的猫的视频

567
00:41:56,350 --> 0:41:57,851
让我们把这一切都集中在一起

568
00:41:58,619 --> 0:42:01,154
首先我将创建一个配置

569
00:41:58,619 --> 0:42:01,154
首先我将创建一个配置

570
00:42:01,221 --> 0:42:04,691
这是一个

571
00:42:04,758 --> 0:42:06,527
类型的配置

572
00:42:07,528 --> 0:42:10,230
我通过使用组名Photos

573
00:42:10,864 --> 0:42:13,300
从素材目录中加载了一组跟踪图像

574
00:42:13,734 --> 0:42:17,838
其中只包含一张图片

575
00:42:18,939 --> 0:42:19,806
接下来

576
00:42:21,408 --> 0:42:22,709
我通过指定trackingImages属性

577
00:42:22,776 --> 0:42:25,379
设置好图像跟踪配置

578
00:42:26,180 --> 0:42:29,183
并将maximumNumberOfTrackedImages

579
00:42:30,450 --> 0:42:33,887
此时 app已经启动了AR会话

580
00:42:33,954 --> 0:42:36,557
并在检测到图像时为你提供其图像锚

581
00:42:36,957 --> 0:42:38,258
但让我们添加一些内容

582
00:42:39,026 --> 0:42:40,894
我将通过在资源面板中

583
00:42:42,029 --> 0:42:47,768
载入视频并根据它创建一个

584
00:42:49,102 --> 0:42:52,072
现在让我们将它添加到真实图像之上

585
00:42:55,142 --> 0:42:58,579
为此我需要检查该锚

586
00:42:59,379 --> 0:43:03,317
并创建一个与场景中的图像

587
00:42:59,379 --> 0:43:03,317
并创建一个与场景中的图像

588
00:43:03,383 --> 0:43:05,052
具有相同物理尺寸的

589
00:43:06,320 --> 0:43:09,389
我将videoPlayer

590
00:43:09,890 --> 0:43:11,792
并开始播放videoPlayer

591
00:43:13,260 --> 0:43:16,230
我通过geometry

592
00:43:16,296 --> 0:43:21,235
并反转它以匹配锚坐标系

593
00:43:22,402 --> 0:43:26,306
就是这样 我们运行它

594
00:43:30,310 --> 0:43:33,380
一旦我将相机对准我的猫的相框

595
00:43:34,114 --> 0:43:37,251
视频将开始播放

596
00:43:44,491 --> 0:43:47,394
由于ARKit实时估计位置

597
00:43:47,461 --> 0:43:50,764
我可以自由移动设备

598
00:43:51,265 --> 0:43:53,934
我可以看到每一帧都在更新

599
00:43:55,736 --> 0:43:56,770
她跑掉了

600
00:43:58,539 --> 0:44:00,741
我们的演示就到这里吧

601
00:43:58,539 --> 0:44:00,741
我们的演示就到这里吧

602
00:44:08,248 --> 0:44:12,753
如你所见 在ARKit中

603
00:44:13,120 --> 0:44:15,856
事实上 制作一个猫的视频

604
00:44:17,991 --> 0:44:21,828
图像跟踪非常适合

605
00:44:22,262 --> 0:44:25,232
但我们不仅限于平坦的2D物体

606
00:44:25,299 --> 0:44:28,435
我们接下来谈谈对象检测

607
00:44:33,607 --> 0:44:38,779
对象检测可用于检测场景中的

608
00:44:39,880 --> 0:44:43,550
就像刚才的图像检测一样

609
00:44:43,617 --> 0:44:47,454
需要是静态的 也就是它不应该移动

610
00:44:48,355 --> 0:44:52,125
此类对象的很好的例子

611
00:44:52,192 --> 0:44:54,027
某些玩具或家居用品

612
00:44:56,496 --> 0:44:57,965
和图像检测一样

613
00:44:58,031 --> 0:45:01,902
首先需要使用运行ARKit

614
00:44:58,031 --> 0:45:01,902
首先需要使用运行ARKit

615
00:45:03,437 --> 0:45:08,642
为此我们提供了一个全功能

616
00:45:08,709 --> 0:45:10,611
它允许你扫描自己的3D对象

617
00:45:11,378 --> 0:45:16,517
此类对象需要具有一些特性

618
00:45:16,583 --> 0:45:18,218
刚性且无反射

619
00:45:18,652 --> 0:45:21,655
它们的大小应该与桌面摆设大致相同

620
00:45:23,790 --> 0:45:27,194
ARKit可用于估计这些物体

621
00:45:27,261 --> 0:45:29,363
在六自由度中的位置和方向

622
00:45:31,765 --> 0:45:35,202
所有这些都完全集成到世界跟踪中

623
00:45:35,269 --> 0:45:37,971
你需要做的只是设置一个属性

624
00:45:38,038 --> 0:45:39,806
就可以开始进行对象检测了

625
00:45:40,707 --> 0:45:42,576
让我们来看看这是如何设置的

626
00:45:43,944 --> 0:45:48,348
我从文件或Xcode素材目录中

627
00:45:49,416 --> 0:45:51,652
我稍后再讨论参考对象

628
00:45:52,452 --> 0:45:54,288
加载完这些参考对象后

629
00:45:54,354 --> 0:45:59,459
我通过指定

630
00:45:59,526 --> 0:46:01,728
以用它们来设置我的

631
00:45:59,526 --> 0:46:01,728
以用它们来设置我的

632
00:46:01,962 --> 0:46:04,298
ARWorldTrackingConfiguration

633
00:46:05,699 --> 0:46:10,504
设置好配置后 使用它运行会话

634
00:46:11,271 --> 0:46:14,341
就像图像检测一样 AR会话运行后

635
00:46:14,408 --> 0:46:16,243
每次更新

636
00:46:16,310 --> 0:46:17,678
在这种情况下

637
00:46:18,579 --> 0:46:21,148
一旦在场景中检测到一个对象

638
00:46:21,215 --> 0:46:26,553
ARFrame中就会出现

639
00:46:28,789 --> 0:46:32,659
该AR对象

640
00:46:33,093 --> 0:46:35,696
所以它有个transform变量

641
00:46:35,762 --> 0:46:40,667
代表其六自由度位置和方向

642
00:46:40,734 --> 0:46:44,805
来告诉我它检测到了哪些对象

643
00:46:46,206 --> 0:46:52,279
只需三行简单代码就可以实现这点

644
00:46:52,980 --> 0:46:53,947
我创建了一个

645
00:46:54,014 --> 0:46:56,416
ARWorldTrackingConfiguration

646
00:46:56,717 --> 0:46:59,953
并指定一组我想要检测的对象

647
00:47:00,554 --> 0:47:05,492
在这个例子中 我设想通过检测

648
00:47:05,559 --> 0:47:07,628
来构建一个简单的AR博物馆app

649
00:47:08,962 --> 0:47:11,231
然后我用它来运行我的会话

650
00:47:12,432 --> 0:47:17,037
事实上 我们在办公室构建了这个

651
00:47:17,437 --> 0:47:18,472
所以让我们来看看

652
00:47:19,072 --> 0:47:22,009
一旦这个半身像

653
00:47:22,075 --> 0:47:24,278
进入我的iOS app视图

654
00:47:24,611 --> 0:47:28,749
我就可以得到其六自由度姿态

655
00:47:29,082 --> 0:47:32,286
一个悬浮在雕像上方的

656
00:47:32,886 --> 0:47:35,856
在这个例子中

657
00:47:35,923 --> 0:47:38,258
这个埃及女王的名字

658
00:47:38,692 --> 0:47:40,327
但你可以添加你的渲染引擎

659
00:47:40,394 --> 0:47:42,362
允许你使用的任何内容

660
00:47:43,897 --> 0:47:47,501
为了构建这个app

661
00:47:48,001 --> 0:47:49,870
所以我们来谈谈对象扫描

662
00:47:51,872 --> 0:47:55,943
对象扫描可以从世界中

663
00:47:57,144 --> 0:47:59,880
它与平面估计关系密切

664
00:47:59,947 --> 0:48:02,249
其中我们使用累积场景信息

665
00:47:59,947 --> 0:48:02,249
其中我们使用累积场景信息

666
00:48:02,516 --> 0:48:05,285
来估计水平或垂直平面的位置

667
00:48:06,086 --> 0:48:09,089
在这种情况下 我们使用这类信息

668
00:48:09,489 --> 0:48:13,427
来收集有关3D对象的信息

669
00:48:15,128 --> 0:48:19,600
为了指定要查找对象的区域

670
00:48:19,666 --> 0:48:22,135
转换 范围和中心

671
00:48:22,769 --> 0:48:24,137
这实际上是一个对象周围的边界框

672
00:48:24,204 --> 0:48:28,141
用来定义它在场景中的位置

673
00:48:29,877 --> 0:48:33,680
Xcode素材目录

674
00:48:33,981 --> 0:48:38,652
因此将它们导入新app非常容易

675
00:48:38,719 --> 0:48:41,288
并可以根据需要重复使用它们

676
00:48:42,556 --> 0:48:44,825
对于扫描 我们添加了一个新配置

677
00:48:44,892 --> 0:48:46,927
即ARObjectScanningConfiguration

678
00:48:48,161 --> 0:48:51,665
但你无需实现自己的扫描app

679
00:48:52,099 --> 0:48:56,270
因为你可以使用

680
00:48:56,336 --> 0:48:59,072
它被称为“扫描和检测3D对象”

681
00:49:00,140 --> 0:49:02,276
我们来看看这个app是如何工作的

682
00:49:02,709 --> 0:49:06,446
我首先在感兴趣的对象周围

683
00:49:06,513 --> 0:49:08,248
在这个例子中

684
00:49:09,316 --> 0:49:11,118
注意边界框不需要

685
00:49:11,185 --> 0:49:12,786
严格包围对象

686
00:49:12,853 --> 0:49:13,687
我们所关心的是

687
00:49:13,754 --> 0:49:17,925
最重要的特征点均在其范围内

688
00:49:19,193 --> 0:49:21,061
当我对边界框感到满意时

689
00:49:21,828 --> 0:49:26,733
我可以点击扫描按钮

690
00:49:27,134 --> 0:49:30,404
我可以看到扫描从下往上进行

691
00:49:30,804 --> 0:49:34,875
可以以空间形式表示扫描对象的程度

692
00:49:35,943 --> 0:49:38,846
注意你不必从所有方向扫描对象

693
00:49:39,546 --> 0:49:43,984
例如 如果在博物馆中有一个

694
00:49:44,351 --> 0:49:48,856
因此你无法从一个特定的视角检测它

695
00:49:49,223 --> 0:49:51,091
那么你就不需要从那边扫描它

696
00:49:53,360 --> 0:49:55,729
一旦你对扫描结果感到满意

697
00:49:56,964 --> 0:50:01,101
你就可以调整范围的中心

698
00:49:56,964 --> 0:50:01,101
你就可以调整范围的中心

699
00:50:01,535 --> 0:50:03,437
它对应于对象的原点

700
00:50:04,238 --> 0:50:05,272
这里唯一的要求

701
00:50:05,339 --> 0:50:08,575
是中心必须保持在对象的范围内

702
00:50:10,010 --> 0:50:14,348
最后 扫描app

703
00:50:15,349 --> 0:50:18,785
在这个例子中

704
00:50:18,852 --> 0:50:20,587
这意味着这是一次很好的扫描

705
00:50:21,755 --> 0:50:26,627
我们在这里的建议是

706
00:50:26,693 --> 0:50:28,862
来测试该检测

707
00:50:29,463 --> 0:50:32,299
在不同纹理和不同光照条件下

708
00:50:34,434 --> 0:50:38,772
完成扫描后 你将得到

709
00:50:39,540 --> 0:50:41,441
我们在前面的图中见过它

710
00:50:42,276 --> 0:50:47,047
此对象通常可以序列化为

711
00:50:47,581 --> 0:50:52,286
它有一个名称变量

712
00:50:52,352 --> 0:50:55,422
以及用于扫描它的中心和范围变量

713
00:50:56,490 --> 0:50:59,059
此外你还将得到执行扫描时

714
00:50:59,126 --> 0:51:02,696
找到的所有原始特征点

715
00:50:59,126 --> 0:51:02,696
找到的所有原始特征点

716
00:51:05,332 --> 0:51:09,536
这就是对象检测

717
00:51:09,770 --> 0:51:12,639
你需要扫描它们

718
00:51:12,706 --> 0:51:14,107
你可以立即下载它

719
00:51:16,310 --> 0:51:19,479
我们接下来谈谈面部跟踪

720
00:51:24,318 --> 0:51:26,787
当我们去年发布iPhone X时

721
00:51:26,854 --> 0:51:30,057
我们为ARKit添加了健壮的

722
00:51:30,457 --> 0:51:34,461
ARKit以每秒60帧的速度

723
00:51:34,528 --> 0:51:37,297
估计每一帧中脸的位置和方向

724
00:51:37,764 --> 0:51:42,870
这个姿态可以通过添加帽子

725
00:51:42,936 --> 0:51:46,006
来增强用户的面部

726
00:51:47,774 --> 0:51:50,143
ARKit还为你提供拟合三角网格

727
00:51:50,210 --> 0:51:52,145
来构成

728
00:51:54,882 --> 0:51:58,151
此ARFaceGeometry

729
00:51:58,218 --> 0:52:05,225
渲染这个面部网格所需的所有信息

730
00:51:58,218 --> 0:52:05,225
渲染这个面部网格所需的所有信息

731
00:52:05,292 --> 0:52:06,860
以及检测坐标

732
00:52:08,795 --> 0:52:11,865
人脸跟踪的主要锚类型

733
00:52:11,932 --> 0:52:15,369
其中包含执行面部跟踪

734
00:52:16,937 --> 0:52:19,873
为了逼真地渲染这样的几何图形

735
00:52:20,407 --> 0:52:22,676
我们添加了定向光估计

736
00:52:23,544 --> 0:52:27,514
这里 ARKit使用你的光照

737
00:52:27,581 --> 0:52:33,086
并估算此ARDirectionLightEstimate

738
00:52:33,153 --> 0:52:35,255
方向以及色温

739
00:52:36,623 --> 0:52:42,095
此估计值足以让大多数app

740
00:52:42,429 --> 0:52:44,698
但如果你的app有更复杂的需求

741
00:52:45,432 --> 0:52:49,369
我们还提供二次球谐系数

742
00:52:49,436 --> 0:52:52,306
来收集整个场景的光照条件

743
00:52:53,307 --> 0:52:55,776
从而让你的内容看起来甚至更棒

744
00:52:57,878 --> 0:53:00,480
ARKit还可以实时跟踪表情

745
00:52:57,878 --> 0:53:00,480
ARKit还可以实时跟踪表情

746
00:53:01,315 --> 0:53:03,784
这些表情是所谓的混合形状

747
00:53:03,851 --> 0:53:06,820
它们有50多种

748
00:53:08,021 --> 0:53:10,657
这种混合形状假定一个

749
00:53:11,191 --> 0:53:13,994
1意味着完全激活

750
00:53:14,294 --> 0:53:18,532
例如 如果我张开嘴

751
00:53:18,599 --> 0:53:21,468
如果我合上嘴 则值接近0

752
00:53:22,769 --> 0:53:25,639
这对为你自己的虚拟角色创建动画

753
00:53:26,106 --> 0:53:29,209
在这个例子中 我使用了张开下颚

754
00:53:29,276 --> 0:53:32,846
以及眨右眼来为这个简单的

755
00:53:34,181 --> 0:53:35,382
但它可以做得更好

756
00:53:36,216 --> 0:53:40,621
事实上 在我们制作表情符号时

757
00:53:41,121 --> 0:53:43,390
你在这里所看到的移动蓝条

758
00:53:43,457 --> 0:53:48,195
被用来将我的面部表情

759
00:53:49,763 --> 0:53:54,234
注意ARKit提供了为你自己的

760
00:53:54,701 --> 0:53:56,470
就像我们制作表情符号时一样

761
00:53:59,673 --> 0:54:00,507
谢谢

762
00:53:59,673 --> 0:54:00,507
谢谢

763
00:54:04,778 --> 0:54:07,848
让我们看看ARKit 2中

764
00:54:08,682 --> 0:54:12,853
我们添加了凝视跟踪

765
00:54:12,920 --> 0:54:14,621
跟踪左眼和右眼

766
00:54:21,495 --> 0:54:24,464
你会发现它们是

767
00:54:24,765 --> 0:54:26,233
还有一lookAtPoint变量

768
00:54:26,300 --> 0:54:29,536
它对应于两个凝视方向的交叉点

769
00:54:30,270 --> 0:54:34,174
你可以使用此信息

770
00:54:34,441 --> 0:54:36,944
或你的app的任何其它形式的输入

771
00:54:37,778 --> 0:54:38,612
而且不止如此

772
00:54:39,680 --> 0:54:41,114
我们增加了对舌头的支持

773
00:54:42,149 --> 0:54:43,884
它以新的混合形状出现

774
00:54:44,585 --> 0:54:47,120
如果我伸出舌头

775
00:54:47,187 --> 0:54:48,589
否则为0

776
00:54:49,556 --> 0:54:53,594
你同样可以使用它来为自己的角色

777
00:54:53,660 --> 0:54:56,530
作为你app的输入形式

778
00:55:00,667 --> 0:55:01,502
谢谢

779
00:55:03,704 --> 0:55:08,141
看到自己一遍又一遍地伸出舌头

780
00:55:08,942 --> 0:55:12,813
ARKit 2新特性

781
00:55:13,814 --> 0:55:16,984
我们看到了保存和加载映射

782
00:55:17,050 --> 0:55:20,521
这是用于持久性和多用户协作的

783
00:55:21,822 --> 0:55:25,959
世界跟踪增强可以进行

784
00:55:26,293 --> 0:55:28,362
以及新的视频格式

785
00:55:29,496 --> 0:55:31,231
对于环境纹理

786
00:55:31,665 --> 0:55:36,370
我们可以通过收集场景纹理

787
00:55:36,637 --> 0:55:40,807
来让内容看起来好像真的在场景中

788
00:55:41,875 --> 0:55:43,510
通过图像跟踪…

789
00:55:46,413 --> 0:55:51,084
通过图像跟踪 我们现在能够

790
00:55:51,518 --> 0:55:53,887
但ARKit也可以检测3D对象

791
00:55:54,588 --> 0:55:57,758
对于脸部追踪

792
00:55:59,893 --> 0:56:01,895
所有这些功能都以ARKit的

793
00:55:59,893 --> 0:56:01,895
所有这些功能都以ARKit的

794
00:56:01,962 --> 0:56:04,498
构建块的形式供你使用

795
00:56:06,233 --> 0:56:09,203
在iOS 12中

796
00:56:09,269 --> 0:56:10,370
包括两个新增配置

797
00:56:10,704 --> 0:56:12,339
即ARImageTrackingConfiguration

798
00:56:12,406 --> 0:56:14,508
它用于独立图像跟踪

799
00:56:14,575 --> 0:56:16,944
以及

800
00:56:18,078 --> 0:56:20,914
此外还有一系列补充类型

801
00:56:20,981 --> 0:56:22,883
可以用于与AR会话进行交互

802
00:56:23,350 --> 0:56:25,619
比如ARFrame

803
00:56:26,720 --> 0:56:28,021
其中有两个新成员

804
00:56:28,088 --> 0:56:30,057
用于对象检测的

805
00:56:30,424 --> 0:56:33,493
和用于持久性及多用户的

806
00:56:34,895 --> 0:56:36,029
ARAnchor代表了

807
00:56:36,096 --> 0:56:39,466
现实世界中的位置

808
00:56:39,533 --> 0:56:44,371
ARObjectAnchor

809
00:56:45,639 --> 0:56:48,909
我很期待看到你们将使用

810
00:56:48,976 --> 0:56:52,312
所有这些构建块所构建的东西

811
00:57:00,721 --> 0:57:04,024
还有另一个很酷的演讲

812
00:57:04,091 --> 0:57:07,528
集成AR Quick Look

813
00:57:08,662 --> 0:57:12,366
非常感谢 请享受大会的其余部分
