1
00:00:16,750 --> 0:00:21,588
（在Vision中进行对象跟踪

2
00:00:27,628 --> 0:00:31,665
大家下午好 欢迎来到演讲

3
00:00:32,732 --> 0:00:35,802
你是否需要面对并解决

4
00:00:36,537 --> 0:00:40,140
如果是的话

5
00:00:40,207 --> 0:00:42,809
或者是tvOS开发者

6
00:00:43,210 --> 0:00:44,811
我叫Sergey Kamensky

7
00:00:44,878 --> 0:00:47,648
我很高兴与你分享

8
00:00:51,585 --> 0:00:53,687
我们今天的议程包括四个部分

9
00:00:53,754 --> 0:00:55,522
首先 我们将讨论

10
00:00:56,190 --> 0:00:58,625
其次 我们将讨论今年

11
00:00:58,692 --> 0:01:00,060
我们引入了什么新变化

12
00:00:58,692 --> 0:01:00,060
我们引入了什么新变化

13
00:01:00,861 --> 0:01:02,696
第三 我们将深入探讨

14
00:01:02,763 --> 0:01:04,665
如何与Vision API

15
00:01:05,498 --> 0:01:06,333
最后

16
00:01:06,400 --> 0:01:08,335
我们将开始这次演讲的主题

17
00:01:08,402 --> 0:01:10,404
即在Vision中进行跟踪

18
00:01:14,007 --> 0:01:15,008
为什么选择Vision

19
00:01:17,911 --> 0:01:21,281
当我们设计该框架时

20
00:01:21,348 --> 0:01:23,450
能够解决你所有的计算机视觉问题

21
00:01:24,117 --> 0:01:26,687
首先是简单而一致的界面

22
00:01:27,454 --> 0:01:33,026
支持多平台 我们的框架在iOS

23
00:01:34,127 --> 0:01:35,429
我们以隐私为导向

24
00:01:35,863 --> 0:01:39,066
这意味着你的数据永远不会离开设备

25
00:01:39,499 --> 0:01:41,068
所有的处理都在本地进行

26
00:01:42,169 --> 0:01:43,570
而且我们持续演进

27
00:01:44,738 --> 0:01:49,009
我们既增强现有算法

28
00:01:51,545 --> 0:01:52,880
来看看Vision基础

29
00:01:54,548 --> 0:01:57,084
当你考虑如何

30
00:01:57,150 --> 0:01:59,419
我希望你从这些方面来思考它

31
00:02:00,287 --> 0:02:03,390
要处理什么 如何处理

32
00:02:04,691 --> 0:02:07,060
要处理什么与请求族相关

33
00:02:07,761 --> 0:02:09,463
这就是你如何告诉我们

34
00:02:10,364 --> 0:02:14,201
如何处理与我们的请求处理程序

35
00:02:14,268 --> 0:02:18,172
我们的请求处理程序

36
00:02:19,106 --> 0:02:20,240
最后是结果

37
00:02:20,774 --> 0:02:23,510
Vision中的结果

38
00:02:24,511 --> 0:02:25,712
请看一下这张幻灯片

39
00:02:26,246 --> 0:02:28,715
如果你要记住此演讲中的任何内容

40
00:02:28,782 --> 0:02:30,984
这张幻灯片可能是

41
00:02:31,285 --> 0:02:33,020
这张幻灯片展示的是一种思想

42
00:02:33,086 --> 0:02:37,591
即如何与Vision、请求

43
00:02:42,296 --> 0:02:43,664
让我们先看一下请求

44
00:02:44,164 --> 0:02:46,867
这些是我们现在支持的一些请求

45
00:02:47,568 --> 0:02:49,937
如你所见 我们有各种检测器

46
00:02:50,437 --> 0:02:52,105
也有ImageRegistrationRequest

47
00:02:52,706 --> 0:02:55,843
我们有两个跟踪器

48
00:02:56,310 --> 0:03:00,047
如果你有兴趣了解有关

49
00:02:56,310 --> 0:03:00,047
如果你有兴趣了解有关

50
00:03:00,113 --> 0:03:02,349
我邀请你参加

51
00:03:02,416 --> 0:03:05,919
其中我的同事Frank

52
00:03:08,488 --> 0:03:10,090
让我们来看看请求处理程序

53
00:03:11,191 --> 0:03:12,359
在Vision中 我们有两个

54
00:03:12,426 --> 0:03:16,196
我们有图像请求处理程序

55
00:03:16,263 --> 0:03:18,832
让我们使用这些标准来比较它们

56
00:03:20,200 --> 0:03:22,069
我们首先看一下图像请求处理程序

57
00:03:24,037 --> 0:03:28,675
图像请求处理程序用于处理

58
00:03:30,077 --> 0:03:32,546
它没有说的是

59
00:03:32,613 --> 0:03:35,649
比如图像衍生物和上一个请求的结果

60
00:03:35,949 --> 0:03:39,019
从而使管道中的其它请求

61
00:03:39,887 --> 0:03:41,221
我来举个例子

62
00:03:41,288 --> 0:03:44,024
如果请求依赖于运行神经网络

63
00:03:44,091 --> 0:03:47,027
如你所知

64
00:03:47,094 --> 0:03:48,495
和特定色彩方案的图像

65
00:03:48,562 --> 0:03:53,066
假设你的神经网络预期处理

66
00:03:53,834 --> 0:03:57,137
你获得的用户输入很少会

67
00:03:57,204 --> 0:03:59,706
所以我们在内部做的是

68
00:03:59,773 --> 0:04:03,076
我们会将其输入神经网络

69
00:03:59,773 --> 0:04:03,076
我们会将其输入神经网络

70
00:04:03,143 --> 0:04:06,280
但我们也会在请求处理程序对象上

71
00:04:06,613 --> 0:04:08,215
所以当下一个请求到来时

72
00:04:08,282 --> 0:04:10,884
如果它需要使用相同的格式

73
00:04:10,951 --> 0:04:12,519
因此不需要重新计算

74
00:04:13,253 --> 0:04:15,889
我们也会缓存从请求中获得的结果

75
00:04:15,956 --> 0:04:17,958
从而使其它请求可以在管道中使用它

76
00:04:18,024 --> 0:04:21,161
我们将稍后再讨论管道

77
00:04:22,930 --> 0:04:24,765
让我们来看看序列请求处理程序

78
00:04:25,566 --> 0:04:28,769
序列请求处理程序用于处理特定操作

79
00:04:28,836 --> 0:04:31,672
例如在一系列帧中进行跟踪

80
00:04:32,339 --> 0:04:35,576
它在内部所做的是

81
00:04:35,642 --> 0:04:38,378
从一帧到另一帧

82
00:04:39,580 --> 0:04:43,483
在Vision中 它用于处理跟踪

83
00:04:43,917 --> 0:04:46,720
所有其它请求都使用我们的

84
00:04:51,124 --> 0:04:52,259
让我们来看看结果

85
00:04:52,693 --> 0:04:54,895
Vision的结果

86
00:04:55,362 --> 0:05:01,034
Observation是从VNObservation类

87
00:04:55,362 --> 0:05:01,034
Observation是从VNObservation类

88
00:05:01,468 --> 0:05:02,903
我们如何获得一个观察

89
00:05:03,604 --> 0:05:07,074
首先 最自然的方式是在处理请求时

90
00:05:07,140 --> 0:05:09,443
你可以查看

91
00:05:09,510 --> 0:05:11,912
而该results属性

92
00:05:11,979 --> 0:05:14,481
这就是我们告诉你处理结果的方式

93
00:05:16,483 --> 0:05:19,219
第二种方法是手动创建它

94
00:05:19,887 --> 0:05:22,789
我们将在演示过程中

95
00:05:27,361 --> 0:05:29,730
现在让我们来看看今年有什么新东西

96
00:05:30,397 --> 0:05:32,900
首先 我们有了新的面部检测器

97
00:05:34,801 --> 0:05:39,306
现在我们可以检测到更多的面孔

98
00:05:39,806 --> 0:05:40,807
让我们看看这个例子

99
00:05:41,175 --> 0:05:43,911
在左侧 你可以看到

100
00:05:44,444 --> 0:05:47,314
如果我们使用去年的检测器

101
00:05:47,381 --> 0:05:48,782
我们只能检测到三张面孔

102
00:05:49,216 --> 0:05:52,786
这些面孔是接近直立位置的面孔

103
00:05:53,921 --> 0:05:55,222
如果你使用今年的

104
00:05:55,289 --> 0:05:57,925
面部检测器

105
00:05:57,991 --> 0:05:59,927
如你所见 所有面孔都可以检测到

106
00:05:59,993 --> 0:06:01,695
方向现在不再是问题

107
00:05:59,993 --> 0:06:01,695
方向现在不再是问题

108
00:06:04,097 --> 0:06:06,300
让我们再看一下细节

109
00:06:15,175 --> 0:06:17,978
首先 我们新的面部检测器

110
00:06:18,312 --> 0:06:20,447
使用与去年相同的API

111
00:06:20,781 --> 0:06:23,217
唯一的区别是如果你想指定修正版本

112
00:06:23,283 --> 0:06:25,719
你需要重写该请求的

113
00:06:25,786 --> 0:06:28,055
并将其明确设置为使用修正版本2

114
00:06:28,422 --> 0:06:30,591
我们将在下一张幻灯片中谈及原因

115
00:06:31,558 --> 0:06:34,127
我们还引入了两个新属性

116
00:06:34,428 --> 0:06:37,798
一个是roll

117
00:06:37,865 --> 0:06:41,034
另一个是yaw

118
00:06:45,072 --> 0:06:45,906
修正版本

119
00:06:46,473 --> 0:06:49,576
当我们需要引入新算法时

120
00:06:49,643 --> 0:06:51,512
我们不会立即弃用旧版本

121
00:06:52,045 --> 0:06:54,781
相反 我们将两种修正版本

122
00:06:54,848 --> 0:06:57,684
或者甚至可能同时发展它们

123
00:06:58,352 --> 0:07:00,454
你通过指定请求的

124
00:06:58,352 --> 0:07:00,454
你通过指定请求的

125
00:07:00,521 --> 0:07:02,689
告诉我们你要使用哪一个

126
00:07:03,991 --> 0:07:05,392
这就是显式行为

127
00:07:05,792 --> 0:07:07,361
但我们也有默认行为

128
00:07:07,761 --> 0:07:11,164
如果你创建了一个请求对象

129
00:07:11,231 --> 0:07:14,201
然后你开始处理该请求

130
00:07:14,801 --> 0:07:18,972
默认情况下 你将得到你的app

131
00:07:19,306 --> 0:07:20,974
所连接的SDK的

132
00:07:21,308 --> 0:07:23,610
最新版本

133
00:07:24,044 --> 0:07:26,380
理解这一点很重要

134
00:07:27,014 --> 0:07:30,050
假设你的app连接到去年的SDK

135
00:07:30,651 --> 0:07:32,753
去年 我们只有一个检测器

136
00:07:33,253 --> 0:07:34,955
所以这就是你将使用的检测器

137
00:07:35,455 --> 0:07:38,058
即使你将该app运行在

138
00:07:38,125 --> 0:07:39,359
而没有进行编译

139
00:07:40,527 --> 0:07:43,397
另一方面 如果你重新编译app

140
00:07:43,463 --> 0:07:47,100
使用当前的SDK

141
00:07:47,167 --> 0:07:49,937
并且你在当前操作系统上运行它

142
00:07:50,003 --> 0:07:53,040
第二个修正版本

143
00:07:54,708 --> 0:07:57,277
我们强烈建议你的app

144
00:07:57,344 --> 0:07:58,912
并引用明确的修正版本

145
00:07:59,513 --> 0:08:02,416
你将得到的

146
00:07:59,513 --> 0:08:02,416
你将得到的

147
00:08:02,916 --> 0:08:06,987
你知道要引用的算法的性能

148
00:08:07,254 --> 0:08:08,355
你知道会发生什么

149
00:08:08,755 --> 0:08:12,025
你可以通过让你的app更具前瞻性

150
00:08:12,326 --> 0:08:13,927
从而避免像这样的错误

151
00:08:13,994 --> 0:08:16,296
即若干年后

152
00:08:16,363 --> 0:08:17,631
如果我们弃用某些版本

153
00:08:18,031 --> 0:08:20,667
那么你今天就可以引用它了

154
00:08:25,506 --> 0:08:29,109
让我们更深入了解一下

155
00:08:32,078 --> 0:08:34,214
我们首先看一个

156
00:08:35,549 --> 0:08:37,183
你应该还记得图像请求处理程序

157
00:08:37,251 --> 0:08:40,687
用于处理同一图像上的

158
00:08:42,523 --> 0:08:44,958
它通过缓存一些信息

159
00:08:45,025 --> 0:08:46,960
来进行优化

160
00:08:47,227 --> 0:08:50,631
以便即将处理的连续请求

161
00:08:51,064 --> 0:08:52,232
可以使用此信息

162
00:08:53,333 --> 0:08:54,501
让我们看一下代码示例

163
00:08:56,904 --> 0:08:58,438
在我们深入研究代码示例之前

164
00:08:58,505 --> 0:08:59,907
我想强调几点

165
00:08:59,973 --> 0:09:02,509
关于此次演讲中的代码示例的问题

166
00:08:59,973 --> 0:09:02,509
关于此次演讲中的代码示例的问题

167
00:09:03,377 --> 0:09:06,380
其中错误处理的代码

168
00:09:06,446 --> 0:09:09,016
我使用了try的简短版本

169
00:09:09,650 --> 0:09:11,685
这只是为了简化示例

170
00:09:11,919 --> 0:09:14,154
当你编写app代码时

171
00:09:14,221 --> 0:09:16,056
以防出现预期外的行为

172
00:09:17,124 --> 0:09:22,729
我还在创建图像请求处理程序对象时

173
00:09:23,530 --> 0:09:27,401
它只是SSD中文件所在的位置

174
00:09:28,435 --> 0:09:29,570
现在我们来看看这个例子

175
00:09:30,737 --> 0:09:34,341
首先 我要创建一个

176
00:09:35,042 --> 0:09:37,578
然后我将创建我的

177
00:09:37,644 --> 0:09:40,581
并传入imageURL

178
00:09:40,647 --> 0:09:42,382
即保存面部图像数据的文件

179
00:09:43,250 --> 0:09:46,186
接下来 我要求requestHandler

180
00:09:46,720 --> 0:09:48,488
最后 我将查看结果

181
00:09:49,823 --> 0:09:52,826
非常简单

182
00:09:53,193 --> 0:09:54,962
我的结果会是这样的

183
00:09:59,967 --> 0:10:01,134
我得到的是什么呢？

184
00:09:59,967 --> 0:10:01,134
我得到的是什么呢？

185
00:10:01,201 --> 0:10:02,970
我得到了一个

186
00:10:03,237 --> 0:10:05,372
该对象中最重要的字段之一

187
00:10:05,439 --> 0:10:07,407
是面部所在的边界框

188
00:10:09,409 --> 0:10:11,078
让我们再看看这张幻灯片

189
00:10:11,144 --> 0:10:13,714
前三行代码几乎是你为了

190
00:10:13,780 --> 0:10:16,283
所要做的全部

191
00:10:19,486 --> 0:10:20,320
谢谢

192
00:10:22,723 --> 0:10:24,925
现在让我们看看

193
00:10:27,895 --> 0:10:29,496
如果你还记得

194
00:10:29,563 --> 0:10:34,001
用于处理特定操作

195
00:10:35,402 --> 0:10:37,304
让我们看看代码示例

196
00:10:37,371 --> 0:10:40,507
几乎是我们用Vision API

197
00:10:40,574 --> 0:10:42,376
最简单的跟踪序列

198
00:10:43,810 --> 0:10:46,146
首先 我将创建我的

199
00:10:47,414 --> 0:10:50,117
然后 我需要指定要跟踪的对象

200
00:10:50,184 --> 0:10:51,885
我通过创建一个

201
00:10:51,952 --> 0:10:53,887
DetectedObjectObservation

202
00:10:53,954 --> 0:10:57,491
它为我们提供了一个位置参数

203
00:10:58,792 --> 0:11:01,061
接下来我将开始跟踪信号

204
00:10:58,792 --> 0:11:01,061
接下来我将开始跟踪信号

205
00:11:01,695 --> 0:11:04,164
在这个例子中

206
00:11:04,231 --> 0:11:05,632
跟踪我的对象

207
00:11:06,800 --> 0:11:08,235
让我们看看序列如何工作

208
00:11:08,735 --> 0:11:10,938
首先 我有个

209
00:11:11,004 --> 0:11:13,674
你使用的可能是相机输入等

210
00:11:14,241 --> 0:11:15,542
这是我获取帧的地方

211
00:11:16,510 --> 0:11:19,112
我得到了帧 创建了请求对象

212
00:11:19,179 --> 0:11:23,884
并将DetectedObjectObservation

213
00:11:24,218 --> 0:11:26,887
这就是我在循环开始之前创建的东西

214
00:11:28,689 --> 0:11:31,959
然后 我将要求我的

215
00:11:33,660 --> 0:11:35,562
我现在要查看结果

216
00:11:35,629 --> 0:11:38,632
这就是我分析结果

217
00:11:39,233 --> 0:11:43,470
最后一步非常重要

218
00:11:43,737 --> 0:11:45,772
我从当前迭代中获取结果

219
00:11:45,839 --> 0:11:48,041
并将其传递给下一次迭代

220
00:11:48,242 --> 0:11:50,277
因此当下一个迭代请求被创建时

221
00:11:50,344 --> 0:11:51,845
我想在其中看到这些结果

222
00:11:53,580 --> 0:11:55,916
如果我在一个包含5帧的序列中

223
00:11:55,983 --> 0:11:57,818
我的结果会是这样的

224
00:12:06,393 --> 0:12:07,861
如何创建一个请求对象

225
00:12:09,496 --> 0:12:11,798
首先 了解请求有两种类型的属性

226
00:12:11,865 --> 0:12:13,300
非常重要

227
00:12:13,367 --> 0:12:16,336
即强制属性和可选属性

228
00:12:17,070 --> 0:12:19,206
强制属性 顾名思义

229
00:12:19,473 --> 0:12:21,408
它们需要通过初始化程序提供

230
00:12:21,608 --> 0:12:24,444
以便能够创建请求对象

231
00:12:25,879 --> 0:12:27,014
我们看看这个例子

232
00:12:28,448 --> 0:12:31,084
这是我们刚才在上一张幻灯片中

233
00:12:31,518 --> 0:12:33,687
一个detectedObjectObservation对象

234
00:12:33,754 --> 0:12:36,456
被传入TrackObjectRequest的

235
00:12:36,790 --> 0:12:38,625
这是一个强制属性的示例

236
00:12:40,627 --> 0:12:42,162
我们也有可选属性

237
00:12:43,363 --> 0:12:45,566
顺便说一下

238
00:12:45,766 --> 0:12:49,369
你可以在声明请求对象的地方

239
00:12:51,004 --> 0:12:53,607
可选属性是这样一组属性

240
00:12:53,674 --> 0:12:55,509
即我们为它们预设了有意义的默认值

241
00:12:55,876 --> 0:12:57,711
所以我们会为你初始化它们

242
00:12:57,778 --> 0:12:59,847
但是如果有需要

243
00:13:00,480 --> 0:13:01,582
让我们看看这个例子

244
00:13:05,786 --> 0:13:08,789
我在这里所做的是我正在创建

245
00:13:09,489 --> 0:13:12,526
如果我什么都不做

246
00:13:12,593 --> 0:13:14,928
直接传递给请求处理程序

247
00:13:14,995 --> 0:13:18,565
我会处理整个图像来寻找条形码

248
00:13:19,032 --> 0:13:20,534
然而我在这里做的是

249
00:13:20,601 --> 0:13:24,505
我将指定一小部分

250
00:13:24,872 --> 0:13:27,508
这就是我想要重点寻找条形码的地方

251
00:13:28,141 --> 0:13:30,711
并且我将覆盖

252
00:13:31,378 --> 0:13:34,448
如果我现在将该请求对象

253
00:13:34,515 --> 0:13:37,117
我将只专注于处理图像的一小部分

254
00:13:38,151 --> 0:13:40,053
这里的regionOfInterest属性

255
00:13:40,120 --> 0:13:42,656
就是我们的可选属性的示例

256
00:13:43,690 --> 0:13:45,459
这里需要重点理解的是

257
00:13:45,526 --> 0:13:49,229
一旦你手中拿到一个请求对象

258
00:13:49,296 --> 0:13:51,064
它就已经是一个完全构造的对象

259
00:13:51,498 --> 0:13:53,500
这是你可以立即开始使用的对象

260
00:13:53,567 --> 0:13:55,636
我们永远不会给你不完整的对象

261
00:13:55,702 --> 0:13:58,639
如果你稍后决定覆盖某些属性

262
00:13:58,705 --> 0:13:59,940
欢迎你这样做

263
00:14:00,007 --> 0:14:03,210
但是只要你获得这个对象

264
00:14:07,481 --> 0:14:09,283
这张幻灯片上没有说到的一件事

265
00:14:09,349 --> 0:14:11,685
其将在下一个演讲中详细介绍

266
00:14:12,586 --> 0:14:15,422
即关于边界框

267
00:14:16,156 --> 0:14:20,294
如你所见

268
00:14:20,794 --> 0:14:24,431
它们在0到1之间

269
00:14:24,998 --> 0:14:27,601
下一个演讲“CoreML

270
00:14:27,668 --> 0:14:29,670
将更详细地介绍这一方面

271
00:14:33,674 --> 0:14:35,442
让我们来看看如何理解结果

272
00:14:37,077 --> 0:14:40,280
如前所述 Vision的结果

273
00:14:41,715 --> 0:14:43,617
观察由请求对象的

274
00:14:43,684 --> 0:14:45,686
results属性填充

275
00:14:46,420 --> 0:14:47,888
你能得到多少个观察呢

276
00:14:49,790 --> 0:14:51,859
其集合可以从0到N

277
00:14:52,492 --> 0:14:53,861
这里还有另一个方面

278
00:14:53,927 --> 0:14:55,996
若得到的results

279
00:14:56,063 --> 0:14:58,298
这表示请求的泛同步失败

280
00:14:58,799 --> 0:15:01,268
这与得到0次观察不同

281
00:14:58,799 --> 0:15:01,268
这与得到0次观察不同

282
00:15:01,902 --> 0:15:03,604
得到0次观察意味着

283
00:15:03,670 --> 0:15:06,073
无论你在寻找什么

284
00:15:06,773 --> 0:15:09,510
举个例子

285
00:15:10,744 --> 0:15:12,579
如果你提供的图片中没有面孔

286
00:15:12,646 --> 0:15:14,481
很自然地 你会得到0次观察

287
00:15:15,315 --> 0:15:19,019
另一方面 如果你输入的图像中

288
00:15:19,453 --> 0:15:22,089
你将得到适当数量的观察结果

289
00:15:24,258 --> 0:15:28,629
观察的另一个重要特性是

290
00:15:28,962 --> 0:15:31,698
我们稍后会看一下使用它的示例

291
00:15:33,233 --> 0:15:35,802
还有两个我希望你注意的属性

292
00:15:35,869 --> 0:15:38,839
它们都在所有观察的基类中声明

293
00:15:39,239 --> 0:15:40,407
一个是唯一ID

294
00:15:41,041 --> 0:15:43,810
该唯一ID标识了

295
00:15:43,877 --> 0:15:46,647
创建此特定结果的处理步骤

296
00:15:47,381 --> 0:15:49,149
另一个是置信度

297
00:15:50,284 --> 0:15:51,485
置信度告诉你

298
00:15:51,552 --> 0:15:54,388
算法对该结果有多大信心

299
00:15:55,489 --> 0:15:58,525
置信度介于0到1之间

300
00:15:59,026 --> 0:16:02,696
同样 这个主题也将在下一个演讲中

301
00:15:59,026 --> 0:16:02,696
同样 这个主题也将在下一个演讲中

302
00:16:07,100 --> 0:16:08,902
让我们看一下请求管道

303
00:16:09,670 --> 0:16:10,804
什么是管道呢？

304
00:16:11,738 --> 0:16:15,375
假设我有三个请求

305
00:16:15,442 --> 0:16:17,377
取决于请求2的执行

306
00:16:17,444 --> 0:16:20,480
而请求2又取决于请求3的执行

307
00:16:22,049 --> 0:16:23,650
当处理顺序与请求顺序相反时

308
00:16:23,717 --> 0:16:26,253
我们该如何处理序列呢？

309
00:16:26,720 --> 0:16:30,324
我在这里做的是

310
00:16:30,390 --> 0:16:32,192
我将从该请求中得到结果

311
00:16:32,259 --> 0:16:33,861
并将其提供给请求2

312
00:16:34,294 --> 0:16:36,630
我将对请求2做同样的事情

313
00:16:36,697 --> 0:16:39,132
最后 我将处理我的请求1

314
00:16:41,568 --> 0:16:45,672
让我们看看如何以隐式和显式顺序

315
00:16:45,739 --> 0:16:47,641
运行请求管道的示例

316
00:16:48,208 --> 0:16:51,044
我们将在下一张幻灯片中

317
00:16:51,111 --> 0:16:53,747
并且将运行我们的人脸特征点检测器

318
00:16:54,381 --> 0:16:55,883
你可能知道

319
00:16:55,949 --> 0:16:58,452
人脸特征点是脸部的特征

320
00:16:58,519 --> 0:17:02,022
它是你的眼睛 眉毛

321
00:16:58,519 --> 0:17:02,022
它是你的眼睛 眉毛

322
00:17:02,956 --> 0:17:05,526
让我们首先看看如何隐式地

323
00:17:14,067 --> 0:17:16,737
我有一个跟我们刚才看过的

324
00:17:17,104 --> 0:17:19,473
首先 我要创建一个人脸特征点请求

325
00:17:20,507 --> 0:17:22,843
然后我将创建图像请求处理程序

326
00:17:23,810 --> 0:17:25,412
接下来我将处理该请求

327
00:17:26,113 --> 0:17:27,848
最后 我将查看结果

328
00:17:28,949 --> 0:17:30,751
如果我的图像中有一张脸

329
00:17:31,218 --> 0:17:33,020
我的结果会是这样的

330
00:17:35,422 --> 0:17:37,491
抱歉

331
00:17:39,226 --> 0:17:40,060
这就是结果

332
00:17:40,727 --> 0:17:42,529
我得到了脸部的边界框

333
00:17:42,963 --> 0:17:47,167
这是脸部所在的位置

334
00:17:48,535 --> 0:17:49,970
这里需要重点理解的是

335
00:17:50,037 --> 0:17:53,073
当开始处理人脸特征点请求时

336
00:17:53,574 --> 0:17:57,811
人脸特征点请求发现

337
00:17:58,612 --> 0:18:01,548
它将会替我们执行面部检测器

338
00:17:58,612 --> 0:18:01,548
它将会替我们执行面部检测器

339
00:18:02,049 --> 0:18:04,351
它从面部检测器获得结果

340
00:18:04,418 --> 0:18:06,720
而这就是搜索人脸特征点的地方

341
00:18:09,223 --> 0:18:10,858
在右侧 你可以看到

342
00:18:12,960 --> 0:18:15,662
观察对象的样子

343
00:18:15,729 --> 0:18:17,631
这些只是该对象的几个字段

344
00:18:17,698 --> 0:18:20,968
一个是我们讨论过的uuid

345
00:18:21,668 --> 0:18:24,471
然后是boundingBox

346
00:18:24,972 --> 0:18:26,940
最后是landmarks字段

347
00:18:27,007 --> 0:18:29,843
它指向描述特征点的某个对象

348
00:18:33,780 --> 0:18:38,352
现在让我们看一下相同的案例

349
00:18:40,621 --> 0:18:42,189
我首先要做的是

350
00:18:42,256 --> 0:18:44,491
我将显式地运行我的面部检测器

351
00:18:46,460 --> 0:18:48,328
你已经在演示文稿中

352
00:18:48,395 --> 0:18:49,730
多次看过这四行代码

353
00:18:49,796 --> 0:18:51,798
当我运行它时 我将得到边界框

354
00:18:52,499 --> 0:18:56,603
如你所见 结果以相同的类型返回

355
00:18:57,204 --> 0:19:00,240
我们在上一张幻灯片中看到的字段

356
00:18:57,204 --> 0:19:00,240
我们在上一张幻灯片中看到的字段

357
00:19:00,307 --> 0:19:03,544
你有一个唯一的数字来标识

358
00:19:04,144 --> 0:19:05,679
然后是边界框的位置

359
00:19:05,746 --> 0:19:08,715
这是处理此请求的主要结果

360
00:19:09,183 --> 0:19:11,952
而landmarks字段被设置为nil

361
00:19:12,019 --> 0:19:13,554
对人脸特征点一无所知

362
00:19:14,755 --> 0:19:18,058
接下来我要做的是

363
00:19:18,492 --> 0:19:20,961
然后我将使用上一步的结果

364
00:19:21,028 --> 0:19:24,565
并将其传入该请求的

365
00:19:25,933 --> 0:19:28,402
然后 我将要求请求处理程序处理它

366
00:19:29,236 --> 0:19:31,004
最后 我将查看结果

367
00:19:31,605 --> 0:19:33,173
如果我在同一张图片上运行它

368
00:19:33,240 --> 0:19:37,778
我得到的结果

369
00:19:38,679 --> 0:19:40,480
但让我们看看观察有何不同

370
00:19:40,881 --> 0:19:43,050
记住 我们说过观察是不可变的

371
00:19:43,550 --> 0:19:47,621
尽管面部检测器和人脸特征点检测器

372
00:19:47,688 --> 0:19:50,691
返回的是相同的类型

373
00:19:50,757 --> 0:19:52,659
被传入的观察

374
00:19:53,193 --> 0:19:55,963
我们所做的是将前两个字段

375
00:19:56,029 --> 0:19:57,631
复制到一个新对象中

376
00:19:58,265 --> 0:20:00,801
然后计算特征点

377
00:19:58,265 --> 0:20:00,801
然后计算特征点

378
00:20:02,202 --> 0:20:06,340
现在如果你看一下两者的uuid

379
00:20:07,140 --> 0:20:09,977
为什么会这样？

380
00:20:10,043 --> 0:20:12,446
它具有相同的处理步骤

381
00:20:14,147 --> 0:20:16,517
你分别应该在何时使用隐式与显式呢

382
00:20:17,251 --> 0:20:19,453
如果你的app非常简单

383
00:20:19,520 --> 0:20:22,256
你可能希望选择隐式方式

384
00:20:22,322 --> 0:20:24,091
它很简单 你创建一个请求

385
00:20:24,157 --> 0:20:25,926
其它一切都自动替你完成

386
00:20:28,595 --> 0:20:31,398
另一方面 如果你的app比较复杂

387
00:20:31,465 --> 0:20:34,501
例如 你想先处理面部

388
00:20:34,902 --> 0:20:35,903
然后做一些过滤

389
00:20:35,969 --> 0:20:38,205
假设你不关心周边的面孔

390
00:20:38,272 --> 0:20:41,074
或者你只想专注于中间的那些

391
00:20:41,875 --> 0:20:43,010
你可以做那一步

392
00:20:43,076 --> 0:20:46,446
然后你可以在剩下的一组面孔上

393
00:20:47,147 --> 0:20:49,550
在这种情况下

394
00:20:50,551 --> 0:20:51,518
因为在这种情况下

395
00:20:51,952 --> 0:20:55,322
特征点检测器不会重新运行

396
00:21:02,529 --> 0:21:04,665
我们希望你的app

397
00:21:04,731 --> 0:21:07,534
在内存使用和执行速度方面

398
00:21:07,601 --> 0:21:09,570
因此接下来的两张幻灯片很重要

399
00:21:11,905 --> 0:21:13,740
你应该将对象保存在内存中多久

400
00:21:16,944 --> 0:21:18,478
对于图像请求处理程序

401
00:21:18,545 --> 0:21:21,481
只要图像还需要处理

402
00:21:22,282 --> 0:21:24,952
这听起来像是一个

403
00:21:25,219 --> 0:21:27,254
但你做到这一点非常重要

404
00:21:28,188 --> 0:21:29,656
如果你过早释放该对象

405
00:21:29,723 --> 0:21:32,159
而你仍有待处理的未完成请求

406
00:21:32,226 --> 0:21:34,394
你将必须重新创建图像请求处理程序

407
00:21:34,661 --> 0:21:36,597
但现在你已经丢失了所有

408
00:21:36,663 --> 0:21:38,465
与前一个对象关联的缓存

409
00:21:38,665 --> 0:21:42,035
因此你必须花费这些开销

410
00:21:43,837 --> 0:21:46,039
另一方面 如果你释放得太晚了

411
00:21:46,473 --> 0:21:49,276
首先这会导致内存碎片

412
00:21:49,743 --> 0:21:52,946
并且你的app将无法回收内存

413
00:21:53,013 --> 0:21:54,848
来做你想要做的其它有意义的事情

414
00:21:56,416 --> 0:21:57,751
所以释放它很重要

415
00:21:57,818 --> 0:22:00,153
只要你需要就使用它

416
00:21:57,818 --> 0:22:00,153
只要你需要就使用它

417
00:22:00,687 --> 0:22:04,224
请记住 它会缓存图像

418
00:22:06,360 --> 0:22:08,595
序列请求处理程序的情况

419
00:22:08,662 --> 0:22:10,764
与其非常相似

420
00:22:10,831 --> 0:22:12,299
如果你过早释放

421
00:22:12,366 --> 0:22:14,067
你几乎作废了整个序列

422
00:22:14,134 --> 0:22:16,170
因为整个缓存现在已经消失了

423
00:22:18,539 --> 0:22:20,807
请求和观察又会怎么样呢？

424
00:22:21,675 --> 0:22:25,412
请求和观察是非常轻量级的对象

425
00:22:25,479 --> 0:22:27,347
你可以根据需要创建和释放它们

426
00:22:27,414 --> 0:22:28,382
无需缓存它们

427
00:22:35,822 --> 0:22:37,324
我们应该在何处处理你的请求

428
00:22:39,860 --> 0:22:43,430
Vision中的许多请求都依赖于

429
00:22:44,464 --> 0:22:49,203
众所周知 在GPU上运行

430
00:22:51,805 --> 0:22:53,907
所以很自然的问题是

431
00:22:55,742 --> 0:22:59,746
我们在Vision中是这样做的

432
00:22:59,813 --> 0:23:01,248
我们会先尝试这样做

433
00:22:59,813 --> 0:23:01,248
我们会先尝试这样做

434
00:23:01,949 --> 0:23:05,052
如果GPU在那个时间点

435
00:23:05,118 --> 0:23:06,386
我们将切换到CPU

436
00:23:07,955 --> 0:23:09,389
因为这是我们的默认行为

437
00:23:10,991 --> 0:23:13,927
但假设你的app需要

438
00:23:13,994 --> 0:23:16,063
在屏幕上显示大量图片

439
00:23:16,129 --> 0:23:19,433
因此你可能希望为该特定作业

440
00:23:20,133 --> 0:23:23,003
在这种情况下 你可以将请求对象的

441
00:23:23,070 --> 0:23:25,105
usesCPUOnly属性

442
00:23:25,439 --> 0:23:28,275
这将告诉我们直接在CPU上

443
00:23:35,282 --> 0:23:38,886
我们已经介绍了如何与Vision

444
00:23:38,952 --> 0:23:42,055
交互的基础知识

445
00:23:42,623 --> 0:23:44,591
让我们切换到这次演讲的主题

446
00:23:44,658 --> 0:23:46,426
即在Vision中进行跟踪

447
00:23:48,862 --> 0:23:52,466
什么是跟踪呢？

448
00:23:52,533 --> 0:23:55,702
在一系列帧中找到感兴趣的对象

449
00:23:56,069 --> 0:23:58,172
通常 你在第一帧中找到该对象

450
00:23:58,238 --> 0:24:01,208
并在接下来尝试在帧序列中查找它

451
00:23:58,238 --> 0:24:01,208
并在接下来尝试在帧序列中查找它

452
00:24:02,142 --> 0:24:03,944
此类app的例子有什么呢？

453
00:24:04,511 --> 0:24:06,213
你可能已经看过很多了

454
00:24:06,813 --> 0:24:10,284
比如实时评注体育赛事

455
00:24:10,350 --> 0:24:12,786
相机的跟踪对焦

456
00:24:15,422 --> 0:24:17,291
你可能会说如果我能对序列中的

457
00:24:17,357 --> 0:24:19,993
每一帧进行检测

458
00:24:20,827 --> 0:24:22,529
原因有多种

459
00:24:23,063 --> 0:24:25,966
首先 你可能没有一个特定的跟踪器

460
00:24:26,033 --> 0:24:28,235
来跟踪你想跟踪的每种类型的对象

461
00:24:28,869 --> 0:24:31,305
假设你想跟踪面孔

462
00:24:31,772 --> 0:24:33,440
因为你可以使用面部检测器

463
00:24:34,041 --> 0:24:37,110
但如果你需要跟踪特定品种的鸟

464
00:24:37,811 --> 0:24:39,646
你可能没有那个检测器

465
00:24:39,713 --> 0:24:42,382
现在你必须创建那个特定的检测器

466
00:24:42,449 --> 0:24:46,386
你可能不想这样做

467
00:24:46,453 --> 0:24:48,589
因为你的app还有别的事情要处理

468
00:24:50,390 --> 0:24:52,759
但我们假设你很幸运

469
00:24:52,993 --> 0:24:54,394
那你应该使用检测器吗

470
00:24:55,128 --> 0:24:57,364
也许在这种情况下也不应该

471
00:24:58,065 --> 0:24:59,333
让我们看看这个例子

472
00:25:00,501 --> 0:25:01,802
你开始跟踪序列

473
00:25:01,869 --> 0:25:03,737
并在第一帧上运行面部检测器

474
00:25:04,137 --> 0:25:05,372
你得到了五张脸

475
00:25:06,073 --> 0:25:09,042
然后在第二帧中再运行它

476
00:25:09,576 --> 0:25:12,279
你怎么知道第二帧中的面孔

477
00:25:12,346 --> 0:25:14,748
就是第一帧中的那些面孔呢？

478
00:25:15,182 --> 0:25:17,651
有可能一个人退出了画面

479
00:25:19,253 --> 0:25:22,689
所以现在你必须匹配你找到的对象

480
00:25:23,023 --> 0:25:26,026
这是一个你可能不想处理的

481
00:25:27,060 --> 0:25:28,462
另一方面

482
00:25:30,364 --> 0:25:32,399
跟踪器使用重要信息来匹配对象

483
00:25:32,466 --> 0:25:34,601
它们知道轨迹

484
00:25:34,668 --> 0:25:37,738
并且它们可以稍微预测

485
00:25:39,573 --> 0:25:40,841
但假设你非常幸运

486
00:25:41,708 --> 0:25:46,013
你正在跟踪面孔 并且你的用例

487
00:25:46,079 --> 0:25:47,581
那你应该使用检测器吗

488
00:25:48,215 --> 0:25:50,217
即便在这种情况下也未必

489
00:25:56,490 --> 0:25:57,658
现在速度是一个问题

490
00:25:58,058 --> 0:26:00,561
跟踪器通常是轻量级算法

491
00:25:58,058 --> 0:26:00,561
跟踪器通常是轻量级算法

492
00:26:00,627 --> 0:26:03,630
而探测器通常会

493
00:26:03,697 --> 0:26:04,698
这会花费更长时间

494
00:26:05,999 --> 0:26:09,436
此外 如果你需要在图形用户界面上

495
00:26:09,503 --> 0:26:11,438
显示跟踪信息

496
00:26:11,505 --> 0:26:13,774
跟踪器更平滑而且不那么抖动

497
00:26:17,244 --> 0:26:20,113
还记得吗 在第一张幻灯片中

498
00:26:23,217 --> 0:26:25,018
这三个词

499
00:26:25,853 --> 0:26:29,223
让我们看看它们如何

500
00:26:30,958 --> 0:26:31,792
首先

501
00:26:32,993 --> 0:26:33,827
是请求

502
00:26:34,361 --> 0:26:37,731
在Vision中

503
00:26:38,165 --> 0:26:40,133
一个是通用的对象跟踪器

504
00:26:40,200 --> 0:26:42,035
另一个是矩形对象跟踪器

505
00:26:42,936 --> 0:26:43,770
如何？

506
00:26:44,505 --> 0:26:45,906
正如你现在应该猜到的那样

507
00:26:45,973 --> 0:26:48,075
我们将使用序列请求处理程序

508
00:26:49,743 --> 0:26:50,577
结果

509
00:26:51,578 --> 0:26:53,247
这里有两种重要的类型

510
00:26:53,313 --> 0:26:56,950
一个是DetectedObjectObservation

511
00:26:57,017 --> 0:26:59,453
即boundingBox

512
00:27:00,287 --> 0:27:02,456
还有一个RectangleObservation

513
00:27:02,990 --> 0:27:04,958
其有四个额外的属性

514
00:27:05,025 --> 0:27:07,127
来表示矩形的顶点

515
00:27:07,861 --> 0:27:09,396
现在你可能会说

516
00:27:09,463 --> 0:27:11,331
为什么还需要矩形的顶点呢？

517
00:27:12,332 --> 0:27:14,234
当你跟踪矩形时

518
00:27:14,735 --> 0:27:16,937
它们是现实生活中的矩形物体

519
00:27:17,004 --> 0:27:19,640
当它们投射到帧中时

520
00:27:20,073 --> 0:27:22,075
例如 它们可能看起来像梯形

521
00:27:23,510 --> 0:27:26,747
所以这种情况下的边界框

522
00:27:26,813 --> 0:27:28,182
而是包含矩形所有顶点的

523
00:27:28,248 --> 0:27:30,184
最小的框

524
00:27:33,720 --> 0:27:34,855
现在让我们看看演示

525
00:27:36,290 --> 0:27:37,958
（演示

526
00:27:47,401 --> 0:27:50,037
我这里有一个示例app

527
00:27:50,103 --> 0:27:52,706
你可以从WWDC网站下载它

528
00:27:52,773 --> 0:27:54,741
链接就在此演讲旁边

529
00:27:55,442 --> 0:27:58,011
该app可以处理一个视频

530
00:27:59,479 --> 0:28:01,315
它将该视频解析为帧

531
00:27:59,479 --> 0:28:01,315
它将该视频解析为帧

532
00:28:01,982 --> 0:28:03,617
你在第一帧中选择一个对象

533
00:28:03,684 --> 0:28:06,954
你想跟踪多个对象

534
00:28:08,555 --> 0:28:09,923
首先 让我们使用这个视频

535
00:28:11,758 --> 0:28:12,993
用户界面很简单

536
00:28:13,393 --> 0:28:15,529
首先 你可以选择对象或矩形

537
00:28:15,729 --> 0:28:19,499
其次 你可以选择要使用的算法

538
00:28:19,833 --> 0:28:22,169
Vision中会发生什么？

539
00:28:23,103 --> 0:28:27,007
快速和准确

540
00:28:28,876 --> 0:28:32,012
在这个例子中我将显示对象

541
00:28:32,779 --> 0:28:33,847
让我们选择对象

542
00:28:35,082 --> 0:28:38,685
我要跟踪红伞下的这个人

543
00:28:38,752 --> 0:28:40,988
我还将尝试追踪这群人

544
00:28:45,893 --> 0:28:46,827
让我们运行它

545
00:28:54,601 --> 0:28:59,640
如你所见

546
00:29:06,947 --> 0:29:09,449
让我们看一个更复杂的例子

547
00:29:10,684 --> 0:29:13,453
我想在这里做的是

548
00:29:14,688 --> 0:29:16,723
在这个例子中

549
00:29:17,891 --> 0:29:19,226
我先选择该对象

550
00:29:23,297 --> 0:29:24,298
然后运行它

551
00:29:30,337 --> 0:29:34,675
正如你所看到的

552
00:29:34,741 --> 0:29:37,244
它的形状 位置 颜色

553
00:29:37,311 --> 0:29:40,013
我们仍然可以跟踪它

554
00:29:47,821 --> 0:29:49,289
现在我们将切换到我的演示机

555
00:29:49,356 --> 0:29:52,626
并查看实际跟踪序列是如何

556
00:30:01,168 --> 0:30:04,705
我启动了Xcode

557
00:30:04,771 --> 0:30:07,941
它上面运行了

558
00:30:09,343 --> 0:30:10,978
我将在调试器中运行它

559
00:30:16,583 --> 0:30:17,885
我要选择对象

560
00:30:19,219 --> 0:30:20,821
我选择什么并不重要

561
00:30:20,888 --> 0:30:22,623
因为我们只想查看序列

562
00:30:23,323 --> 0:30:24,358
然后运行它

563
00:30:26,727 --> 0:30:29,329
我在这里有一个断点

564
00:30:29,396 --> 0:30:31,965
它在performTracking函数中中断

565
00:30:32,032 --> 0:30:34,301
这是此app中最重要的函数

566
00:30:34,601 --> 0:30:36,837
这是实现实际序列的函数

567
00:30:38,438 --> 0:30:39,706
让我们看看这里做了什么

568
00:30:40,340 --> 0:30:43,544
首先 我们创建了一个

569
00:30:44,011 --> 0:30:46,780
然后我们读取第一帧

570
00:30:46,847 --> 0:30:48,949
因为该帧用于选择对象

571
00:30:50,284 --> 0:30:51,818
这里是取消标志

572
00:30:52,753 --> 0:30:56,390
然后我将初始化

573
00:30:56,456 --> 0:30:58,425
记住我们在幻灯片中看到的例子

574
00:31:01,328 --> 0:31:04,231
接下来我设置了簿记

575
00:31:04,298 --> 0:31:05,632
显示结果

576
00:31:06,466 --> 0:31:09,670
它被保存在trackedPolyRect类型中

577
00:31:11,038 --> 0:31:13,574
然后我在type上

578
00:31:13,941 --> 0:31:16,443
type来自用户界面

579
00:31:16,510 --> 0:31:18,412
这个例子中

580
00:31:20,080 --> 0:31:22,115
我们选择了两个对象

581
00:31:23,283 --> 0:31:26,119
这是来自用户界面的信息

582
00:31:26,453 --> 0:31:28,322
我们应该能在这里看到这两个对象

583
00:31:31,859 --> 0:31:36,096
好的 的确有两个

584
00:31:36,330 --> 0:31:38,532
它将初始化

585
00:31:38,599 --> 0:31:40,434
它会根据传入的

586
00:31:40,501 --> 0:31:42,202
创建DetectedObjectObservation

587
00:31:43,437 --> 0:31:45,072
正如幻灯片中显示的那样

588
00:31:46,206 --> 0:31:49,743
并且我们初始化我们的簿记结构

589
00:31:50,043 --> 0:31:50,878
让我们运行它

590
00:31:58,452 --> 0:32:00,087
让我们看一下观察对象

591
00:31:58,452 --> 0:32:00,087
让我们看一下观察对象

592
00:32:08,028 --> 0:32:09,997
这里有几个重要的字段

593
00:32:10,063 --> 0:32:12,032
这是我们讨论过的唯一ID

594
00:32:12,799 --> 0:32:15,602
这是我们在标准化坐标中的边界框

595
00:32:17,971 --> 0:32:20,841
如果我一直运行下来

596
00:32:21,108 --> 0:32:23,911
因为这个例子中我们没有使用它

597
00:32:26,346 --> 0:32:28,916
这是我创建序列请求处理程序的地方

598
00:32:30,884 --> 0:32:32,286
这是我的帧计数器

599
00:32:33,854 --> 0:32:35,622
这是如果出现问题时的标志

600
00:32:36,623 --> 0:32:38,926
现在我终于要开始跟踪序列了

601
00:32:39,493 --> 0:32:41,328
正如你所看到的

602
00:32:41,728 --> 0:32:43,463
退出该循环的条件是

603
00:32:43,530 --> 0:32:47,000
用户请求取消或者视频已经结束

604
00:32:51,271 --> 0:32:53,574
我要初始化

605
00:32:54,875 --> 0:32:56,009
rects结构

606
00:32:56,076 --> 0:32:58,946
来保存稍后要显示的

607
00:32:59,012 --> 0:33:02,850
图形用户界面的信息

608
00:32:59,012 --> 0:33:02,850
图形用户界面的信息

609
00:33:03,250 --> 0:33:05,619
inputObservations

610
00:33:06,386 --> 0:33:08,755
对于每一个

611
00:33:15,095 --> 0:33:18,198
我将该request追加到

612
00:33:18,265 --> 0:33:19,900
在这个例子中我们必须这样做

613
00:33:22,536 --> 0:33:26,707
现在我跳出了循环

614
00:33:27,574 --> 0:33:29,209
如你看下perform函数

615
00:33:29,276 --> 0:33:32,446
它接收一个请求集合

616
00:33:33,113 --> 0:33:36,016
在幻灯片中 我们只将一个请求

617
00:33:36,083 --> 0:33:37,885
传递给该集合

618
00:33:37,951 --> 0:33:40,621
但在这里我们将同时跟踪两个请求

619
00:33:41,321 --> 0:33:42,589
我将执行它

620
00:33:44,591 --> 0:33:49,229
现在既然已经执行了请求

621
00:33:49,296 --> 0:33:52,065
为此我将查看

622
00:33:54,968 --> 0:33:56,570
我将获取results属性

623
00:33:56,637 --> 0:33:59,773
并获取该属性的第一个对象

624
00:33:59,840 --> 0:34:02,776
它是其中唯一的观察

625
00:33:59,840 --> 0:34:02,776
它是其中唯一的观察

626
00:34:03,710 --> 0:34:04,711
我在这里做的是

627
00:34:04,778 --> 0:34:08,248
我要查看observation的

628
00:34:08,748 --> 0:34:11,284
并且我将阈值随意设置为0.5

629
00:34:12,018 --> 0:34:13,453
所以如果它高于阈值

630
00:34:13,520 --> 0:34:16,822
我将用实线绘制边框

631
00:34:17,157 --> 0:34:19,860
如果它低于阈值

632
00:34:19,927 --> 0:34:23,330
因此我可以看出是否出了问题

633
00:34:26,366 --> 0:34:27,634
剩下的就是简单簿记过程

634
00:34:27,701 --> 0:34:31,004
我只是填充该rects结构

635
00:34:32,072 --> 0:34:34,308
这是最后一步 它非常重要

636
00:34:34,373 --> 0:34:36,710
我从当前迭代中获取观察

637
00:34:36,777 --> 0:34:39,913
并将其传递给下一次迭代

638
00:34:43,784 --> 0:34:45,385
我再执行一次

639
00:34:45,985 --> 0:34:47,754
我运行到这个断点

640
00:34:49,156 --> 0:34:50,389
这里会显示我的帧

641
00:34:51,859 --> 0:34:54,194
并睡眠frameRateInSeconds

642
00:34:54,261 --> 0:34:55,762
来模拟实际的视频

643
00:34:56,730 --> 0:34:58,365
然后很快

644
00:34:58,432 --> 0:35:01,068
你就开始了跟踪序列的第二次迭代

645
00:34:58,432 --> 0:35:01,068
你就开始了跟踪序列的第二次迭代

646
00:35:03,971 --> 0:35:05,339
现在让我们回到幻灯片

647
00:35:11,512 --> 0:35:12,546
谢谢

648
00:35:18,886 --> 0:35:21,788
我们看一下刚看到的东西中

649
00:35:23,056 --> 0:35:26,960
首先是如何初始化跟踪的初始对象

650
00:35:27,961 --> 0:35:28,896
我们看到了两种方式

651
00:35:28,962 --> 0:35:30,764
有自动方式

652
00:35:30,831 --> 0:35:33,934
它通常通过运行某些探测器

653
00:35:34,635 --> 0:35:37,571
第二种是手动方式

654
00:35:53,153 --> 0:35:56,823
我们还看到我们为每个跟踪对象

655
00:35:57,324 --> 0:35:58,525
都使用了一个跟踪请求

656
00:35:58,792 --> 0:36:00,460
这里的关系是一对一的

657
00:35:58,792 --> 0:36:00,460
这里的关系是一对一的

658
00:36:03,630 --> 0:36:05,499
我们还看到两种类型的跟踪器

659
00:36:05,732 --> 0:36:07,301
一种是通用跟踪器

660
00:36:07,568 --> 0:36:09,570
另一种是矩形对象跟踪器

661
00:36:12,639 --> 0:36:15,909
我们还了解到每种跟踪器类型

662
00:36:16,343 --> 0:36:17,845
即快速算法或准确算法

663
00:36:17,911 --> 0:36:21,682
它代表了速度和准确性之间的权衡

664
00:36:23,784 --> 0:36:27,187
最后但同样重要的是

665
00:36:27,921 --> 0:36:30,924
来判断我们是否应该相信我们的结果

666
00:36:34,461 --> 0:36:37,664
在Vision中实施跟踪序列

667
00:36:41,201 --> 0:36:43,303
首先我们来谈谈跟踪器的数量

668
00:36:45,372 --> 0:36:47,708
你可以同时跟踪多少个对象

669
00:36:48,408 --> 0:36:53,814
在Vision中我们有一个限制

670
00:36:53,881 --> 0:36:56,583
所以你可拥有16个通用对象跟踪器

671
00:36:56,650 --> 0:36:59,453
和16个矩形对象跟踪器

672
00:37:00,554 --> 0:37:02,956
如果你尝试分配更多

673
00:37:03,891 --> 0:37:07,060
如果发生这种情况

674
00:37:07,127 --> 0:37:08,996
你正在使用的一些跟踪器

675
00:37:09,396 --> 0:37:10,330
这该怎么做呢？

676
00:37:12,466 --> 0:37:15,769
第一种方法是你可以

677
00:37:16,236 --> 0:37:19,706
并将该请求提供给请求处理程序

678
00:37:20,240 --> 0:37:21,975
这样 请求处理程序就会知道

679
00:37:22,042 --> 0:37:24,545
与此请求对象关联的跟踪器

680
00:37:24,611 --> 0:37:25,612
应该被释放

681
00:37:26,280 --> 0:37:29,316
另一种方法是释放整个

682
00:37:29,383 --> 0:37:32,886
在这种情况下

683
00:37:32,953 --> 0:37:33,954
都将被释放

684
00:37:38,892 --> 0:37:41,195
现在假设你已经实现了跟踪序列

685
00:37:41,261 --> 0:37:43,564
你可能面临的潜在挑战是什么？

686
00:37:44,364 --> 0:37:47,301
正如你所见

687
00:37:47,367 --> 0:37:49,937
几乎可以改变自己的一切

688
00:37:50,003 --> 0:37:52,873
它们可以改变自己的

689
00:37:53,340 --> 0:37:55,776
这对算法来说是一个很大的挑战

690
00:37:56,510 --> 0:37:57,678
那么你能做什么呢？

691
00:37:58,545 --> 0:38:01,048
一个不幸的答案是

692
00:37:58,545 --> 0:38:01,048
一个不幸的答案是

693
00:38:01,114 --> 0:38:03,851
适合所有的解决方案

694
00:38:04,151 --> 0:38:05,919
首先 分别尝试快速和准确算法

695
00:38:05,986 --> 0:38:09,189
从而弄清楚你的特定用例

696
00:38:09,256 --> 0:38:11,325
更适合使用哪种算法

697
00:38:15,162 --> 0:38:17,264
如果由你来手动选择边界框

698
00:38:17,798 --> 0:38:21,068
请尝试在场景中找到一个明显的对象

699
00:38:22,870 --> 0:38:24,338
该使用多大的置信度阈值

700
00:38:25,239 --> 0:38:26,940
同样 这里也没有唯一的标准

701
00:38:27,174 --> 0:38:29,643
你会发现某些用例可以使用某些阈值

702
00:38:29,710 --> 0:38:32,746
而其它用例只能使用其它阈值

703
00:38:34,948 --> 0:38:36,984
还有一种我推荐的技术

704
00:38:37,050 --> 0:38:38,719
假设你有一个很长的跟踪序列

705
00:38:38,785 --> 0:38:40,921
举个例子 它有1000帧

706
00:38:42,155 --> 0:38:44,224
如果你开始跟踪序列

707
00:38:44,525 --> 0:38:47,828
你在第一帧中选择的对象将开始偏离

708
00:38:47,895 --> 0:38:53,567
你离开那个初始帧越远

709
00:38:54,234 --> 0:38:57,838
你可以做的是把那个序列

710
00:38:57,905 --> 0:38:59,139
比如每个50帧

711
00:38:59,640 --> 0:39:02,943
你运行检测器

712
00:38:59,640 --> 0:39:02,943
你运行检测器

713
00:39:03,343 --> 0:39:06,246
你重新运行检测器

714
00:39:06,313 --> 0:39:07,614
不断重复这个过程

715
00:39:08,182 --> 0:39:10,317
从终端用户的角度来看

716
00:39:10,384 --> 0:39:12,786
它看起来就像是在跟踪一个对象

717
00:39:13,687 --> 0:39:16,557
然而你在内部所做的是

718
00:39:16,623 --> 0:39:18,158
你正在跟踪较小的序列

719
00:39:18,225 --> 0:39:21,495
这是一种更智能的

720
00:39:27,234 --> 0:39:28,936
让我们总结一下今天的内容

721
00:39:30,237 --> 0:39:32,439
首先我们讨论了

722
00:39:32,739 --> 0:39:34,875
我们谈到它是一个多平台框架

723
00:39:35,209 --> 0:39:38,745
并且是面向隐私的

724
00:39:41,381 --> 0:39:43,250
其次 我们谈到了新特性

725
00:39:43,517 --> 0:39:47,154
我们推出了一种新的

726
00:39:48,021 --> 0:39:49,556
我们还谈到了修正版本

727
00:39:51,225 --> 0:39:54,161
接下来我们讨论了

728
00:39:54,461 --> 0:39:58,565
并讨论了请求

729
00:39:59,933 --> 0:40:03,804
最后 如何在Vision中

730
00:39:59,933 --> 0:40:03,804
最后 如何在Vision中

731
00:40:05,672 --> 0:40:10,043
若想了解更多信息

732
00:40:10,477 --> 0:40:12,846
我也建议你留下来参加下一场演讲

733
00:40:12,913 --> 0:40:14,548
下午三点钟 在这个会议室

734
00:40:14,615 --> 0:40:17,551
Frank将介绍Vision

735
00:40:17,784 --> 0:40:20,554
如果你想部署自己的模型

736
00:40:21,154 --> 0:40:24,892
该演讲还将介绍

737
00:40:24,958 --> 0:40:26,693
这次演讲中没有谈到的细节

738
00:40:27,561 --> 0:40:30,497
明天下午三到五点

739
00:40:31,131 --> 0:40:33,934
谢谢大家 愿你们在WWDC
