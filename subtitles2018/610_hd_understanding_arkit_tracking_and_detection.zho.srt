1
00:00:07,516 --> 0:00:16,500
[ 音乐 ]

2
00:00:19,516 --> 0:00:25,500
[ 掌声 ]

3
00:00:26,456 --> 0:00:27,606
&gt;&gt; 大家好

4
00:00:27,986 --> 0:00:29,526
很高兴今天能在这里

5
00:00:29,526 --> 0:00:31,446
与大家探讨

6
00:00:31,446 --> 0:00:34,326
ARKit 的跟踪与检测功能

7
00:00:34,326 --> 0:00:36,496
将如何创造出更好

8
00:00:36,496 --> 0:00:39,576
的 AR（增强现实）体验

9
00:00:40,596 --> 0:00:42,316
我是 Marion

10
00:00:42,346 --> 0:00:43,266
我来自 ARKit 团队

11
00:00:43,266 --> 0:00:44,756
你呢

12
00:00:46,126 --> 0:00:49,206
你是一位对 ARKit 的发展有着

13
00:00:49,206 --> 0:00:50,736
浓厚兴趣的

14
00:00:50,736 --> 0:00:51,276
资深 ARKit 开发者吗

15
00:00:51,896 --> 0:00:53,316
那么这个演讲就是为你准备的

16
00:00:53,316 --> 0:00:56,596
刚刚接触 ARKit

17
00:00:57,546 --> 0:00:59,606
那从这次演讲中 你将会学到

18
00:00:59,606 --> 0:01:01,046
不同的跟踪技巧以及

19
00:00:59,606 --> 0:01:01,046
不同的跟踪技巧以及

20
00:01:01,206 --> 0:01:02,876
一些 AR 常用的

21
00:01:02,966 --> 0:01:04,676
基本知识与术语

22
00:01:04,676 --> 0:01:06,756
这些基本知识与术语

23
00:01:06,756 --> 0:01:08,176
能帮助你创造属于你

24
00:01:08,176 --> 0:01:10,016
自己的第一份 AR 的体验

25
00:01:10,726 --> 0:01:14,586
让我们开始吧

26
00:01:15,106 --> 0:01:17,926
跟踪指的是什么

27
00:01:18,066 --> 0:01:19,536
跟踪将提供你的相机

28
00:01:19,836 --> 0:01:22,906
在真实世界中

29
00:01:23,156 --> 0:01:24,956
观察位置与方向

30
00:01:25,376 --> 0:01:26,806
有了这些位置与方向

31
00:01:26,806 --> 0:01:29,036
你就可以在你的相机中

32
00:01:29,036 --> 0:01:30,096
加入增强视觉的因素

33
00:01:31,146 --> 0:01:33,656
比如在这个视频中

34
00:01:33,696 --> 0:01:35,836
在真实的物理平台上的

35
00:01:35,836 --> 0:01:39,206
桌子和椅子

36
00:01:39,926 --> 0:01:41,416
是增强视觉的虚拟内容

37
00:01:42,756 --> 0:01:44,006
顺便说一下 这个是宜家

38
00:01:45,296 --> 0:01:47,186
需要注意的是 视觉内容

39
00:01:47,186 --> 0:01:48,926
从视觉上看都是正确的

40
00:01:49,746 --> 0:01:52,086
正确的放置位置 正确的大小

41
00:01:52,686 --> 0:01:54,516
以及正确的透视角度

42
00:01:55,066 --> 0:01:56,836
因此不同的跟踪

43
00:01:56,836 --> 0:01:58,906
技术能为相机

44
00:01:59,076 --> 0:02:01,586
提供不同的参考系

45
00:01:59,076 --> 0:02:01,586
提供不同的参考系

46
00:02:01,816 --> 0:02:03,556
这意味着相机相对于

47
00:02:03,556 --> 0:02:05,246
你的世界参考系

48
00:02:05,246 --> 0:02:07,916
相机相对于图像或者 3D 物品的参考

49
00:02:09,086 --> 0:02:11,186
在接下来这一个小时里

50
00:02:11,296 --> 0:02:12,396
我们将会讨论

51
00:02:12,396 --> 0:02:14,116
不同类型的跟踪技术

52
00:02:14,936 --> 0:02:16,476
这样子你就可以

53
00:02:16,476 --> 0:02:17,626
针对你的特定用例

54
00:02:17,626 --> 0:02:18,796
做出正确的选择

55
00:02:19,216 --> 0:02:22,296
我们将会讨论

56
00:02:22,296 --> 0:02:24,196
现在已有的 AR 技术

57
00:02:24,486 --> 0:02:26,726
这包括 方向跟踪

58
00:02:26,866 --> 0:02:28,876
世界跟踪 以及 平面检测

59
00:02:29,616 --> 0:02:31,376
之后 我们才会进一步

60
00:02:31,496 --> 0:02:33,466
分析 ARKit 2 上

61
00:02:33,466 --> 0:02:36,536
自带的新的

62
00:02:36,536 --> 0:02:38,076
跟踪和检测技术

63
00:02:38,696 --> 0:02:40,316
这些技术包括 保存和加载地图

64
00:02:40,316 --> 0:02:43,776
图像跟踪以及物品检测

65
00:02:45,266 --> 0:02:46,926
在我们对这些

66
00:02:46,926 --> 0:02:48,886
技术做进一步的探讨之前

67
00:02:48,946 --> 0:02:51,416
不妨先来对 ARKit 进行一个

68
00:02:51,416 --> 0:02:52,646
简单的概括性的回顾

69
00:02:53,176 --> 0:02:55,936
这对刚刚接触 ARKit 的人来说会

70
00:02:55,936 --> 0:02:56,276
很有趣

71
00:02:56,786 --> 0:03:00,556
首先 你需要创造

72
00:02:56,786 --> 0:03:00,556
首先 你需要创造

73
00:03:00,556 --> 0:03:01,826
一个 ARSession

74
00:03:02,496 --> 0:03:04,576
ARSession 是处理

75
00:03:04,576 --> 0:03:06,946
从配置到运行 AR 技术

76
00:03:06,946 --> 0:03:11,226
所有事务的对象

77
00:03:11,476 --> 0:03:15,696
并且返回 AR 技术的结果

78
00:03:16,196 --> 0:03:19,406
然后你需要做的是

79
00:03:19,496 --> 0:03:21,866
描述你想要运行的技术类型

80
00:03:22,266 --> 0:03:23,256
比如说 你想要使用

81
00:03:23,256 --> 0:03:25,136
哪种跟踪技术或者

82
00:03:25,136 --> 0:03:26,376
想要启用哪种功能

83
00:03:26,436 --> 0:03:28,096
比如说平面检测功能

84
00:03:28,966 --> 0:03:32,506
接下来 你需要使用这一

85
00:03:32,506 --> 0:03:35,406
特定的 ARConfiguration

86
00:03:36,256 --> 0:03:39,476
并运行你自己的

87
00:03:39,476 --> 0:03:40,116
ARSession 实例

88
00:03:41,576 --> 0:03:43,206
然后 ARsession 会开始内部

89
00:03:44,306 --> 0:03:47,336
配置 AVCaptureSession

90
00:03:47,336 --> 0:03:49,596
并开始接受图像

91
00:03:50,096 --> 0:03:55,096
以及启动动作管理器

92
00:03:55,096 --> 0:03:57,616
接收运动传感数据

93
00:03:57,616 --> 0:03:58,626
这就是你设备

94
00:03:58,626 --> 0:04:02,666
上内置的 ARKit 输入系统

95
00:03:58,626 --> 0:04:02,666
上内置的 ARKit 输入系统

96
00:04:04,066 --> 0:04:07,066
数据经过处理后

97
00:04:07,066 --> 0:04:09,306
会返回为每秒 60 帧

98
00:04:09,306 --> 0:04:10,926
的 ARFrames

99
00:04:12,126 --> 0:04:14,226
ARFrame 是一种及时快照

100
00:04:14,226 --> 0:04:15,396
它能为你的

101
00:04:15,396 --> 0:04:17,055
AR 景象提供

102
00:04:17,055 --> 0:04:17,875
所需要的一切

103
00:04:18,366 --> 0:04:20,776
就好像 你拍摄的图像

104
00:04:20,776 --> 0:04:25,176
会成为你的 AR 场景的背景

105
00:04:25,826 --> 0:04:27,136
跟踪相机运动

106
00:04:27,136 --> 0:04:31,256
适用于调整虚拟相机

107
00:04:31,706 --> 0:04:34,406
使之能从

108
00:04:34,446 --> 0:04:36,746
物理相机的角度

109
00:04:36,746 --> 0:04:37,586
调整虚拟物品的角度

110
00:04:38,766 --> 0:04:40,426
它还能提供

111
00:04:40,876 --> 0:04:42,086
环境相关信息

112
00:04:42,086 --> 0:04:43,536
举个例子 它可以检测到盘子

113
00:04:43,536 --> 0:04:46,806
现在 让我们

114
00:04:46,806 --> 0:04:48,436
从第一个跟踪技术开始

115
00:04:48,436 --> 0:04:49,216
说起和构建

116
00:04:51,586 --> 0:04:53,176
方向跟踪

117
00:04:54,386 --> 0:04:56,396
你们觉得方向跟踪所跟踪

118
00:04:56,536 --> 0:04:56,976
的是什么呢

119
00:04:57,176 --> 0:04:58,156
方向

120
00:04:58,676 --> 0:05:01,226
说明它只跟踪事物的旋转

121
00:04:58,676 --> 0:05:01,226
说明它只跟踪事物的旋转

122
00:05:02,136 --> 0:05:03,206
你可以这样子想象

123
00:05:03,206 --> 0:05:05,456
你只能用你的帽子来观察

124
00:05:05,456 --> 0:05:06,916
虚拟物品 而帽子本身只能

125
00:05:07,156 --> 0:05:09,046
通过旋转来观察事物

126
00:05:09,966 --> 0:05:11,716
也就是说你可以从

127
00:05:11,866 --> 0:05:13,406
固定的位置的不同角度

128
00:05:13,406 --> 0:05:15,656
来体验虚拟物体

129
00:05:15,656 --> 0:05:17,276
但任何位置上的改变都

130
00:05:17,276 --> 0:05:17,856
无法被跟踪到

131
00:05:19,596 --> 0:05:21,546
旋转数据是分三个

132
00:05:21,546 --> 0:05:22,836
轴来跟踪

133
00:05:22,946 --> 0:05:24,276
这也是为什么有的人

134
00:05:24,316 --> 0:05:26,416
称它为三个自由度跟踪

135
00:05:26,416 --> 0:05:28,926
你可以把这个技术应用到一个

136
00:05:28,926 --> 0:05:30,846
球形的虚拟环境中

137
00:05:30,846 --> 0:05:32,256
像体验一个 360 度的视频

138
00:05:32,256 --> 0:05:35,066
在这个视频中

139
00:05:35,066 --> 0:05:36,496
体验者可以从同一个位置

140
00:05:36,496 --> 0:05:38,116
对虚拟内容进行观察

141
00:05:39,346 --> 0:05:41,226
也可以用于观察一个距离很远

142
00:05:41,226 --> 0:05:43,196
的 AR 物品上

143
00:05:44,336 --> 0:05:46,296
方向跟踪并不适用

144
00:05:46,396 --> 0:05:48,236
于真实世界中的视觉增强

145
00:05:48,236 --> 0:05:49,746
因为在真实世界中

146
00:05:49,746 --> 0:05:50,886
你希望能从不同的位置

147
00:05:50,886 --> 0:05:52,666
对事物进行观察

148
00:05:54,226 --> 0:05:56,416
让我们来看一下

149
00:05:56,556 --> 0:05:58,016
当方向跟踪在运行时

150
00:05:58,016 --> 0:05:59,486
究竟发生了些什么

151
00:06:00,036 --> 0:06:02,676
这很简单

152
00:06:03,106 --> 0:06:05,016
它只是使用了核心运动

153
00:06:05,016 --> 0:06:07,166
的旋转数据

154
00:06:07,326 --> 0:06:09,146
其将传感器融合应用到

155
00:06:09,146 --> 0:06:09,986
运动传感器数据中

156
00:06:11,426 --> 0:06:13,626
与相机图像相比

157
00:06:13,756 --> 0:06:15,696
运动数据的更新频率要高出许多

158
00:06:15,696 --> 0:06:17,936
一旦相机图像可用

159
00:06:18,206 --> 0:06:20,736
方向跟踪就会

160
00:06:20,736 --> 0:06:22,616
采用这一系列数据中

161
00:06:22,616 --> 0:06:23,636
最新的运动数据

162
00:06:23,776 --> 0:06:25,766
然后将两个结果

163
00:06:26,086 --> 0:06:27,206
返回给 ARFrame

164
00:06:27,526 --> 0:06:28,066
就这样

165
00:06:28,126 --> 0:06:28,956
很简单

166
00:06:29,686 --> 0:06:31,726
请注意 在方向跟踪中

167
00:06:31,726 --> 0:06:33,266
并不会处理到

168
00:06:33,266 --> 0:06:34,526
相机回馈信息

169
00:06:34,856 --> 0:06:35,956
也就是说这个过程

170
00:06:35,956 --> 0:06:37,036
中并不会有电脑版本产生

171
00:06:38,286 --> 0:06:40,376
如果你想要运转方向跟踪功能

172
00:06:40,986 --> 0:06:43,096
你只需要在你的

173
00:06:43,286 --> 0:06:45,876
ARSession 添加

174
00:06:45,876 --> 0:06:47,256
AROrientation TrackingConfiguration

175
00:06:48,266 --> 0:06:49,686
所产生的结果将会

176
00:06:49,686 --> 0:06:52,216
通过 ARFrame 返回为

177
00:06:52,996 --> 0:06:55,116
一个 ARCamera 对象

178
00:06:55,116 --> 0:06:57,926
通常一个 ARCamera 对象

179
00:06:57,926 --> 0:06:59,826
会包括不同的变换

180
00:06:59,826 --> 0:07:01,106
在方向跟踪中

181
00:06:59,826 --> 0:07:01,106
在方向跟踪中

182
00:07:01,106 --> 0:07:02,936
这些变形只包括

183
00:07:02,936 --> 0:07:05,006
你的现实相机反馈的

184
00:07:05,116 --> 0:07:06,016
旋转数据

185
00:07:07,096 --> 0:07:09,616
这些旋转数据

186
00:07:09,666 --> 0:07:11,596
也可以用欧拉角来代表

187
00:07:12,276 --> 0:07:14,306
你可以根据自己的喜好来选择

188
00:07:16,866 --> 0:07:18,686
接下来让我们

189
00:07:18,686 --> 0:07:20,326
讨论一些更高级的跟踪技术

190
00:07:21,156 --> 0:07:22,546
我们先从世界跟踪开始

191
00:07:23,076 --> 0:07:25,406
世界跟踪能够跟踪你的

192
00:07:25,676 --> 0:07:28,186
相机角度方向

193
00:07:28,186 --> 0:07:30,096
还能跟踪真实环境中

194
00:07:30,316 --> 0:07:32,056
的位置变化

195
00:07:32,326 --> 0:07:34,196
而且无需事先了解

196
00:07:34,196 --> 0:07:35,226
你的周围环境

197
00:07:36,266 --> 0:07:37,656
大家可以看到

198
00:07:37,656 --> 0:07:41,316
左边是现实生活中

199
00:07:41,316 --> 0:07:42,766
相机所看到的景象

200
00:07:42,766 --> 0:07:45,416
右边是跟踪相机

201
00:07:45,546 --> 0:07:47,466
在探索过程中的

202
00:07:47,466 --> 0:07:50,506
跟踪轨迹 这轨迹是用

203
00:07:50,506 --> 0:07:51,756
坐标系来表示

204
00:07:52,986 --> 0:07:54,406
接下来 我们将详细

205
00:07:54,506 --> 0:07:55,466
的说明当世界跟踪运作时

206
00:07:55,536 --> 0:07:56,376
到底发生了什么

207
00:07:56,896 --> 0:08:00,406
世界跟踪所使用

208
00:07:56,896 --> 0:08:00,406
世界跟踪所使用

209
00:08:00,406 --> 0:08:03,236
运动传感器

210
00:08:03,236 --> 0:08:05,436
是你设备上的加速器

211
00:08:05,436 --> 0:08:08,376
和陀螺仪的运动数据

212
00:08:08,376 --> 0:08:10,796
由此高频计算它的方向

213
00:08:11,116 --> 0:08:12,436
和旋转变化

214
00:08:14,706 --> 0:08:16,926
它同时还为 Metal

215
00:08:17,116 --> 0:08:19,276
提供了正确比例数据

216
00:08:20,656 --> 0:08:23,056
从专业方面来说

217
00:08:23,106 --> 0:08:24,306
这一部分的跟踪系统

218
00:08:24,306 --> 0:08:25,976
也被称为惯性里程计

219
00:08:27,076 --> 0:08:29,046
尽管这一运动数据

220
00:08:29,106 --> 0:08:31,026
能为短间隔动作

221
00:08:31,026 --> 0:08:32,826
提供详细的运动信息

222
00:08:32,826 --> 0:08:34,226
但当出现

223
00:08:34,275 --> 0:08:36,566
突然的运动时

224
00:08:36,645 --> 0:08:38,635
就可能会出现较长时间的漂移

225
00:08:39,015 --> 0:08:40,456
因为这些数据

226
00:08:40,616 --> 0:08:42,126
并没有理想中准确

227
00:08:42,126 --> 0:08:43,296
而且有累积误差的可能性

228
00:08:44,496 --> 0:08:45,616
这也是为什么它不能

229
00:08:45,906 --> 0:08:47,446
单独用于跟踪

230
00:08:48,036 --> 0:08:51,456
为了补偿这一漂移

231
00:08:51,696 --> 0:08:53,416
世界跟踪在使用

232
00:08:53,516 --> 0:08:55,506
相机图像时

233
00:08:55,506 --> 0:08:59,606
应用了一个电脑版本

234
00:09:00,706 --> 0:09:02,426
这个技术提供了

235
00:09:02,506 --> 0:09:05,276
更高的准确性 但却

236
00:09:05,276 --> 0:09:06,546
以计算时间为代价

237
00:09:08,056 --> 0:09:10,176
同时 它对快速相机运动

238
00:09:10,226 --> 0:09:12,596
十分敏感

239
00:09:12,666 --> 0:09:14,396
这就会导致

240
00:09:14,396 --> 0:09:15,296
图像上的模糊

241
00:09:16,516 --> 0:09:18,626
这一系统中

242
00:09:18,626 --> 0:09:20,926
的版本也被称为

243
00:09:20,926 --> 0:09:21,666
视觉里程计

244
00:09:22,016 --> 0:09:24,276
通过对计算机视觉和

245
00:09:24,276 --> 0:09:26,576
动作这两个系统的融合

246
00:09:26,576 --> 0:09:30,016
ARKit 吸收了这两个系统的优势

247
00:09:30,646 --> 0:09:32,086
ARKit 选用了计算机视觉中

248
00:09:32,086 --> 0:09:34,116
长时间间隔的

249
00:09:34,116 --> 0:09:34,946
高准确性

250
00:09:35,566 --> 0:09:37,576
而从运动数据中 它吸收了

251
00:09:37,576 --> 0:09:39,586
短时间间隔的准确性

252
00:09:39,586 --> 0:09:41,356
和测量过程中的

253
00:09:41,356 --> 0:09:43,446
高更新频率以及

254
00:09:43,446 --> 0:09:44,276
以米为测量单位

255
00:09:44,856 --> 0:09:47,456
通过合并这两个系统

256
00:09:47,456 --> 0:09:49,446
在处理某些框架时

257
00:09:49,586 --> 0:09:51,036
世界跟踪可以跳过

258
00:09:51,246 --> 0:09:52,886
计算机视觉这一步骤

259
00:09:52,886 --> 0:09:54,736
但仍保持高效和

260
00:09:54,776 --> 0:09:55,966
高反馈的跟踪

261
00:09:56,956 --> 0:09:58,856
这样子就可以减少 CPU 占用

262
00:09:58,856 --> 0:10:00,356
你可以将多出的 CPU 空间用于

263
00:09:58,856 --> 0:10:00,356
你可以将多出的 CPU 空间用于

264
00:10:00,356 --> 0:10:00,956
你的 App 上

265
00:10:02,876 --> 0:10:04,376
在专业方面

266
00:10:04,376 --> 0:10:06,456
这一合并技术也被称为

267
00:10:06,506 --> 0:10:07,696
视觉惯性里程计

268
00:10:08,916 --> 0:10:11,456
让我们进一步了解这一部分

269
00:10:11,456 --> 0:10:13,306
中的视觉部分

270
00:10:14,116 --> 0:10:15,926
计算机视觉正在

271
00:10:15,926 --> 0:10:19,286
处理我导出的图像

272
00:10:19,286 --> 0:10:21,096
中的一个重点区域

273
00:10:21,356 --> 0:10:23,526
像这里的蓝色和橙色的点

274
00:10:24,406 --> 0:10:26,006
它们被提取出来

275
00:10:26,006 --> 0:10:27,496
以确保在同一环境

276
00:10:27,946 --> 0:10:30,246
的其他图像中

277
00:10:30,286 --> 0:10:31,516
也能提取出同样的点

278
00:10:33,016 --> 0:10:34,096
这些重点区域

279
00:10:34,096 --> 0:10:35,166
也被称为特征

280
00:10:36,496 --> 0:10:37,616
可以看到

281
00:10:37,616 --> 0:10:39,996
根据它们本身的相似性与外观

282
00:10:40,196 --> 0:10:43,686
可以在同一相机的不同图像中

283
00:10:43,686 --> 0:10:44,326
将它们匹配起来

284
00:10:45,176 --> 0:10:46,716
接下来发生的与你眼睛

285
00:10:46,716 --> 0:10:48,566
看到 3D 的效果

286
00:10:48,566 --> 0:10:49,286
的过程类似

287
00:10:50,176 --> 0:10:51,576
这两个点之间

288
00:10:51,576 --> 0:10:54,226
有细微的倾向一侧的距离

289
00:10:55,056 --> 0:10:56,976
它们之间的视觉差

290
00:10:56,976 --> 0:10:58,796
是十分重要的

291
00:10:58,796 --> 0:11:00,636
因为它们对环境

292
00:10:58,796 --> 0:11:00,636
因为它们对环境

293
00:11:00,636 --> 0:11:02,236
的观察角度有细微的差别

294
00:11:02,606 --> 0:11:04,706
这个差别能让画面更立体

295
00:11:04,816 --> 0:11:06,056
更有深度感

296
00:11:07,106 --> 0:11:08,406
这就是 ARKit 在处理

297
00:11:08,406 --> 0:11:10,176
三角测量时

298
00:11:10,176 --> 0:11:12,076
对于同一相机

299
00:11:12,076 --> 0:11:14,156
不同角度的处理方式

300
00:11:14,736 --> 0:11:16,206
只要画面中有足够的视觉差

301
00:11:16,256 --> 0:11:18,046
ARKit 就能运行这一功能

302
00:11:18,896 --> 0:11:20,786
它能够计算这些

303
00:11:20,786 --> 0:11:23,306
匹配特征之间的深度数据

304
00:11:23,706 --> 0:11:26,826
换句话说 这些图像中的 2D 特征

305
00:11:26,826 --> 0:11:29,316
通过 3D 方式获得重组

306
00:11:30,806 --> 0:11:32,066
但是 这个重组

307
00:11:32,066 --> 0:11:34,476
成功的关键

308
00:11:35,706 --> 0:11:37,536
在于相机位置的

309
00:11:37,676 --> 0:11:39,776
改变以提供

310
00:11:39,776 --> 0:11:41,316
足够的视觉差

311
00:11:42,356 --> 0:11:44,626
比如说 往某一侧倾斜的运动

312
00:11:44,966 --> 0:11:47,746
单纯的旋转并不能为重组

313
00:11:47,746 --> 0:11:48,856
提供足够的信息

314
00:11:50,536 --> 0:11:52,606
这就是有关你所身处环境的

315
00:11:52,606 --> 0:11:53,646
第一份小地图

316
00:11:53,646 --> 0:11:55,826
在 ARKit 中 我们将它称为

317
00:11:55,906 --> 0:11:56,136
世界地图

318
00:11:57,396 --> 0:11:59,826
与此同时 你的镜头的

319
00:11:59,826 --> 0:12:01,626
相机位置以及

320
00:11:59,826 --> 0:12:01,626
相机位置以及

321
00:12:01,626 --> 0:12:04,226
方向都经过了计算

322
00:12:04,226 --> 0:12:06,766
在用字母 C 这里标志出来

323
00:12:07,476 --> 0:12:08,546
这代表着你的世界跟踪

324
00:12:08,546 --> 0:12:09,396
刚刚完成初始化

325
00:12:09,396 --> 0:12:12,496
这是跟踪系统的

326
00:12:12,496 --> 0:12:12,746
初始化阶段

327
00:12:12,746 --> 0:12:15,886
当世界地图

328
00:12:15,886 --> 0:12:17,326
经过初始重组时

329
00:12:17,326 --> 0:12:19,216
也定义了

330
00:12:19,406 --> 0:12:21,246
世界原点

331
00:12:21,986 --> 0:12:23,676
它被设置为

332
00:12:23,886 --> 0:12:27,326
第一个相机的三角框架帧的原点

333
00:12:28,046 --> 0:12:30,296
同时也被设置为与重力对齐

334
00:12:31,056 --> 0:12:34,286
我们在幻灯片中用 W 来代表

335
00:12:34,906 --> 0:12:35,786
因此 你现在可以在

336
00:12:35,786 --> 0:12:37,246
它的世界坐标系

337
00:12:37,246 --> 0:12:39,366
重建作为世界地图的

338
00:12:39,366 --> 0:12:40,896
真实环境

339
00:12:40,896 --> 0:12:42,046
的小型代表

340
00:12:42,836 --> 0:12:44,446
你所使用的相机

341
00:12:44,596 --> 0:12:46,646
也被用同样的

342
00:12:46,646 --> 0:12:48,646
世界坐标来跟踪

343
00:12:50,896 --> 0:12:53,186
现在 你可以往

344
00:12:53,186 --> 0:12:56,436
你的相机视角中加入视觉

345
00:12:56,436 --> 0:12:57,056
内容来增强它们

346
00:12:58,656 --> 0:13:01,016
为了将视觉内容

347
00:12:58,656 --> 0:13:01,016
为了将视觉内容

348
00:13:01,066 --> 0:13:03,416
正确的加入到一个 ARSession 中

349
00:13:03,416 --> 0:13:06,256
你需要运用到 ARkit 中的

350
00:13:06,636 --> 0:13:07,826
ARAnchors 这里我们用

351
00:13:07,826 --> 0:13:08,096
A 来表示

352
00:13:09,536 --> 0:13:12,326
ARAnchors 在这个世界地图

353
00:13:12,536 --> 0:13:14,076
也就是这个世界坐标体系

354
00:13:14,076 --> 0:13:15,816
中是一个参考点

355
00:13:16,486 --> 0:13:18,386
ARAnchor 是不可或缺的

356
00:13:18,386 --> 0:13:20,686
因为世界跟踪在跟踪

357
00:13:20,686 --> 0:13:22,206
时可能会更新

358
00:13:22,206 --> 0:13:23,636
也就是说

359
00:13:23,636 --> 0:13:25,336
所有分配给它的视觉内容都

360
00:13:25,336 --> 0:13:27,776
并会被更新并正确

361
00:13:27,886 --> 0:13:32,746
的增强到相机的景象中

362
00:13:32,746 --> 0:13:34,446
既然你已经应用了

363
00:13:34,446 --> 0:13:36,436
ARAnchors 你可以将视觉内容

364
00:13:36,466 --> 0:13:38,356
添加到锚点中

365
00:13:38,386 --> 0:13:40,496
这些内容稍后会

366
00:13:40,886 --> 0:13:44,116
以正确的增强方式添加到相机的镜头中

367
00:13:45,576 --> 0:13:48,676
从现在开始 这个基于你环境的

368
00:13:48,676 --> 0:13:51,026
3D 世界地图将会成为

369
00:13:51,026 --> 0:13:53,236
世界跟踪的参考系统

370
00:13:54,046 --> 0:13:56,456
它也是新图像的参考依据

371
00:13:57,076 --> 0:13:58,796
不同图像中的特征

372
00:13:58,796 --> 0:14:01,806
互相匹配且进行三角化

373
00:13:58,796 --> 0:14:01,806
互相匹配且进行三角化

374
00:14:02,666 --> 0:14:04,216
与此同时 新的稳定的

375
00:14:04,216 --> 0:14:05,776
特征被提取出来

376
00:14:06,106 --> 0:14:08,246
经过匹配 三角化

377
00:14:08,246 --> 0:14:10,496
最终能帮助你扩展

378
00:14:10,556 --> 0:14:10,786
你的世界地图

379
00:14:11,286 --> 0:14:14,036
换句话说 ARKit 正在学习你的环境

380
00:14:15,926 --> 0:14:17,006
这个学习过程会产生

381
00:14:17,006 --> 0:14:18,686
对当前相机的

382
00:14:18,946 --> 0:14:20,936
位置和方向的

383
00:14:20,936 --> 0:14:21,956
跟踪计算的更新

384
00:14:23,086 --> 0:14:24,986
最后就能向

385
00:14:24,986 --> 0:14:26,796
当前相机视角输出

386
00:14:26,796 --> 0:14:27,426
正确的增强元素

387
00:14:27,946 --> 0:14:31,966
当你持续不断的探索世界时

388
00:14:31,966 --> 0:14:33,786
世界跟踪可以

389
00:14:33,786 --> 0:14:35,706
持续的跟踪你的相机

390
00:14:35,706 --> 0:14:37,826
并持续学习你

391
00:14:38,076 --> 0:14:39,426
所处的环境

392
00:14:40,506 --> 0:14:42,866
但是随着时间的推移

393
00:14:42,866 --> 0:14:45,506
增强效果可能会出现细微的漂移

394
00:14:45,506 --> 0:14:47,366
就像你可以在左边

395
00:14:47,366 --> 0:14:49,096
的图像中看到

396
00:14:49,096 --> 0:14:51,086
对增强效果的细微补偿

397
00:14:52,366 --> 0:14:54,736
这是因为就算很细微的补偿

398
00:14:55,216 --> 0:14:58,016
或者细微的错误

399
00:14:58,016 --> 0:14:59,786
在经过累积后都

400
00:14:59,786 --> 0:15:01,496
会变得明显

401
00:14:59,786 --> 0:15:01,496
会变得明显

402
00:15:03,486 --> 0:15:05,286
现在这个设备回到了

403
00:15:05,426 --> 0:15:07,216
一个熟悉的视角

404
00:15:07,216 --> 0:15:09,036
它之前曾经探索过这个视角

405
00:15:09,086 --> 0:15:10,956
比如说是我们

406
00:15:10,956 --> 0:15:11,756
探索的出发点

407
00:15:11,756 --> 0:15:14,116
ARKit 会进行另外

408
00:15:14,116 --> 0:15:15,846
一种优化步骤

409
00:15:16,546 --> 0:15:18,086
这一步骤会让

410
00:15:18,086 --> 0:15:19,496
视觉惯性里程计系统

411
00:15:19,496 --> 0:15:21,986
这个 ARKit 支持的系统

412
00:15:21,986 --> 0:15:24,176
转换为一个

413
00:15:24,286 --> 0:15:26,206
视觉惯性 SLAM 系统

414
00:15:27,376 --> 0:15:28,826
现在让我们回到

415
00:15:28,916 --> 0:15:30,726
世界跟踪开始探索

416
00:15:30,726 --> 0:15:32,476
的第一张图像

417
00:15:33,956 --> 0:15:35,136
世界跟踪现在要做的事情是

418
00:15:35,136 --> 0:15:37,166
检查当前的跟踪信息

419
00:15:37,166 --> 0:15:39,246
与世界地图对当前

420
00:15:39,446 --> 0:15:41,076
景象的渲染是否

421
00:15:41,076 --> 0:15:43,666
与过去的吻合

422
00:15:43,926 --> 0:15:45,276
也就是一开始的图像

423
00:15:45,276 --> 0:15:48,586
接着 ARKit 就会进行

424
00:15:48,586 --> 0:15:51,886
优化步骤

425
00:15:52,066 --> 0:15:54,316
将当前的信息以及世界地图

426
00:15:54,316 --> 0:15:56,306
与你的真实

427
00:15:56,306 --> 0:15:57,666
物理环境匹配起来

428
00:15:58,816 --> 0:16:00,256
你们是否有注意到

429
00:15:58,816 --> 0:16:00,256
你们是否有注意到

430
00:16:00,256 --> 0:16:01,946
在这个过程中

431
00:16:01,946 --> 0:16:02,536
ARAnchor 也被更新了

432
00:16:03,006 --> 0:16:04,476
这也是为什么你在

433
00:16:04,476 --> 0:16:07,036
往你眼前的景象中加入

434
00:16:07,036 --> 0:16:09,436
视觉内容时要使用 ARAnchor

435
00:16:09,956 --> 0:16:14,396
在这个视频中 你可以看到

436
00:16:14,396 --> 0:16:16,956
这个修正步骤在真实相机反馈

437
00:16:16,956 --> 0:16:17,716
中重复了一次

438
00:16:18,116 --> 0:16:20,736
在左边 我们可以从相机角度看

439
00:16:20,736 --> 0:16:22,596
这个环境 还能看到

440
00:16:22,596 --> 0:16:25,026
这个图像中所跟踪的特征

441
00:16:25,406 --> 0:16:26,986
在右边则是

442
00:16:26,986 --> 0:16:28,356
从俯视角度观察该景象的画面

443
00:16:28,356 --> 0:16:30,596
这代表着 ARKit

444
00:16:30,596 --> 0:16:33,166
了解这个环境并

445
00:16:34,016 --> 0:16:36,386
正在展示这个环境的 3D 重建

446
00:16:36,996 --> 0:16:39,506
这些点的颜色

447
00:16:39,506 --> 0:16:41,346
是根据重建点的高度

448
00:16:41,346 --> 0:16:43,306
进行编译

449
00:16:43,306 --> 0:16:45,036
蓝色代表地面高度

450
00:16:45,096 --> 0:16:46,746
红色代表桌子和椅子

451
00:16:47,376 --> 0:16:51,216
一旦相机回到

452
00:16:51,216 --> 0:16:52,576
它曾经拍摄过的视角

453
00:16:52,576 --> 0:16:54,546
就好像这里的开始的视角

454
00:16:54,546 --> 0:16:56,636
ARKit 就会应用

455
00:16:56,636 --> 0:16:57,896
这一优化步骤

456
00:16:57,996 --> 0:16:59,436
你需要留意的是

457
00:16:59,436 --> 0:17:01,416
点云以及相机轨迹

458
00:16:59,436 --> 0:17:01,416
点云以及相机轨迹

459
00:17:02,826 --> 0:17:04,106
不知道你是否有留意到这一更新

460
00:17:04,556 --> 0:17:05,506
让我再展示一次

461
00:17:05,996 --> 0:17:10,866
这个更新将 ARKit

462
00:17:10,866 --> 0:17:12,175
的知识与你的现实世界

463
00:17:12,286 --> 0:17:15,016
以及相机运动

464
00:17:15,016 --> 0:17:17,536
匹配起来

465
00:17:17,536 --> 0:17:19,425
为接下来的相机帧数

466
00:17:19,425 --> 0:17:20,626
提供更好的增强效果

467
00:17:21,955 --> 0:17:23,306
顺便说一下

468
00:17:23,306 --> 0:17:24,935
有关世界跟踪的所有计算

469
00:17:25,435 --> 0:17:28,886
以及有关你所处环境的信息

470
00:17:29,346 --> 0:17:31,076
这些事情都只需要在你的设备

471
00:17:31,076 --> 0:17:31,996
上就可以完成

472
00:17:32,136 --> 0:17:33,496
这些数据也只存在于

473
00:17:33,496 --> 0:17:35,096
你的设备上

474
00:17:35,096 --> 0:17:37,836
如何才能将这一复杂的

475
00:17:37,836 --> 0:17:40,566
技术应用到你的 App 中呢

476
00:17:41,926 --> 0:17:45,606
很简单

477
00:17:45,716 --> 0:17:47,486
如果你想要运行世界跟踪功能

478
00:17:47,526 --> 0:17:49,766
你只需要在你的 ARSession 中

479
00:17:49,816 --> 0:17:51,896
添加 ARWorldTrackingConfiguration 类

480
00:17:52,976 --> 0:17:55,376
它将会以 ARCamera

481
00:17:55,376 --> 0:17:57,956
的对象 ARFrame 形式返回结果

482
00:18:00,216 --> 0:18:03,636
ARCamera 的对象

483
00:18:03,636 --> 0:18:05,626
包括了转换

484
00:18:05,626 --> 0:18:06,946
在世界跟踪中

485
00:18:07,496 --> 0:18:08,806
转换还包括旋转

486
00:18:08,806 --> 0:18:12,116
以及对跟踪相机的平移

487
00:18:13,286 --> 0:18:15,106
除此之外 ARCamera 还包括

488
00:18:15,106 --> 0:18:16,886
有关跟踪状态

489
00:18:16,886 --> 0:18:18,556
以及 trackingStateReason

490
00:18:18,556 --> 0:18:19,746
的信息

491
00:18:20,166 --> 0:18:22,366
这将会为当前

492
00:18:22,366 --> 0:18:24,206
的跟踪质量提供

493
00:18:24,306 --> 0:18:25,216
一些信息

494
00:18:26,736 --> 0:18:27,976
接下来 要讨论的是跟踪质量

495
00:18:28,516 --> 0:18:29,906
你是否曾经使用过一个

496
00:18:30,016 --> 0:18:32,376
跟踪十分差劲

497
00:18:32,376 --> 0:18:34,476
或者根本不能跟踪

498
00:18:34,476 --> 0:18:35,646
的 AR App 呢

499
00:18:36,276 --> 0:18:37,346
那时你的感觉是怎么样的

500
00:18:38,446 --> 0:18:39,846
是不是很崩溃

501
00:18:39,846 --> 0:18:40,796
可能你再也不会用

502
00:18:40,796 --> 0:18:41,166
这个 App 了

503
00:18:42,026 --> 0:18:43,566
那么 怎样才在你的 App 中

504
00:18:43,566 --> 0:18:45,326
实现更好的跟踪质量呢

505
00:18:46,726 --> 0:18:48,626
要实现更好的跟踪质量

506
00:18:48,626 --> 0:18:49,846
我们需要明白什么主要因素

507
00:18:49,846 --> 0:18:51,646
会影响到跟踪质量

508
00:18:52,156 --> 0:18:53,856
在这里 我想要强调其中三点

509
00:18:55,056 --> 0:18:56,756
首先 世界跟踪依赖于

510
00:18:56,756 --> 0:18:58,996
持续不断的

511
00:18:59,056 --> 0:19:01,136
相机图像以及传感器数据

512
00:18:59,056 --> 0:19:01,136
相机图像以及传感器数据

513
00:19:01,556 --> 0:19:02,876
如果这些信息中断太久

514
00:19:02,876 --> 0:19:05,456
就可能会限制跟踪

515
00:19:06,926 --> 0:19:08,676
第二 世界跟踪在

516
00:19:08,676 --> 0:19:10,326
纹理明显 光线充足的环境下

517
00:19:10,326 --> 0:19:12,306
才最好的工作

518
00:19:12,646 --> 0:19:14,656
因为世界跟踪需要用到

519
00:19:14,656 --> 0:19:16,726
这些特征点来作为参考

520
00:19:16,726 --> 0:19:18,916
并最终将其位置三角化

521
00:19:18,916 --> 0:19:20,976
所以环境需要有

522
00:19:20,976 --> 0:19:23,136
足够的视觉复杂性这一点

523
00:19:23,136 --> 0:19:23,686
是十分重要的

524
00:19:24,706 --> 0:19:26,216
如果环境达不到这一要求

525
00:19:26,466 --> 0:19:28,046
比如说光线过于昏暗

526
00:19:28,046 --> 0:19:29,666
或者你正对着一面白墙

527
00:19:29,666 --> 0:19:31,606
那跟踪的表现

528
00:19:31,606 --> 0:19:32,656
就会很糟糕

529
00:19:33,166 --> 0:19:36,936
第三点是世界跟踪的

530
00:19:36,936 --> 0:19:38,646
最佳运作环境

531
00:19:38,646 --> 0:19:39,366
是静态环境

532
00:19:40,326 --> 0:19:42,026
如果你的取景框中

533
00:19:42,106 --> 0:19:45,166
大部分事物都是移动的

534
00:19:45,166 --> 0:19:47,036
那就会导致视觉数据与

535
00:19:47,036 --> 0:19:50,526
运动数据不吻合

536
00:19:50,526 --> 0:19:51,776
最终可能会导致漂移

537
00:19:52,846 --> 0:19:54,706
同时 设备不能处于

538
00:19:54,706 --> 0:19:55,936
一个移动平台上

539
00:19:55,936 --> 0:19:57,456
比如说公交车或者电梯

540
00:19:58,326 --> 0:19:59,726
假设设备被放置在电梯之中

541
00:19:59,726 --> 0:20:01,176
那运动传感器

542
00:19:59,726 --> 0:20:01,176
那运动传感器

543
00:20:01,176 --> 0:20:02,706
会检测到

544
00:20:02,706 --> 0:20:04,476
向上或向下的运动趋势

545
00:20:04,476 --> 0:20:06,766
但视觉上 它所处的环境

546
00:20:06,816 --> 0:20:07,566
并没有变化

547
00:20:08,066 --> 0:20:11,846
那你要如何获得

548
00:20:11,846 --> 0:20:13,966
用户在使用

549
00:20:14,036 --> 0:20:16,926
你的 App 时的体验的反馈呢

550
00:20:18,236 --> 0:20:20,586
ARKit 会监控它自己的跟踪表现

551
00:20:21,186 --> 0:20:22,776
我们在 ARKit 中应用了机器学习

552
00:20:23,076 --> 0:20:24,536
ARKit 在成千上万组

553
00:20:24,536 --> 0:20:26,546
的数据学习中获得提升

554
00:20:26,546 --> 0:20:28,706
这些数据中包括在不同

555
00:20:28,706 --> 0:20:30,266
情况下跟踪的表现

556
00:20:31,776 --> 0:20:33,276
为了训练出一个能够

557
00:20:33,276 --> 0:20:35,036
告诉你跟踪表现的分类器

558
00:20:35,036 --> 0:20:36,846
我们使用了一些注释

559
00:20:36,846 --> 0:20:39,486
比如视觉内容的数量

560
00:20:39,566 --> 0:20:41,136
在图像中跟踪到的可视特征

561
00:20:41,936 --> 0:20:44,346
以及设备当时的运动速率

562
00:20:45,496 --> 0:20:47,866
在运作时 跟踪的表现

563
00:20:47,866 --> 0:20:50,636
是由这些参数

564
00:20:50,636 --> 0:20:51,746
所决定的

565
00:20:52,616 --> 0:20:55,116
在这段视频中

566
00:20:55,116 --> 0:20:57,026
我们将镜头遮住

567
00:20:57,026 --> 0:20:58,406
但保持活动和

568
00:20:58,406 --> 0:21:00,846
对环境的探索

569
00:20:58,406 --> 0:21:00,846
对环境的探索

570
00:21:00,846 --> 0:21:03,926
可以看到左下角的健康预估

571
00:21:03,926 --> 0:21:06,296
指数在下降

572
00:21:07,796 --> 0:21:09,596
而当我们把遮住镜头

573
00:21:09,596 --> 0:21:11,336
的东西移走时

574
00:21:11,336 --> 0:21:12,496
这一数据就回归正常

575
00:21:14,036 --> 0:21:16,066
ARKit 通过给用户

576
00:21:16,066 --> 0:21:18,586
提供一个跟踪状态将

577
00:21:18,586 --> 0:21:19,596
数据进行简化

578
00:21:20,466 --> 0:21:22,186
跟踪状态由三个

579
00:21:22,316 --> 0:21:23,426
不同的值表示

580
00:21:23,426 --> 0:21:26,776
正常也就是

581
00:21:26,856 --> 0:21:29,676
健康状态 大部分情况下

582
00:21:29,676 --> 0:21:30,546
都是这种状态

583
00:21:30,546 --> 0:21:31,956
没错 在大部分情况下都会是这种状态

584
00:21:32,506 --> 0:21:34,096
另一种状态是限制状态

585
00:21:34,236 --> 0:21:35,396
这种状态会出现在跟踪

586
00:21:35,396 --> 0:21:36,426
表现糟糕时

587
00:21:37,486 --> 0:21:40,116
如果出现了这种状态

588
00:21:40,116 --> 0:21:42,056
那 ARKit 还会告诉你限制

589
00:21:42,056 --> 0:21:43,786
的原因 比如说没有足够的

590
00:21:43,946 --> 0:21:45,406
特征或者

591
00:21:45,406 --> 0:21:47,766
移动太快或者

592
00:21:47,806 --> 0:21:49,966
当前正处于初始化阶段

593
00:21:50,716 --> 0:21:53,496
还有另一种状态便是不可用状态

594
00:21:53,496 --> 0:21:55,256
这意味着当前还未

595
00:21:55,256 --> 0:21:56,086
完成初始化

596
00:21:57,116 --> 0:21:58,826
这样子 不管跟踪状态何时改变

597
00:21:58,916 --> 0:22:01,146
你都能够知道

598
00:21:58,916 --> 0:22:01,146
你都能够知道

599
00:22:01,416 --> 0:22:03,636
相机在跟踪过程中确实

600
00:22:03,636 --> 0:22:04,076
会发生变化

601
00:22:05,216 --> 0:22:06,116
这个功能让你能够

602
00:22:06,116 --> 0:22:08,666
在限制状态出现时

603
00:22:08,896 --> 0:22:10,346
及时地通知

604
00:22:10,346 --> 0:22:11,096
你的用户

605
00:22:12,116 --> 0:22:13,226
你还需要为你的

606
00:22:13,426 --> 0:22:15,426
用户提供有关

607
00:22:15,776 --> 0:22:17,936
改善他们跟踪情况

608
00:22:17,936 --> 0:22:19,626
的高实用性且可操作的建议

609
00:22:20,136 --> 0:22:22,566
因为改进跟踪环境的主动权

610
00:22:22,566 --> 0:22:23,396
掌握在用户手上

611
00:22:23,776 --> 0:22:25,706
这些建议可以是

612
00:22:25,706 --> 0:22:27,996
我们之前讨论过的 往某侧移动以

613
00:22:28,096 --> 0:22:30,656
运行设备进行初始化

614
00:22:31,206 --> 0:22:32,236
或者确保有足够的

615
00:22:32,236 --> 0:22:34,316
光线来保证

616
00:22:34,416 --> 0:22:35,646
足够的复杂性

617
00:22:36,236 --> 0:22:39,646
接下来 我将会总结一下

618
00:22:39,646 --> 0:22:40,446
世界跟踪

619
00:22:42,136 --> 0:22:46,156
世界跟踪为你的相机

620
00:22:46,156 --> 0:22:48,046
提供方向和位置的 6

621
00:22:48,276 --> 0:22:51,556
自由度跟踪

622
00:22:51,556 --> 0:22:53,356
这个跟踪基于你所处的环境

623
00:22:53,356 --> 0:22:55,286
但它本身对你所处

624
00:22:55,286 --> 0:22:56,916
对环境并没有

625
00:22:56,916 --> 0:22:59,066
事先的了解

626
00:22:59,606 --> 0:23:01,866
这个跟踪让在真实

627
00:22:59,606 --> 0:23:01,866
这个跟踪让在真实

628
00:23:01,866 --> 0:23:02,836
世界中的增强内容

629
00:23:02,836 --> 0:23:05,166
可以从任何角度进行察看

630
00:23:06,636 --> 0:23:08,676
世界跟踪也创造了一个

631
00:23:08,676 --> 0:23:11,196
世界地图

632
00:23:11,196 --> 0:23:13,206
这个世界地图成为

633
00:23:13,206 --> 0:23:15,416
新的图像定位的参考系统

634
00:23:17,266 --> 0:23:18,386
为了提供更好的用户体验

635
00:23:18,386 --> 0:23:20,466
跟踪质量需要被监控

636
00:23:20,466 --> 0:23:22,696
并能为用户提供

637
00:23:22,786 --> 0:23:25,196
反馈或者指引

638
00:23:25,736 --> 0:23:28,676
世界跟踪只在你的

639
00:23:28,676 --> 0:23:29,606
设备上运行

640
00:23:30,056 --> 0:23:31,516
所有的结果都只保存

641
00:23:31,516 --> 0:23:31,926
于你的设备中

642
00:23:33,446 --> 0:23:34,716
如果你还没有尝试过这个

643
00:23:35,576 --> 0:23:37,206
不妨在我们的开发者

644
00:23:37,206 --> 0:23:37,836
模板中试一下

645
00:23:37,936 --> 0:23:39,116
比如说你可以创造你的

646
00:23:39,116 --> 0:23:41,546
AR 初体验

647
00:23:41,546 --> 0:23:43,726
你可以做出一些探索

648
00:23:43,726 --> 0:23:44,866
花上 15 分钟时间了解不同情况

649
00:23:44,866 --> 0:23:46,536
下的跟踪质量 比如不同灯光效果

650
00:23:46,616 --> 0:23:48,036
或者运动频率

651
00:23:48,536 --> 0:23:50,476
一直记住 在用户遇到

652
00:23:50,476 --> 0:23:53,596
限制跟踪状态时

653
00:23:53,826 --> 0:23:56,696
要及时给他提供指导

654
00:23:56,696 --> 0:23:58,646
以保证他能有一个

655
00:23:58,886 --> 0:24:00,036
好的跟踪体验

656
00:23:58,886 --> 0:24:00,036
好的跟踪体验

657
00:24:01,456 --> 0:24:04,586
世界跟踪与相机相关

658
00:24:04,586 --> 0:24:06,566
它与你的相机位置

659
00:24:06,566 --> 0:24:09,186
以及你所处的环境相关

660
00:24:10,156 --> 0:24:13,026
接下来 我们要讨论

661
00:24:13,026 --> 0:24:14,536
视觉内容是如何

662
00:24:14,836 --> 0:24:16,776
与现实环境互动的

663
00:24:17,136 --> 0:24:19,156
这一效果时通过平面检测

664
00:24:19,156 --> 0:24:19,726
来实现

665
00:24:22,916 --> 0:24:24,876
接下来这段视频

666
00:24:24,876 --> 0:24:26,576
没错这段视频也来自于 Ikea App

667
00:24:26,576 --> 0:24:28,146
是一个很棒的平面检测例子

668
00:24:28,466 --> 0:24:30,626
将虚拟物品放置到

669
00:24:30,626 --> 0:24:32,566
你的真实环境中

670
00:24:32,566 --> 0:24:34,426
并与它互动

671
00:24:35,296 --> 0:24:37,786
首先 我们要留意的是

672
00:24:37,786 --> 0:24:39,256
在 Ikea App 中 设计者是如何

673
00:24:39,256 --> 0:24:41,156
指引用户进行运动的

674
00:24:42,296 --> 0:24:44,326
接着 一旦检测到水平面后

675
00:24:44,376 --> 0:24:46,796
一个虚拟桌子出现

676
00:24:46,796 --> 0:24:49,916
并等待着你去放置

677
00:24:51,256 --> 0:24:52,856
你为它选好位置 并按照你的

678
00:24:52,856 --> 0:24:54,646
想法旋转后

679
00:24:54,646 --> 0:24:55,886
你可以将它固定到环境中

680
00:24:56,126 --> 0:24:56,976
你是否有留意到

681
00:24:56,976 --> 0:24:59,206
当你将这个桌子固定住时

682
00:24:59,256 --> 0:25:02,196
平面与桌子之间的互动

683
00:24:59,256 --> 0:25:02,196
平面与桌子之间的互动

684
00:25:02,196 --> 0:25:04,916
就是地面有轻微的抖动

685
00:25:05,276 --> 0:25:06,986
当我们知道地面在哪时

686
00:25:06,986 --> 0:25:08,416
我们就可以实现这个效果

687
00:25:09,556 --> 0:25:10,996
让我们进一步的讨论

688
00:25:10,996 --> 0:25:12,696
这个过程中究竟发生了什么

689
00:25:14,016 --> 0:25:16,086
平面检测所运用的

690
00:25:16,166 --> 0:25:18,726
是我刚刚所提到的

691
00:25:18,726 --> 0:25:20,656
世界地图

692
00:25:20,656 --> 0:25:22,726
在这里我们用

693
00:25:22,726 --> 0:25:24,716
黄色的点来代表它

694
00:25:25,176 --> 0:25:28,446
平面检测利用这些点

695
00:25:28,446 --> 0:25:30,966
来检测水平或者垂直的平面

696
00:25:30,966 --> 0:25:32,936
比如说地面

697
00:25:32,936 --> 0:25:34,586
长椅以及小墙面

698
00:25:35,386 --> 0:25:36,946
它通过积累多个

699
00:25:36,946 --> 0:25:38,746
ARFrame 的信息

700
00:25:38,746 --> 0:25:39,266
来实现这个效果

701
00:25:40,096 --> 0:25:42,496
随着用户对场景的

702
00:25:42,596 --> 0:25:44,316
进一步探索 它能获得

703
00:25:44,316 --> 0:25:46,026
更多有关真实平面的信息

704
00:25:46,896 --> 0:25:48,086
这些信息让平面检测

705
00:25:48,086 --> 0:25:52,056
可以提供和扩张平面

706
00:25:52,056 --> 0:25:52,916
就像一个凸壳

707
00:25:53,386 --> 0:25:58,006
如果在同一物理表面上

708
00:25:58,006 --> 0:26:00,676
检测到多于一个的平面

709
00:25:58,006 --> 0:26:00,676
检测到多于一个的平面

710
00:26:00,676 --> 0:26:02,876
就像我们现在看的这一部分

711
00:26:02,876 --> 0:26:04,636
绿色和紫色的平面

712
00:26:05,226 --> 0:26:06,726
一旦他们重叠到一起

713
00:26:06,726 --> 0:26:07,926
就会被合并

714
00:26:08,596 --> 0:26:11,356
如果水平和垂直的平面

715
00:26:11,356 --> 0:26:13,026
出现相交的情况

716
00:26:13,026 --> 0:26:15,666
它们相交的地方会被剪掉

717
00:26:15,666 --> 0:26:18,156
这也是 ARKit 2 中的新功能

718
00:26:19,876 --> 0:26:21,776
平面检测被设定为

719
00:26:21,776 --> 0:26:24,686
有很少的误差

720
00:26:24,766 --> 0:26:27,166
因为它重新使用了世界跟踪

721
00:26:27,166 --> 0:26:28,296
中的 3D 点

722
00:26:29,086 --> 0:26:31,166
然后它将平面放置到

723
00:26:31,166 --> 0:26:33,386
这些点云中

724
00:26:33,696 --> 0:26:36,346
并不断的集合越来越多的点

725
00:26:36,346 --> 0:26:38,326
将已重叠的平面

726
00:26:38,606 --> 0:26:40,286
合并起来

727
00:26:40,936 --> 0:26:42,516
所以 检测第一个平面

728
00:26:42,516 --> 0:26:44,806
需要耗费一定的时间

729
00:26:46,096 --> 0:26:47,276
这对你有什么影响呢

730
00:26:48,856 --> 0:26:50,726
当你的 App 刚启动时

731
00:26:50,726 --> 0:26:53,036
可能并不会立刻有可以

732
00:26:53,036 --> 0:26:55,846
放置物品或与物品互动的平面

733
00:26:57,266 --> 0:26:58,786
如果你的体验一定需要

734
00:26:58,876 --> 0:27:01,496
检测到平面的话

735
00:26:58,876 --> 0:27:01,496
检测到平面的话

736
00:27:01,786 --> 0:27:04,006
你需要指引你的用户

737
00:27:04,426 --> 0:27:06,006
移动相机以获得足够

738
00:27:06,106 --> 0:27:08,956
的平移来提供

739
00:27:08,956 --> 0:27:11,616
足够的视觉差以保证密集的重建

740
00:27:11,616 --> 0:27:13,396
以及在场景中

741
00:27:13,396 --> 0:27:15,086
有足够的视觉复杂性

742
00:27:15,976 --> 0:27:18,096
对了 单有旋转数据是

743
00:27:18,096 --> 0:27:20,416
不足以重建的

744
00:27:20,946 --> 0:27:23,856
怎么样才能启用

745
00:27:23,856 --> 0:27:24,796
平面检测呢

746
00:27:25,786 --> 0:27:26,966
这个也很简单

747
00:27:27,116 --> 0:27:28,676
因为平面检测使用的是

748
00:27:28,676 --> 0:27:30,286
世界跟踪的 3D 地图

749
00:27:30,286 --> 0:27:32,486
它可以通过使用

750
00:27:32,646 --> 0:27:33,316
ARWorldTrackingConfiguration

751
00:27:33,316 --> 0:27:35,126
这一参数进行配置

752
00:27:35,646 --> 0:27:38,346
planeDetection 的特征

753
00:27:38,346 --> 0:27:40,706
可以设置为

754
00:27:40,706 --> 0:27:41,986
水平 垂直

755
00:27:41,986 --> 0:27:43,626
或者像这种情况下

756
00:27:43,706 --> 0:27:43,926
两者皆可

757
00:27:45,116 --> 0:27:47,446
接下来你便可以

758
00:27:47,446 --> 0:27:49,046
用这个配置

759
00:27:49,106 --> 0:27:50,226
来运行你的 ARSession

760
00:27:50,476 --> 0:27:52,146
ARKit 就开始对

761
00:27:52,146 --> 0:27:52,856
平面的检测了

762
00:27:53,526 --> 0:27:56,086
那这些检测到的平面结果

763
00:27:56,086 --> 0:27:58,576
是如何返回给你的呢

764
00:28:01,496 --> 0:28:03,216
检测的平面会以

765
00:28:03,216 --> 0:28:05,006
ARPlaneAnchor 的形式返回给你

766
00:28:05,936 --> 0:28:07,996
ARPlaneAnchor 是 ARAnchor

767
00:28:07,996 --> 0:28:08,946
的一个子类

768
00:28:10,106 --> 0:28:14,476
每个 ARAnchor 都会提供一个变换

769
00:28:14,476 --> 0:28:16,326
这个变换包含锚点在你

770
00:28:16,606 --> 0:28:17,566
的世界地图中的位置信息

771
00:28:18,126 --> 0:28:20,216
一个平面锚点

772
00:28:20,216 --> 0:28:25,886
也有着有关平面表面的几何信息

773
00:28:26,816 --> 0:28:27,856
这些信息有两种

774
00:28:27,856 --> 0:28:28,976
可选的代表方式

775
00:28:29,316 --> 0:28:31,256
一种是像一个有着中心点

776
00:28:31,256 --> 0:28:35,096
和拓展的密封箱子

777
00:28:35,096 --> 0:28:37,306
另一种则是 3D 网格

778
00:28:37,356 --> 0:28:39,336
这些网格描述着

779
00:28:39,386 --> 0:28:41,306
所检测到的平面的凸壳以及其几何特征

780
00:28:42,636 --> 0:28:44,646
当平面出现增加 更新

781
00:28:44,736 --> 0:28:48,266
或者移除的情况时

782
00:28:48,266 --> 0:28:51,266
需要有提示

783
00:28:51,266 --> 0:28:53,676
来通知我们

784
00:28:54,656 --> 0:28:57,086
这能让你及时

785
00:28:57,086 --> 0:28:59,566
利用这些平面并对更新

786
00:28:59,686 --> 0:29:01,486
做出反应

787
00:28:59,686 --> 0:29:01,486
做出反应

788
00:29:01,666 --> 0:29:03,366
你可以对平面做些什么呢

789
00:29:04,866 --> 0:29:05,956
我们刚刚在 Ikea App 中

790
00:29:05,956 --> 0:29:07,796
看到的就是很好的例子

791
00:29:08,026 --> 0:29:09,496
比如说你可以通过冲击测试

792
00:29:09,496 --> 0:29:10,916
来放置虚拟物品

793
00:29:12,046 --> 0:29:13,606
你也可以与一些

794
00:29:13,606 --> 0:29:15,186
虚拟物品进行真实互动

795
00:29:15,286 --> 0:29:17,846
就好像我们刚刚也看到了

796
00:29:17,846 --> 0:29:18,706
抖动是可以实现的

797
00:29:19,816 --> 0:29:21,966
你也可以在检测到的平面

798
00:29:21,966 --> 0:29:23,226
上添加一个遮挡平面

799
00:29:23,226 --> 0:29:25,346
所有虚拟物品

800
00:29:25,496 --> 0:29:27,286
就会被隐藏到

801
00:29:27,336 --> 0:29:30,916
这个遮挡屏幕之下或者之后

802
00:29:32,616 --> 0:29:34,436
所以 让我来总结一下我们现在

803
00:29:34,436 --> 0:29:35,386
所学习了的东西

804
00:29:36,536 --> 0:29:37,756
我们了解了方向跟踪

805
00:29:37,816 --> 0:29:41,176
世界跟踪

806
00:29:41,286 --> 0:29:44,386
以及平面检测

807
00:29:45,306 --> 0:29:47,666
接下里 Michele 将会

808
00:29:47,666 --> 0:29:48,986
进一步的介绍我们的新的

809
00:29:48,986 --> 0:29:50,316
跟踪技术 这些技术

810
00:29:50,386 --> 0:29:52,976
将会应用在 ARKit 2 中

811
00:29:53,046 --> 0:29:54,246
让我们欢迎 Michele

812
00:29:55,176 --> 0:29:57,500
[ 掌声 ]

813
00:29:58,396 --> 0:29:59,186
&gt;&gt; 谢谢 Marion

814
00:30:00,506 --> 0:30:01,766
大家好 我是 Michele

815
00:30:01,766 --> 0:30:02,766
很高兴能负责为大家

816
00:30:02,766 --> 0:30:03,636
讲解此次演示

817
00:30:03,636 --> 0:30:03,886
余下的话题

818
00:30:05,136 --> 0:30:07,886
接下来要讨论的是保存和

819
00:30:07,886 --> 0:30:08,326
加载地图

820
00:30:08,936 --> 0:30:10,486
这个功能可以

821
00:30:10,486 --> 0:30:11,816
将一个阶段中所需要的

822
00:30:11,816 --> 0:30:12,906
所有数据存储起来

823
00:30:13,336 --> 0:30:14,256
这样子 在稍后的另一个阶段中

824
00:30:14,256 --> 0:30:16,216
这些数据就可以

825
00:30:16,216 --> 0:30:18,116
被重新运用并用于

826
00:30:18,336 --> 0:30:19,706
创造与某个特定地区

827
00:30:19,706 --> 0:30:21,486
体验一致的 AR 体验

828
00:30:22,406 --> 0:30:23,566
或者它也可以存储在

829
00:30:23,566 --> 0:30:25,416
另一个设备上来创造

830
00:30:25,606 --> 0:30:27,906
多用户的 AR 体验

831
00:30:28,646 --> 0:30:30,000
举个例子

832
00:30:37,076 --> 0:30:38,876
这里有个男人

833
00:30:38,876 --> 0:30:40,906
让我们叫他 Andre

834
00:30:40,906 --> 0:30:41,956
他手里拿着设备

835
00:30:41,956 --> 0:30:43,506
绕着桌子走

836
00:30:43,506 --> 0:30:44,416
正在体验 AR 

837
00:30:45,366 --> 0:30:47,596
你可以看到他现在

838
00:30:47,986 --> 0:30:48,856
通过手上的设备往桌上

839
00:30:48,856 --> 0:30:50,766
添加了一个虚拟花瓶

840
00:30:50,766 --> 0:30:52,506
让这一切看起来更有趣了

841
00:30:54,556 --> 0:30:56,746
几分钟后

842
00:30:56,986 --> 0:30:58,126
他的朋友也来了

843
00:30:58,366 --> 0:31:00,576
现在他们都看着这个场景

844
00:30:58,366 --> 0:31:00,576
现在他们都看着这个场景

845
00:31:00,656 --> 0:31:01,926
Andre 的设备在左边

846
00:31:02,446 --> 0:31:04,196
而他朋友的设备

847
00:31:04,196 --> 0:31:05,126
在右边

848
00:31:06,546 --> 0:31:07,446
你可以看到他们

849
00:31:07,706 --> 0:31:08,836
正在看同一个空间

850
00:31:08,926 --> 0:31:09,886
他们能看到彼此

851
00:31:10,266 --> 0:31:11,806
但最重要的是

852
00:31:11,806 --> 0:31:13,386
他们可以看到同样的虚拟内容

853
00:31:14,286 --> 0:31:15,446
他们正在体验的是一个

854
00:31:15,446 --> 0:31:19,246
共享的 AR 体验

855
00:31:19,246 --> 0:31:21,456
上面这个例子

856
00:31:21,456 --> 0:31:23,916
可以被分为三个阶段

857
00:31:24,266 --> 0:31:25,816
第一个阶段 Andre 围着

858
00:31:25,816 --> 0:31:27,616
桌子走动并获得了世界地图

859
00:31:28,886 --> 0:31:30,246
第二个阶段 世界地图在设备之间

860
00:31:30,536 --> 0:31:31,506
进行了分享

861
00:31:32,276 --> 0:31:34,346
第三个阶段 他的朋友的设备

862
00:31:34,496 --> 0:31:36,056
重新定位了世界地图

863
00:31:37,496 --> 0:31:38,986
这意味着这个对象

864
00:31:38,986 --> 0:31:40,536
在新设备上也能识别

865
00:31:40,536 --> 0:31:42,936
这个场景与另一个设备上的是相同的

866
00:31:43,586 --> 0:31:45,406
然后根据地图来

867
00:31:45,406 --> 0:31:46,556
计算设备的准确位置

868
00:31:46,556 --> 0:31:48,356
之后就开始跟踪

869
00:31:48,356 --> 0:31:50,106
就好像新加入的设备

870
00:31:50,106 --> 0:31:51,746
自己获取了这个世界地图一样

871
00:31:52,376 --> 0:31:54,956
接下来 我们将会对

872
00:31:54,956 --> 0:31:56,196
这三个阶段进行更细节的了解

873
00:31:56,866 --> 0:31:59,246
但首先 让我们复习一下

874
00:31:59,246 --> 0:32:00,426
什么是世界地图

875
00:31:59,246 --> 0:32:00,426
什么是世界地图

876
00:32:01,156 --> 0:32:02,866
世界地图包括

877
00:32:02,866 --> 0:32:04,966
系统定位所需要的

878
00:32:04,966 --> 0:32:06,476
所有跟踪数据

879
00:32:06,986 --> 0:32:08,546
包括那些特征点

880
00:32:08,546 --> 0:32:09,866
就像 Marion 刚刚已经

881
00:32:09,866 --> 0:32:10,506
详细解释的一样

882
00:32:10,876 --> 0:32:12,596
也包括了这个点的

883
00:32:12,596 --> 0:32:13,000
周围外观

884
00:32:17,046 --> 0:32:18,496
世界地图还包括了

885
00:32:18,496 --> 0:32:19,826
被添加到环节中的

886
00:32:19,826 --> 0:32:21,806
所有锚点 不管这些锚点是由用户

887
00:32:21,916 --> 0:32:23,636
添加的 比如说像平面

888
00:32:24,896 --> 0:32:26,126
我的意思是像由系统

889
00:32:26,126 --> 0:32:26,456
添加的平面

890
00:32:26,456 --> 0:32:28,126
或者由用户添加的锚点

891
00:32:28,446 --> 0:32:29,646
就像我们刚刚在示范中看到的花瓶

892
00:32:30,746 --> 0:32:33,786
这个数据是可串性化

893
00:32:33,786 --> 0:32:35,536
可获取的 你可以利用这些数据

894
00:32:35,536 --> 0:32:37,346
来创造一些可持续的

895
00:32:37,826 --> 0:32:39,966
或者多用户的 AR 体验

896
00:32:40,326 --> 0:32:41,486
现在让我们来看看第一个阶段

897
00:32:41,486 --> 0:32:43,526
也就是获得

898
00:32:43,526 --> 0:32:44,006
世界地图的阶段

899
00:32:44,756 --> 0:32:47,576
我们可以回放第一段视频

900
00:32:48,056 --> 0:32:49,396
Andre 绕着桌子走到时候

901
00:32:49,396 --> 0:32:51,026
在左边 你可以

902
00:32:51,026 --> 0:32:52,596
看到他的设备 就在这里

903
00:32:53,256 --> 0:32:57,216
在右边 你可以以鸟瞰角度

904
00:32:57,216 --> 0:32:59,126
看到通过跟踪系统获得

905
00:32:59,126 --> 0:33:00,426
的世界地图

906
00:32:59,126 --> 0:33:00,426
的世界地图

907
00:33:00,776 --> 0:33:02,516
你可以围绕着

908
00:33:03,096 --> 0:33:04,566
它的桌子和椅子

909
00:33:05,136 --> 0:33:06,956
在这个获取过程中

910
00:33:06,956 --> 0:33:09,276
有几点需要我们注意

911
00:33:10,076 --> 0:33:11,986
首先 Marion 在跟踪部分

912
00:33:11,986 --> 0:33:13,946
所说到的所有技术 都在这里得到应用

913
00:33:14,456 --> 0:33:15,606
场景需要有足够的

914
00:33:15,606 --> 0:33:17,356
视觉复杂性

915
00:33:17,446 --> 0:33:18,626
地图才能获得足够的深度特征点

916
00:33:19,546 --> 0:33:21,276
场景需要是静止的 当然 

917
00:33:22,086 --> 0:33:22,856
如果有细微的变化

918
00:33:22,856 --> 0:33:24,276
也是没有关系的

919
00:33:24,276 --> 0:33:25,716
大家也可以看到桌布被风吹动

920
00:33:26,026 --> 0:33:27,216
但场景的主要部分

921
00:33:27,566 --> 0:33:27,956
需要是静止的

922
00:33:29,036 --> 0:33:30,606
另外 在我们

923
00:33:30,606 --> 0:33:31,936
获取世界地图进行分享时

924
00:33:31,936 --> 0:33:33,856
我们需要从多个

925
00:33:33,856 --> 0:33:35,316
不同的角度来

926
00:33:35,316 --> 0:33:36,636
探索这个环境

927
00:33:37,516 --> 0:33:38,746
特别是我们想要

928
00:33:38,746 --> 0:33:41,046
覆盖所有我们想要

929
00:33:41,106 --> 0:33:42,786
被定位的方向

930
00:33:45,366 --> 0:33:46,926
为了让这个过程变得更简单

931
00:33:47,356 --> 0:33:50,296
我们增加了世界地图状态提示

932
00:33:50,296 --> 0:33:51,736
让你能了解有关

933
00:33:51,736 --> 0:33:52,566
世界地图的信息

934
00:33:53,446 --> 0:33:54,816
如果你们有参加过

935
00:33:54,816 --> 0:33:56,066
What's New in ARKit 这一演讲

936
00:33:56,636 --> 0:33:57,506
那么 Arsalan 会极大的扩展这一点

937
00:33:57,546 --> 0:33:58,936
以便快速回顾一下

938
00:33:59,576 --> 0:34:00,766
当你开始会话时

939
00:33:59,576 --> 0:34:00,766
当你开始会话时

940
00:34:00,766 --> 0:34:02,776
世界地图的状态 会从限制状态开始

941
00:34:02,776 --> 0:34:03,936
随着设备了解了

942
00:34:04,056 --> 0:34:06,746
更多场景的信息

943
00:34:06,746 --> 0:34:07,616
就会变成初始化的状态

944
00:34:08,036 --> 0:34:09,176
最后当系统

945
00:34:09,176 --> 0:34:10,856
确定你保持在同一位置时

946
00:34:10,856 --> 0:34:12,156
我们终于可以开始

947
00:34:12,156 --> 0:34:12,766
绘制地图了

948
00:34:13,726 --> 0:34:15,206
这是在制图阶段

949
00:34:15,206 --> 0:34:16,976
你想要保存的地图

950
00:34:17,626 --> 0:34:20,315
所以 这是好的信息

951
00:34:20,315 --> 0:34:22,056
但是这主要是

952
00:34:22,056 --> 0:34:24,746
用户方面的适用操作

953
00:34:24,966 --> 0:34:26,076
所以 这对开发者而言

954
00:34:26,156 --> 0:34:26,926
代表着什么呢

955
00:34:27,505 --> 0:34:29,085
意味着你需要给用户提供指引

956
00:34:30,275 --> 0:34:32,255
我们可以告知用户制图时

957
00:34:32,326 --> 0:34:33,985
的不同状态

958
00:34:33,985 --> 0:34:35,835
甚至可以设置不允许保存

959
00:34:35,835 --> 0:34:37,886
或者分享世界地图

960
00:34:37,926 --> 0:34:39,476
直到制图状态显示绘制状态

961
00:34:39,996 --> 0:34:44,255
我们也可以监控在

962
00:34:44,326 --> 0:34:45,576
获取环节中的

963
00:34:45,576 --> 0:34:48,696
跟踪质量

964
00:34:48,926 --> 0:34:50,045
如果跟踪状态出现一段时间的

965
00:34:50,045 --> 0:34:51,356
限制状态 比如说持续几秒

966
00:34:51,356 --> 0:34:52,146
就可以反馈给用户

967
00:34:53,056 --> 0:34:54,356
或许甚至可以提供

968
00:34:54,356 --> 0:34:56,036
重新开始获取环节的选项

969
00:34:56,476 --> 0:34:58,496
在设备的接受方面

970
00:34:58,496 --> 0:35:00,726
我们也可以指引用户

971
00:34:58,496 --> 0:35:00,726
我们也可以指引用户

972
00:35:00,726 --> 0:35:02,346
以获得更好的定位流程

973
00:35:03,116 --> 0:35:04,856
我们又回到了获取设备这里

974
00:35:04,856 --> 0:35:06,776
当我们在地图阶段时

975
00:35:06,776 --> 0:35:08,196
我们可以对场景进行拍照

976
00:35:08,196 --> 0:35:10,296
然后将它与

977
00:35:10,396 --> 0:35:11,946
世界地图一起发出

978
00:35:11,986 --> 0:35:13,596
在接收端

979
00:35:13,596 --> 0:35:16,026
我们可以让用户找到这个视觉

980
00:35:16,026 --> 0:35:17,766
来开始你的分享体验

981
00:35:18,386 --> 0:35:20,946
这就是获得

982
00:35:20,946 --> 0:35:21,336
世界地图的流程

983
00:35:21,506 --> 0:35:22,876
现在 让我们了解一下如何

984
00:35:22,876 --> 0:35:24,666
才能分享世界地图

985
00:35:24,966 --> 0:35:26,366
首先 你可以简单地通过

986
00:35:26,366 --> 0:35:27,386
在 ARSession 中运行

987
00:35:27,496 --> 0:35:29,926
getCurrentWorldMap 方式来

988
00:35:29,926 --> 0:35:30,536
获得世界地图

989
00:35:30,896 --> 0:35:33,286
这是获得世界地图的一种方式

990
00:35:34,376 --> 0:35:37,476
世界地图是一个可串行化的类

991
00:35:37,566 --> 0:35:38,856
所以我们可以通过

992
00:35:38,956 --> 0:35:40,576
NSKeyedArchiver 公式来

993
00:35:40,576 --> 0:35:42,106
将它串行为一串二进制的数据

994
00:35:42,106 --> 0:35:44,446
你可以选择将这串数据

995
00:35:44,446 --> 0:35:46,276
保存在硬盘中

996
00:35:46,276 --> 0:35:48,946
供单机持续性的应用

997
00:35:49,706 --> 0:35:52,636
你也可以选择 将它在设备之间分享

998
00:35:52,906 --> 0:35:54,626
为了实现设备间分享

999
00:35:54,626 --> 0:35:56,176
你可以使用 MultiPeerConnectivity 框架

1000
00:35:56,936 --> 0:35:58,196
这个框架有着很棒的功能

1001
00:35:58,196 --> 0:36:00,186
像自动装置 发现附近设备等

1002
00:35:58,196 --> 0:36:00,186
像自动装置 发现附近设备等

1003
00:36:00,186 --> 0:36:01,856
这使设备之间的数据

1004
00:36:01,856 --> 0:36:04,876
可以高效沟通

1005
00:36:05,626 --> 0:36:06,996
我们在 ARKit 中也有如何运用

1006
00:36:06,996 --> 0:36:09,396
这个技巧的例子

1007
00:36:09,396 --> 0:36:11,036
被称为 创造一个多用户的

1008
00:36:11,036 --> 0:36:12,126
AR 体验

1009
00:36:12,126 --> 0:36:13,516
你可以在我们的开发者网站上找到这个例子

1010
00:36:14,076 --> 0:36:16,836
让我们看一下

1011
00:36:16,836 --> 0:36:18,086
接收端设备一旦收到世界地图之后

1012
00:36:18,086 --> 0:36:20,096
需要如何设置

1013
00:36:20,096 --> 0:36:21,046
世界跟踪参数

1014
00:36:21,046 --> 0:36:22,126
来使用这个地图

1015
00:36:22,426 --> 0:36:22,986
非常简单

1016
00:36:22,986 --> 0:36:24,916
你只需要赋予世界地图

1017
00:36:24,916 --> 0:36:27,366
原始数据地图的特征

1018
00:36:28,266 --> 0:36:29,986
当你运行这个会话时

1019
00:36:29,986 --> 0:36:31,426
系统会尝试去寻找

1020
00:36:31,526 --> 0:36:32,376
之前的世界地图

1021
00:36:33,636 --> 0:36:35,336
但这会耗费一定时间

1022
00:36:35,336 --> 0:36:36,366
因为用户可能并没有

1023
00:36:36,366 --> 0:36:37,546
从以前的位置拍摄

1024
00:36:37,546 --> 0:36:37,906
这一的场景

1025
00:36:38,756 --> 0:36:39,656
那我们怎样才能知道

1026
00:36:39,656 --> 0:36:40,606
定位正在运行呢

1027
00:36:41,686 --> 0:36:44,666
这个信息可以在跟踪阶段获得

1028
00:36:44,666 --> 0:36:45,896
只要你用原始世界地图

1029
00:36:45,896 --> 0:36:47,596
开始一个会话

1030
00:36:47,596 --> 0:36:49,256
那跟踪阶段就会被

1031
00:36:49,256 --> 0:36:51,366
限制为重新定位

1032
00:36:51,906 --> 0:36:54,226
但在这里你还是可以获得

1033
00:36:54,226 --> 0:36:55,816
跟踪数据

1034
00:36:56,176 --> 0:36:58,726
但原始世界将会是

1035
00:36:58,726 --> 0:37:00,826
第一个相机 就像在一个新的会话中一样

1036
00:36:58,726 --> 0:37:00,826
第一个相机 就像在一个新的会话中一样

1037
00:37:01,336 --> 0:37:04,156
只要用户将设备

1038
00:37:04,156 --> 0:37:05,606
对着同样的场景时

1039
00:37:05,606 --> 0:37:06,616
系统就会开始定位

1040
00:37:07,076 --> 0:37:08,136
跟踪阶段会重归正常

1041
00:37:08,136 --> 0:37:09,866
同时原始世界

1042
00:37:09,866 --> 0:37:12,036
将会与记录的世界地图一致

1043
00:37:13,046 --> 0:37:15,496
在这里 你之前的所有

1044
00:37:15,496 --> 0:37:16,856
锚点在你的会话中

1045
00:37:16,856 --> 0:37:17,916
都是可用的 所以你可以将以前

1046
00:37:17,976 --> 0:37:19,226
的虚拟内容放回来

1047
00:37:21,816 --> 0:37:23,506
在这里需要留意的是

1048
00:37:23,506 --> 0:37:24,986
这些幕后操作

1049
00:37:25,206 --> 0:37:26,806
是我们

1050
00:37:26,806 --> 0:37:28,206
正在匹配那些特征点

1051
00:37:28,616 --> 0:37:29,996
你获得世界地图的场景

1052
00:37:29,996 --> 0:37:32,306
与你想要重新定位的

1053
00:37:32,306 --> 0:37:33,326
场景之间需要有

1054
00:37:33,536 --> 0:37:35,146
足够的视觉相似性

1055
00:37:36,116 --> 0:37:37,406
所以 如果你想在晚上的时候

1056
00:37:37,406 --> 0:37:38,696
回到这个桌子旁进行 AR 体验

1057
00:37:38,696 --> 0:37:41,946
那效果很可能会不好

1058
00:37:42,206 --> 0:37:44,116
这就是你如何通过

1059
00:37:44,506 --> 0:37:46,516
保存或者加载地图

1060
00:37:46,516 --> 0:37:47,686
来实现多用户体验

1061
00:37:47,686 --> 0:37:49,526
或者持续体验的方式

1062
00:37:50,536 --> 0:37:54,846
接下来要讲的是图像跟踪

1063
00:37:54,846 --> 0:37:56,306
AR 就是

1064
00:37:56,306 --> 0:37:59,186
在物理世界上添加

1065
00:37:59,186 --> 0:38:00,306
一些虚拟内容

1066
00:37:59,186 --> 0:38:00,306
一些虚拟内容

1067
00:38:00,536 --> 0:38:01,686
在物理世界中

1068
00:38:01,686 --> 0:38:02,866
到处都可以发现图像

1069
00:38:02,866 --> 0:38:05,226
想想

1070
00:38:05,226 --> 0:38:07,086
杂志封面

1071
00:38:07,426 --> 0:38:08,336
广告

1072
00:38:08,816 --> 0:38:10,136
图像跟踪是一个工具

1073
00:38:10,136 --> 0:38:11,506
它让你能识别这些真实图像

1074
00:38:11,506 --> 0:38:13,976
并能在他们的周围

1075
00:38:14,216 --> 0:38:15,876
建立 AR 体验

1076
00:38:17,876 --> 0:38:18,746
让我们来看一个例子

1077
00:38:20,006 --> 0:38:21,896
大家可以看到这里

1078
00:38:21,896 --> 0:38:23,356
有两张图像被同时跟踪

1079
00:38:24,426 --> 0:38:26,766
在左边的图像中

1080
00:38:27,206 --> 0:38:29,376
一只美丽的大象被

1081
00:38:29,376 --> 0:38:30,716
放在真实图像中的大象之上

1082
00:38:31,606 --> 0:38:32,896
而在右边 一个真实的图像

1083
00:38:32,896 --> 0:38:35,116
被转化为一个虚拟的屏幕

1084
00:38:36,186 --> 0:38:37,626
还有一点需要留意的

1085
00:38:37,626 --> 0:38:39,016
图像可以在环境中随意移动

1086
00:38:39,016 --> 0:38:40,966
因为相机正在以每秒 60 帧的

1087
00:38:40,966 --> 0:38:42,146
速度进行跟踪

1088
00:38:43,306 --> 0:38:45,146
接下来让我们

1089
00:38:45,146 --> 0:38:46,866
讨论在这景象

1090
00:38:47,156 --> 0:38:47,476
背后发生了些什么

1091
00:38:47,476 --> 0:38:48,996
假设说你有一张

1092
00:38:48,996 --> 0:38:50,526
和这个大象一样的图像

1093
00:38:50,896 --> 0:38:52,186
你想要在一个类似这样的

1094
00:38:52,236 --> 0:38:52,876
景象中找到它

1095
00:38:54,266 --> 0:38:55,556
我们将会用灰度来实现这个效果

1096
00:38:55,556 --> 0:38:56,956
第一种方式

1097
00:38:56,956 --> 0:38:58,426
和我们在跟踪时用到的

1098
00:38:58,426 --> 0:38:58,836
有点类似

1099
00:38:58,936 --> 0:38:59,956
我们会跟踪

1100
00:39:00,046 --> 0:39:01,526
参考图像与

1101
00:39:01,526 --> 0:39:03,566
当前环境中的特征点

1102
00:39:04,676 --> 0:39:05,996
然后 我们会试图

1103
00:39:05,996 --> 0:39:07,356
在当前的场景中匹配

1104
00:39:07,356 --> 0:39:09,346
那些在参考图像中特征点

1105
00:39:10,436 --> 0:39:11,696
通过运用一些投影几何

1106
00:39:11,696 --> 0:39:12,866
以及线性代数

1107
00:39:13,186 --> 0:39:14,366
这就可以根据

1108
00:39:14,456 --> 0:39:16,126
当前的场景

1109
00:39:16,126 --> 0:39:17,286
对图像的位置方向

1110
00:39:17,286 --> 0:39:19,156
进行初步的估算

1111
00:39:20,516 --> 0:39:21,386
但我们并不满足于此

1112
00:39:22,566 --> 0:39:23,486
为了让你能获得一个

1113
00:39:23,486 --> 0:39:25,836
真正准确的姿势并做到

1114
00:39:25,836 --> 0:39:27,846
每秒 60 帧的跟踪

1115
00:39:27,846 --> 0:39:29,276
我们做了一个高密度跟踪阶段

1116
00:39:29,986 --> 0:39:31,316
在初步的预估之后

1117
00:39:31,976 --> 0:39:33,136
所以我们将现有场景的像素

1118
00:39:33,136 --> 0:39:36,486
回溯到一个

1119
00:39:36,566 --> 0:39:38,266
长方形的形状

1120
00:39:38,266 --> 0:39:40,916
也就是你在右边 右上角所能看到的

1121
00:39:41,306 --> 0:39:42,776
所以这是将

1122
00:39:43,536 --> 0:39:45,296
现有图像的像素回溯成

1123
00:39:45,296 --> 0:39:46,986
一个长方形后重组的图像

1124
00:39:47,916 --> 0:39:48,766
我们将重组后的图像

1125
00:39:48,766 --> 0:39:50,456
与我们所拥有的

1126
00:39:50,506 --> 0:39:51,726
参考图像进行对比

1127
00:39:51,726 --> 0:39:54,056
来创造你们所看到的

1128
00:39:54,056 --> 0:39:55,436
下面这个误差图像

1129
00:39:56,636 --> 0:39:59,836
接下来 我们会优化图像的位置方向

1130
00:39:59,906 --> 0:40:00,976
这样能使误差最小化

1131
00:39:59,906 --> 0:40:00,976
这样能使误差最小化

1132
00:40:03,366 --> 0:40:04,786
这也意味着

1133
00:40:04,786 --> 0:40:06,796
对你来说这个结果将会

1134
00:40:06,796 --> 0:40:07,266
十分准确

1135
00:40:08,146 --> 0:40:09,706
谢谢

1136
00:40:10,526 --> 0:40:12,076
而且还能做到每秒

1137
00:40:12,076 --> 0:40:12,826
跟踪 60 帧

1138
00:40:15,296 --> 0:40:16,906
让我们看看我们在

1139
00:40:16,906 --> 0:40:18,126
ARKit 中时如何实现这些

1140
00:40:18,916 --> 0:40:21,856
照常 ARKit 的 API

1141
00:40:21,856 --> 0:40:22,506
十分简单

1142
00:40:22,566 --> 0:40:24,566
我们只需要做三个简单的步骤

1143
00:40:24,606 --> 0:40:26,686
首先 我们需要收集

1144
00:40:26,686 --> 0:40:27,596
所有参考图像

1145
00:40:28,486 --> 0:40:31,346
接着 我们要设置 AR 会话的参数

1146
00:40:31,606 --> 0:40:32,646
我们有两种选择

1147
00:40:33,166 --> 0:40:34,306
一个是世界跟踪配置

1148
00:40:34,306 --> 0:40:37,426
它也提供设备位置

1149
00:40:37,426 --> 0:40:39,296
这个是我们目前所讨论的方式

1150
00:40:39,856 --> 0:40:42,576
在 iOS 12 中 将会有一个新的配置

1151
00:40:42,576 --> 0:40:44,476
这是一个独立的跟踪配置

1152
00:40:44,936 --> 0:40:47,906
一旦你开始了会话

1153
00:40:47,906 --> 0:40:49,896
你就会开始通过 ARImageAnchor

1154
00:40:49,896 --> 0:40:52,326
的形式接收结果

1155
00:40:53,296 --> 0:40:54,306
下面 我们将会

1156
00:40:54,306 --> 0:40:55,626
进一步研究这三个步骤

1157
00:40:56,036 --> 0:40:57,886
让我们从参考图像开始

1158
00:40:58,426 --> 0:41:01,286
往你的 App 中加入

1159
00:40:58,426 --> 0:41:01,286
往你的 App 中加入

1160
00:41:01,286 --> 0:41:02,836
参考图像的最便捷方式

1161
00:41:02,896 --> 0:41:05,106
就是通过资产目录

1162
00:41:06,146 --> 0:41:08,066
你可以创造一个 AR 资源组

1163
00:41:08,066 --> 0:41:10,386
然后将你的图像拖拽到组里

1164
00:41:11,566 --> 0:41:13,026
接下来 你需要设置

1165
00:41:13,026 --> 0:41:14,446
图像的物理尺寸

1166
00:41:14,556 --> 0:41:16,196
你可以通过右上角

1167
00:41:16,196 --> 0:41:17,436
的特征窗口进行设置

1168
00:41:17,946 --> 0:41:20,656
设置物理尺寸是硬性要求

1169
00:41:21,096 --> 0:41:23,126
这里有几个原因

1170
00:41:24,466 --> 0:41:26,126
第一 设置物理尺寸能让

1171
00:41:26,126 --> 0:41:27,816
图像以物理尺寸展示

1172
00:41:28,386 --> 0:41:29,776
也就是说 你的内容

1173
00:41:29,816 --> 0:41:31,076
将会是物理大小

1174
00:41:31,186 --> 0:41:32,666
在 ARKit 中 所有事物都是

1175
00:41:32,666 --> 0:41:33,986
以米为测量单位 所以你的虚拟

1176
00:41:33,986 --> 0:41:36,226
物品将会以米为长度

1177
00:41:37,056 --> 0:41:38,626
第二 设置正确的

1178
00:41:38,626 --> 0:41:40,266
图像物理尺寸是十分重要的

1179
00:41:40,266 --> 0:41:41,586
因为我们

1180
00:41:41,946 --> 0:41:43,166
有可能会将图像跟踪

1181
00:41:43,216 --> 0:41:44,476
与世界跟踪合并到一起

1182
00:41:44,936 --> 0:41:46,206
这会立刻给予图像

1183
00:41:46,526 --> 0:41:48,616
与世界之间

1184
00:41:48,616 --> 0:41:51,196
一致的姿态

1185
00:41:51,196 --> 0:41:52,826
让我们看看有关参考图像的

1186
00:41:53,256 --> 0:41:54,166
更多例子

1187
00:41:54,816 --> 0:41:57,806
这里有两幅很漂亮的图像

1188
00:41:58,476 --> 0:41:59,776
这些图像十分适合

1189
00:41:59,776 --> 0:42:01,066
图像跟踪

1190
00:41:59,776 --> 0:42:01,066
图像跟踪

1191
00:42:01,156 --> 0:42:03,426
它们有着高饱和度

1192
00:42:03,426 --> 0:42:04,956
高对比度

1193
00:42:04,956 --> 0:42:06,376
合理分配的柱形图

1194
00:42:06,376 --> 0:42:08,416
并且没有重复的结构

1195
00:42:08,536 --> 0:42:10,486
这里也有一些

1196
00:42:10,486 --> 0:42:12,936
和系统适配度不高的图像

1197
00:42:13,436 --> 0:42:14,726
右边这个图像就是不合适类型

1198
00:42:15,456 --> 0:42:16,176
的一个示范

1199
00:42:17,116 --> 0:42:19,606
如果我们仔细观察上面

1200
00:42:19,696 --> 0:42:21,596
这两个例子 我们会发现

1201
00:42:21,596 --> 0:42:23,466
好的图像都有着

1202
00:42:23,466 --> 0:42:25,036
许多关键点

1203
00:42:25,536 --> 0:42:26,306
图像中的

1204
00:42:26,306 --> 0:42:27,716
柱形图在整个

1205
00:42:27,716 --> 0:42:28,686
范围内平均分布

1206
00:42:29,316 --> 0:42:30,126
而右边的图像

1207
00:42:30,126 --> 0:42:33,096
则只有很少的关键点

1208
00:42:33,096 --> 0:42:34,286
它们的柱形图也

1209
00:42:34,286 --> 0:42:36,536
倾向白色的部分

1210
00:42:37,236 --> 0:42:40,536
在 Xcode 中

1211
00:42:40,536 --> 0:42:42,066
你可以直观的了解到一张图像

1212
00:42:42,066 --> 0:42:42,916
是否适合用于跟踪

1213
00:42:44,076 --> 0:42:46,066
你只需要将图像拖放到

1214
00:42:46,066 --> 0:42:48,396
Xcode 中 Xcode 就会自动

1215
00:42:48,576 --> 0:42:50,316
对图像进行分析 然后界面会以

1216
00:42:50,316 --> 0:42:52,206
警告的方式给你及早的反馈

1217
00:42:52,276 --> 0:42:54,516
这个反馈甚至还要早于你 运行你的 App

1218
00:42:55,646 --> 0:42:56,666
比如说 如果你点击

1219
00:42:56,666 --> 0:42:59,086
底部的这张图像

1220
00:42:59,086 --> 0:43:01,836
这张图像可能是杂志的某一页

1221
00:42:59,086 --> 0:43:01,836
这张图像可能是杂志的某一页

1222
00:43:01,836 --> 0:43:04,396
我们可以看到 Xcode 反馈说

1223
00:43:04,556 --> 0:43:06,436
这张图像的柱形图 分布不均匀

1224
00:43:06,676 --> 0:43:07,626
你可以看到图像中

1225
00:43:07,626 --> 0:43:09,326
有大量的白色

1226
00:43:09,476 --> 0:43:11,716
这个图像也包含了许多重复结构

1227
00:43:11,716 --> 0:43:15,636
这些重复结构主要是文字

1228
00:43:15,776 --> 0:43:18,076
另一个例子就是 如果你有两张

1229
00:43:18,076 --> 0:43:20,126
极其类似且

1230
00:43:20,126 --> 0:43:22,176
十分容易在检测时混淆的图像

1231
00:43:22,506 --> 0:43:24,366
Xcode 也会

1232
00:43:24,366 --> 0:43:25,166
做出相应的警告

1233
00:43:25,906 --> 0:43:26,986
这两张有关

1234
00:43:26,986 --> 0:43:28,906
同一齿状山脉的图像

1235
00:43:28,906 --> 0:43:29,756
就是一个很好的例子

1236
00:43:30,156 --> 0:43:32,446
如果这个警告出现时

1237
00:43:32,446 --> 0:43:33,506
我们并不能做什么来改正

1238
00:43:33,936 --> 0:43:34,956
例如 让我们回到这张

1239
00:43:34,956 --> 0:43:38,236
有着重复结构

1240
00:43:38,236 --> 0:43:40,746
且分布不平均

1241
00:43:40,746 --> 0:43:41,746
的柱形图图像

1242
00:43:42,516 --> 0:43:44,186
你可以尝试去定位

1243
00:43:44,186 --> 0:43:45,126
这张图像中区别足够

1244
00:43:45,126 --> 0:43:46,596
明显的地方 在这里

1245
00:43:46,596 --> 0:43:48,736
就是页面中真正的

1246
00:43:48,736 --> 0:43:49,536
图像部分

1247
00:43:50,106 --> 0:43:51,276
接下来 你就可以对图像

1248
00:43:51,276 --> 0:43:52,756
进行裁剪 将裁剪后的图像

1249
00:43:52,756 --> 0:43:53,556
作为参考图像

1250
00:43:53,876 --> 0:43:55,126
这样子 Xcode 就不会发出

1251
00:43:55,476 --> 0:43:56,756
任何警告

1252
00:43:56,756 --> 0:43:58,296
你也可以获得更好的

1253
00:43:58,946 --> 0:43:59,716
跟踪质量

1254
00:44:00,026 --> 0:44:03,106
我们还可以

1255
00:44:03,436 --> 0:44:06,376
使用多个 AR 资源群组

1256
00:44:07,496 --> 0:44:09,176
这个功能可以同时检测

1257
00:44:09,396 --> 0:44:10,146
多张图像

1258
00:44:10,416 --> 0:44:11,726
为了保持体验的高效性

1259
00:44:11,726 --> 0:44:13,816
与反应性

1260
00:44:14,236 --> 0:44:15,296
你可以设置每个组最多

1261
00:44:15,296 --> 0:44:16,736
只能处理 25 张图像

1262
00:44:17,936 --> 0:44:19,016
但群组的数量

1263
00:44:19,246 --> 0:44:19,826
是没有限制的

1264
00:44:20,036 --> 0:44:21,896
接着 你就可以通过编程

1265
00:44:21,896 --> 0:44:23,156
的方式切换不同的群组

1266
00:44:23,376 --> 0:44:26,076
举个例子 如果你想要创造

1267
00:44:26,076 --> 0:44:27,076
一个博物馆中的 AR 体验

1268
00:44:27,076 --> 0:44:28,846
这个博物馆可能有着

1269
00:44:28,846 --> 0:44:30,646
成千上百的图像

1270
00:44:31,816 --> 0:44:33,396
通常 这些图像

1271
00:44:33,396 --> 0:44:34,586
分布在博物馆的

1272
00:44:34,586 --> 0:44:35,336
不同房间中

1273
00:44:35,646 --> 0:44:37,646
那你将要做的是

1274
00:44:38,246 --> 0:44:39,756
将在同一个房间中的图像

1275
00:44:39,756 --> 0:44:41,476
划分到一个群组中

1276
00:44:41,686 --> 0:44:43,036
在另外一个房间的图像则属于

1277
00:44:43,036 --> 0:44:43,596
另一个群组

1278
00:44:44,246 --> 0:44:45,806
随后 你可以用中心定点

1279
00:44:45,806 --> 0:44:48,686
来切换不同的房间

1280
00:44:49,186 --> 0:44:52,116
在这种情况下

1281
00:44:52,446 --> 0:44:53,906
只要在不同的群组中就可以

1282
00:44:53,946 --> 0:44:55,126
存在类似的图像

1283
00:44:55,896 --> 0:44:57,716
好的 以上就是有关

1284
00:44:57,746 --> 0:44:58,266
参考图像的信息

1285
00:44:58,266 --> 0:45:01,916
现在 让我们来看看我们的两个配置吧

1286
00:44:58,266 --> 0:45:01,916
现在 让我们来看看我们的两个配置吧

1287
00:45:03,006 --> 0:45:05,286
ARImageTrackingConfiguration

1288
00:45:05,416 --> 0:45:06,876
是一个全新的 独立的

1289
00:45:06,876 --> 0:45:07,836
图像跟踪配置

1290
00:45:07,866 --> 0:45:09,646
这意味着它不会运转世界跟踪

1291
00:45:10,636 --> 0:45:11,846
也就是说并没有

1292
00:45:12,136 --> 0:45:12,916
它并没有世界原点

1293
00:45:13,216 --> 0:45:14,676
所以 每一张反馈给你的图像

1294
00:45:14,676 --> 0:45:16,526
都是与当前相机的视角有关

1295
00:45:18,246 --> 0:45:19,346
你可以将图像跟踪

1296
00:45:19,406 --> 0:45:21,646
与世界跟踪的配置结合起来

1297
00:45:22,466 --> 0:45:24,536
在这种情况下

1298
00:45:24,536 --> 0:45:25,636
你可以获得所有与

1299
00:45:25,636 --> 0:45:27,506
场景理解有关的东西

1300
00:45:27,506 --> 0:45:29,356
像平面检测 灯光预估

1301
00:45:29,356 --> 0:45:30,026
以及其他信息

1302
00:45:31,066 --> 0:45:32,796
那这两种配置中

1303
00:45:32,796 --> 0:45:34,376
哪一种更适合我们使用呢

1304
00:45:35,066 --> 0:45:35,536
让我们看看

1305
00:45:35,626 --> 0:45:37,106
ARImageTrackingConfigurations

1306
00:45:37,516 --> 0:45:39,916
是专门为围绕

1307
00:45:39,916 --> 0:45:41,226
图像发生的 AR 体验

1308
00:45:41,226 --> 0:45:42,906
所量身定制的

1309
00:45:43,196 --> 0:45:44,826
在屏幕的左侧 我们可以看到相应的例子

1310
00:45:46,606 --> 0:45:48,516
图像可以是

1311
00:45:48,516 --> 0:45:49,666
课本的某一页

1312
00:45:50,506 --> 0:45:51,576
为了让这次体验更

1313
00:45:51,576 --> 0:45:53,976
吸引人 我们

1314
00:45:54,286 --> 0:45:54,746
动态地将图表重叠

1315
00:45:54,746 --> 0:45:55,846
在这个例子里就是

1316
00:45:55,846 --> 0:45:56,796
如何画一个等边三角形

1317
00:45:57,866 --> 0:45:58,746
你可以看到

1318
00:45:58,746 --> 0:46:00,966
这个体验真的是为图像量身定制的

1319
00:45:58,746 --> 0:46:00,966
这个体验真的是为图像量身定制的

1320
00:46:01,116 --> 0:46:03,826
让我们来看另一个例子

1321
00:46:04,546 --> 0:46:05,516
图像跟踪用于

1322
00:46:05,606 --> 0:46:07,266
触发一些超出

1323
00:46:07,266 --> 0:46:09,496
图像范围的内容

1324
00:46:09,496 --> 0:46:12,086
在这种情况下 你需要使用

1325
00:46:12,086 --> 0:46:13,726
ARWorldTrackingConfiguration

1326
00:46:13,806 --> 0:46:14,906
因为你需要设备的位置信息

1327
00:46:14,906 --> 0:46:16,506
来保持对图像外的

1328
00:46:16,506 --> 0:46:19,856
内容的跟踪

1329
00:46:20,076 --> 0:46:21,146
因为图像跟踪并不使用

1330
00:46:21,246 --> 0:46:23,576
运动数据

1331
00:46:23,576 --> 0:46:24,676
所以它适用于像

1332
00:46:24,676 --> 0:46:26,506
公交车或者电梯这类

1333
00:46:27,076 --> 0:46:28,146
运动数据与视觉数据

1334
00:46:28,146 --> 0:46:29,896
不匹配的场景下

1335
00:46:30,916 --> 0:46:33,036
下面要讲的是

1336
00:46:33,036 --> 0:46:33,756
如何在代码中实现这些功能

1337
00:46:35,236 --> 0:46:36,916
你可以轻易地分辨出

1338
00:46:36,946 --> 0:46:37,716
这里的三个步骤

1339
00:46:37,936 --> 0:46:40,476
第一个是收集所有图像

1340
00:46:40,876 --> 0:46:41,796
这里有个很方便的功能

1341
00:46:41,796 --> 0:46:43,396
ARReferenceImage 这个功能

1342
00:46:43,566 --> 0:46:45,436
可以收集在

1343
00:46:45,436 --> 0:46:47,436
特定群组中的

1344
00:46:47,436 --> 0:46:48,366
所有图像

1345
00:46:48,556 --> 0:46:50,226
这个群组被命名为 Room1

1346
00:46:51,896 --> 0:46:53,256
我们可以简单的赋予

1347
00:46:53,256 --> 0:46:55,626
ARImageTrackingConfigurations

1348
00:46:55,626 --> 0:46:56,306
中这些图像

1349
00:46:56,306 --> 0:46:58,156
trackingImages 的特征

1350
00:46:58,586 --> 0:46:59,516
然后运行这一会话

1351
00:47:00,866 --> 0:47:02,766
你就能开始接收

1352
00:47:02,766 --> 0:47:04,306
反馈 比如说在这个会话中

1353
00:47:04,306 --> 0:47:06,346
didUpdate 这个锚点指定了方式

1354
00:47:06,466 --> 0:47:08,266
你可以检查这个

1355
00:47:08,266 --> 0:47:10,096
锚点是否是属于

1356
00:47:10,146 --> 0:47:11,156
ARImageAnchor

1357
00:47:12,606 --> 0:47:14,186
在这个锚点中

1358
00:47:14,246 --> 0:47:15,386
你可以找到图像的

1359
00:47:15,386 --> 0:47:17,336
位置与方向以及

1360
00:47:17,336 --> 0:47:18,656
参考图像本身

1361
00:47:18,716 --> 0:47:19,856
你可能会问

1362
00:47:19,856 --> 0:47:21,836
那要怎样才能找到以

1363
00:47:21,836 --> 0:47:23,156
你式命名的图像名称

1364
00:47:23,156 --> 0:47:24,126
然后确认哪些图像被

1365
00:47:24,126 --> 0:47:24,746
检测到了呢

1366
00:47:25,986 --> 0:47:26,966
这里有一个叫 Boolean 的特性

1367
00:47:26,966 --> 0:47:28,936
它能告诉你这个

1368
00:47:28,936 --> 0:47:30,086
图像当前是否

1369
00:47:30,086 --> 0:47:33,666
正在被跟踪

1370
00:47:33,866 --> 0:47:35,306
除了我们现在

1371
00:47:35,306 --> 0:47:36,496
所了解到的使用例子外

1372
00:47:36,496 --> 0:47:38,576
当你围绕着

1373
00:47:38,576 --> 0:47:40,546
图像进行建设

1374
00:47:40,546 --> 0:47:43,196
图像检测以及跟踪还有

1375
00:47:43,976 --> 0:47:44,126
其他的功能

1376
00:47:45,226 --> 0:47:47,496
就好像 如果两个设备都对着

1377
00:47:47,496 --> 0:47:48,866
同一张真实图像

1378
00:47:48,866 --> 0:47:51,446
你可以在两个设备中

1379
00:47:51,446 --> 0:47:52,416
同时检测到这个图像

1380
00:47:52,846 --> 0:47:54,586
这可以为你提供一个

1381
00:47:54,586 --> 0:47:56,066
分享坐标体系

1382
00:47:56,066 --> 0:47:57,626
这个体系可以为分享体验

1383
00:47:58,106 --> 0:47:59,386
提供另一种选择

1384
00:48:01,476 --> 0:48:03,846
另一个例子是 如果你刚好知道

1385
00:48:03,846 --> 0:48:05,226
某张图像在世界上的

1386
00:48:05,346 --> 0:48:06,916
真实位置

1387
00:48:08,296 --> 0:48:09,826
比如说 你知道这幅公园地图

1388
00:48:09,826 --> 0:48:12,076
在现实生活中的位置

1389
00:48:12,456 --> 0:48:14,446
你可以通过图像跟踪来

1390
00:48:14,446 --> 0:48:15,956
获得拍摄图像

1391
00:48:15,956 --> 0:48:17,576
设备的位置

1392
00:48:17,576 --> 0:48:19,636
还能获得这个设备

1393
00:48:19,636 --> 0:48:20,656
在这个世界上的位置

1394
00:48:20,656 --> 0:48:21,946
你可以利用这些数据

1395
00:48:21,946 --> 0:48:23,956
来找到真实世界

1396
00:48:24,196 --> 0:48:27,086
中真正的方向

1397
00:48:27,646 --> 0:48:31,376
有关图像跟踪的讨论

1398
00:48:31,436 --> 0:48:31,836
就到此为止

1399
00:48:31,836 --> 0:48:34,006
接下来我们要讨论的是

1400
00:48:34,066 --> 0:48:35,266
物体检测

1401
00:48:38,016 --> 0:48:39,546
通过图像跟踪 我们了解了

1402
00:48:39,546 --> 0:48:41,716
我们是如何检测图像的

1403
00:48:41,716 --> 0:48:43,676
而图像在真实生活中

1404
00:48:43,676 --> 0:48:44,406
是一个平面物体

1405
00:48:45,376 --> 0:48:46,786
物品检测将这个概念

1406
00:48:46,786 --> 0:48:48,256
扩展到三维世界

1407
00:48:48,346 --> 0:48:50,586
让检测常用物品成为可能

1408
00:48:51,046 --> 0:48:53,626
但是这个物品

1409
00:48:53,816 --> 0:48:55,826
在场景中需要是静止的

1410
00:48:55,826 --> 0:48:57,086
它与图像不同

1411
00:48:57,086 --> 0:48:57,676
检测图像时是可以移动的

1412
00:48:58,866 --> 0:49:00,046
我们可以看看这个例子

1413
00:48:58,866 --> 0:49:00,046
我们可以看看这个例子

1414
00:49:00,046 --> 0:49:02,566
这是娜芙蒂蒂胸像

1415
00:49:02,716 --> 0:49:04,266
这是一个可以在

1416
00:49:04,266 --> 0:49:05,326
博物馆中展览的雕塑

1417
00:49:05,636 --> 0:49:07,576
现在 你可以用 ARKit 来检测它

1418
00:49:08,216 --> 0:49:10,376
然后可以在真实物品的

1419
00:49:10,376 --> 0:49:14,476
上方显示一些信息

1420
00:49:15,466 --> 0:49:17,536
ARKit 中的物品检测

1421
00:49:17,736 --> 0:49:18,936
指的是有关物品的

1422
00:49:18,936 --> 0:49:21,116
真实实例的检测

1423
00:49:21,646 --> 0:49:22,506
所以我们正在讨论的

1424
00:49:22,506 --> 0:49:23,986
不是检测所有的雕像

1425
00:49:24,346 --> 0:49:26,406
而是检测这一特定的

1426
00:49:26,406 --> 0:49:27,456
娜芙蒂蒂雕像

1427
00:49:28,836 --> 0:49:29,986
我们如何在 ARKit

1428
00:49:29,986 --> 0:49:31,006
中重现这些物体呢

1429
00:49:31,626 --> 0:49:33,736
首先你需要扫描这个物体

1430
00:49:33,806 --> 0:49:35,086
事实上 只需要两步就可以

1431
00:49:35,086 --> 0:49:35,196
完成这个检测

1432
00:49:35,316 --> 0:49:36,936
首先 你对这个对象进行扫描

1433
00:49:36,936 --> 0:49:38,336
然后你就可以检测它

1434
00:49:39,096 --> 0:49:40,546
让我们讨论一下扫描这一部分

1435
00:49:40,546 --> 0:49:42,446
对开发者来说

1436
00:49:42,446 --> 0:49:44,166
扫描这一部分主要是你们的责任

1437
00:49:44,886 --> 0:49:46,016
你们需要负责

1438
00:49:46,016 --> 0:49:47,296
创造可用于

1439
00:49:47,406 --> 0:49:49,176
检测的物体重现

1440
00:49:51,276 --> 0:49:53,716
从内在来看 这个物体

1441
00:49:53,716 --> 0:49:55,616
的重现方式与

1442
00:49:55,616 --> 0:49:56,196
世界地图类似

1443
00:49:56,776 --> 0:49:58,576
在左边 你可以看到

1444
00:49:58,916 --> 0:50:00,456
娜芙蒂蒂胸像上的

1445
00:49:58,916 --> 0:50:00,456
娜芙蒂蒂胸像上的

1446
00:50:00,456 --> 0:50:01,886
3D 特征点

1447
00:50:03,056 --> 0:50:04,876
你可以使用扫描与检测 3D 物体

1448
00:50:05,006 --> 0:50:06,516
的开发者样本来

1449
00:50:06,516 --> 0:50:08,496
扫描物体

1450
00:50:08,496 --> 0:50:10,416
这个样本可在网站上获得

1451
00:50:11,706 --> 0:50:13,136
需要注意的是

1452
00:50:13,136 --> 0:50:14,766
你在运行时的

1453
00:50:14,766 --> 0:50:17,306
检测质量很大程度

1454
00:50:17,306 --> 0:50:18,986
上受扫描质量的影响

1455
00:50:19,776 --> 0:50:21,826
所以 不妨花上一些时间来

1456
00:50:21,826 --> 0:50:23,266
研究在扫描过程中

1457
00:50:23,336 --> 0:50:27,386
如何才能获得最好的质量

1458
00:50:27,546 --> 0:50:29,066
一旦你建立并运行了

1459
00:50:29,066 --> 0:50:30,666
这个开发者样本

1460
00:50:30,786 --> 0:50:32,406
你会在你的设备上 看到与这个类似的东西

1461
00:50:33,286 --> 0:50:35,976
第一步是要找到你

1462
00:50:35,976 --> 0:50:37,926
物体周围的空间

1463
00:50:39,036 --> 0:50:40,066
App 会试图

1464
00:50:40,066 --> 0:50:41,556
自动估算这个

1465
00:50:41,586 --> 0:50:42,946
密封盒子的大小

1466
00:50:42,946 --> 0:50:43,946
探索不同的特征点

1467
00:50:44,896 --> 0:50:46,206
但你可以随时通过

1468
00:50:46,256 --> 0:50:48,856
拖拽这个盒子的边框来

1469
00:50:49,066 --> 0:50:50,726
调整它 使它变大或缩小

1470
00:50:52,876 --> 0:50:55,766
有一点需要特别留意

1471
00:50:55,766 --> 0:50:57,266
当你围着这个物体扫描时

1472
00:50:57,266 --> 0:50:58,796
不能漏掉这个物体

1473
00:50:58,796 --> 0:51:01,166
的任何特征点

1474
00:50:58,796 --> 0:51:01,166
的任何特征点

1475
00:51:01,856 --> 0:51:03,606
你也可以从上方用两指手势

1476
00:51:03,606 --> 0:51:05,146
对盒子进行旋转操作

1477
00:51:05,856 --> 0:51:08,406
你需要确保这个盒子

1478
00:51:08,606 --> 0:51:09,666
围绕着物体

1479
00:51:09,696 --> 0:51:11,286
而且包括了这一物体的

1480
00:51:11,916 --> 0:51:11,986
所有特征点

1481
00:51:13,016 --> 0:51:15,246
接下来就是真正的扫描部分

1482
00:51:16,376 --> 0:51:19,356
在这个阶段

1483
00:51:19,356 --> 0:51:21,426
我们希望能真正的从各个

1484
00:51:21,736 --> 0:51:23,486
角度来观察这个对象

1485
00:51:23,486 --> 0:51:24,846
任何你觉得你的用户可能会想要

1486
00:51:24,846 --> 0:51:25,806
检测的角度都需要被考虑到

1487
00:51:27,056 --> 0:51:28,386
为了让你能更容易的

1488
00:51:28,386 --> 0:51:30,096
统计物体哪部分

1489
00:51:30,096 --> 0:51:31,656
已经被获取了

1490
00:51:31,656 --> 0:51:33,376
就像对这个美丽的

1491
00:51:33,376 --> 0:51:34,356
地板的重现

1492
00:51:34,726 --> 0:51:36,046
你可以看到在顶部

1493
00:51:36,456 --> 0:51:37,676
有一个百分比

1494
00:51:37,676 --> 0:51:38,786
它能告诉你有多少块地板

1495
00:51:38,786 --> 0:51:39,366
已经被获取了

1496
00:51:40,406 --> 0:51:41,786
在这个阶段中

1497
00:51:41,786 --> 0:51:43,716
你需要在物体有着

1498
00:51:43,716 --> 0:51:45,126
许多特征点的 或者

1499
00:51:45,126 --> 0:51:46,546
有明显区别性的区域花上足够的时间

1500
00:51:46,726 --> 0:51:47,726
这是十分重要的

1501
00:51:47,726 --> 0:51:49,446
你需要走近对象去

1502
00:51:49,446 --> 0:51:50,346
捕获所有细节

1503
00:51:50,686 --> 0:51:52,046
你还需要从所有

1504
00:51:52,046 --> 0:51:56,246
角度对对象进行扫描

1505
00:51:56,436 --> 0:51:59,456
就好像你在这里看到的

1506
00:51:59,686 --> 0:52:01,186
如果你对你已经获取的

1507
00:51:59,686 --> 0:52:01,186
如果你对你已经获取的

1508
00:52:01,186 --> 0:52:02,996
信息感到满意了

1509
00:52:02,996 --> 0:52:04,476
你可以进入下一步

1510
00:52:04,476 --> 0:52:06,656
你可以通过简单的拖拽

1511
00:52:06,656 --> 0:52:09,166
在颜色系统中

1512
00:52:09,166 --> 0:52:10,026
调整原点

1513
00:52:10,536 --> 0:52:12,496
这个系统将会

1514
00:52:12,496 --> 0:52:13,886
是锚点在检测阶段时

1515
00:52:13,886 --> 0:52:16,506
反馈给你的系统

1516
00:52:16,506 --> 0:52:17,766
所以你要确保你将它

1517
00:52:17,766 --> 0:52:19,316
放置在一个适合你的

1518
00:52:19,316 --> 0:52:20,606
虚拟内容的位置上

1519
00:52:20,676 --> 0:52:25,526
现在 你已经有了

1520
00:52:25,676 --> 0:52:27,286
有关物体的完整重现

1521
00:52:27,286 --> 0:52:30,686
这个重现可以用来检测

1522
00:52:30,686 --> 0:52:33,106
现在 App 会转换到

1523
00:52:33,106 --> 0:52:34,976
检测模式

1524
00:52:36,106 --> 0:52:37,386
我们强力建议你使用这一模式

1525
00:52:37,386 --> 0:52:39,966
以尽早获得有关

1526
00:52:39,966 --> 0:52:41,046
检测质量的反馈

1527
00:52:41,866 --> 0:52:44,776
你可能会想要

1528
00:52:44,776 --> 0:52:46,126
从不同的角度来观察这个物体

1529
00:52:46,126 --> 0:52:47,796
以确认这个物体

1530
00:52:47,796 --> 0:52:49,776
已经从所有不同角度

1531
00:52:50,336 --> 0:52:51,696
检测过了

1532
00:52:51,696 --> 0:52:53,816
你可以将你的设备移开

1533
00:52:53,816 --> 0:52:55,086
然后从另一个角度拿回来

1534
00:52:55,836 --> 0:52:58,386
你需要确保扫描质量好到可以

1535
00:52:58,386 --> 0:53:00,006
用于检测物体

1536
00:52:58,386 --> 0:53:00,006
用于检测物体

1537
00:53:00,526 --> 0:53:03,106
你也可以移动这些物体

1538
00:53:03,276 --> 0:53:05,866
这样 光照条件就会不同

1539
00:53:06,836 --> 0:53:08,216
你需要确保在这些情况下

1540
00:53:08,216 --> 0:53:09,196
物体还能被检测到

1541
00:53:09,196 --> 0:53:10,316
对像玩具这样的物体而言

1542
00:53:10,366 --> 0:53:12,536
这是十分重要的

1543
00:53:12,536 --> 0:53:13,646
因为你并不知道

1544
00:53:13,646 --> 0:53:15,546
它们的真实位置

1545
00:53:17,096 --> 0:53:18,776
我们也建议你将物体

1546
00:53:18,776 --> 0:53:21,556
放在一个完全不同的环境中

1547
00:53:22,016 --> 0:53:24,576
但还能确保它能被检测到

1548
00:53:25,666 --> 0:53:27,756
如果这个物体不能被检测到

1549
00:53:27,756 --> 0:53:28,806
那你可能需要回到

1550
00:53:28,806 --> 0:53:31,406
扫描环节重新扫描

1551
00:53:31,406 --> 0:53:32,506
并确保你所处的环境光线充足

1552
00:53:33,786 --> 0:53:35,956
光线充足的环境

1553
00:53:35,956 --> 0:53:37,916
在扫描时是至关重要的

1554
00:53:38,506 --> 0:53:39,776
如果你使用的是 Verilux 灯泡

1555
00:53:39,776 --> 0:53:41,916
那 500 lux 是最佳的亮度

1556
00:53:43,046 --> 0:53:45,006
如果这个亮度也不够

1557
00:53:45,166 --> 0:53:46,556
你可能需要保存不同的

1558
00:53:46,556 --> 0:53:50,446
扫描版本

1559
00:53:50,576 --> 0:53:51,826
当你对检测质量

1560
00:53:51,826 --> 0:53:53,266
感到满意时

1561
00:53:53,266 --> 0:53:55,136
你只需要将这个模型

1562
00:53:55,136 --> 0:53:57,846
存入你的 Mac 里

1563
00:53:57,846 --> 0:53:59,656
并将它添加到 AR 资源群组中

1564
00:53:59,746 --> 0:54:03,016
就像你对图像所做的一样

1565
00:53:59,746 --> 0:54:03,016
就像你对图像所做的一样

1566
00:54:03,016 --> 0:54:04,526
有一些物体

1567
00:54:04,526 --> 0:54:07,366
十分适合这个系统

1568
00:54:07,586 --> 0:54:08,706
这些物体就像

1569
00:54:08,706 --> 0:54:08,986
屏幕左侧展示的一样

1570
00:54:09,586 --> 0:54:10,956
首先 它们都是刚性物体

1571
00:54:10,956 --> 0:54:13,206
纹理丰富

1572
00:54:13,206 --> 0:54:14,696
区分明显

1573
00:54:15,436 --> 0:54:16,416
但也有着一些

1574
00:54:16,416 --> 0:54:18,506
物体不适用于这一系统

1575
00:54:19,066 --> 0:54:22,136
可以参考屏幕右侧的物体

1576
00:54:22,686 --> 0:54:24,756
像金属的

1577
00:54:24,826 --> 0:54:26,606
透明的 或者金属的

1578
00:54:26,606 --> 0:54:27,796
反光的物体不适合用于

1579
00:54:27,856 --> 0:54:28,096
这个系统

1580
00:54:29,206 --> 0:54:31,496
透明的物体

1581
00:54:31,576 --> 0:54:32,796
比如说玻璃材质的物体

1582
00:54:32,796 --> 0:54:34,376
也不适用于这个系统

1583
00:54:34,376 --> 0:54:35,236
因为这些物体的外观

1584
00:54:35,236 --> 0:54:37,766
受它们所处的场景影响

1585
00:54:38,816 --> 0:54:38,936
太大

1586
00:54:39,756 --> 0:54:41,506
这就是如何扫描物体

1587
00:54:41,706 --> 0:54:43,126
让我再强调一次

1588
00:54:43,126 --> 0:54:44,026
确保你的环境光线充足

1589
00:54:44,996 --> 0:54:46,596
接下来要了解的是

1590
00:54:46,646 --> 0:54:48,056
我们如何在 ARKit 中检测到这一操作

1591
00:54:50,046 --> 0:54:51,956
如果你觉得这个很眼熟

1592
00:54:51,956 --> 0:54:53,606
那时因为这个 API

1593
00:54:53,606 --> 0:54:55,006
与其中之一的图像十分相似

1594
00:54:55,586 --> 0:54:56,716
我们会有很方便的特征

1595
00:54:56,816 --> 0:54:58,586
来将所有对象集中到

1596
00:54:58,586 --> 0:54:58,896
一个群组中

1597
00:54:59,506 --> 0:55:00,656
在这里 指的是

1598
00:54:59,506 --> 0:55:00,656
在这里 指的是

1599
00:55:00,656 --> 0:55:01,916
ARReferenceObjects 这一类

1600
00:55:02,806 --> 0:55:05,136
当你配置你的

1601
00:55:05,136 --> 0:55:07,056
ARWorldTracking 参数时

1602
00:55:07,056 --> 0:55:08,516
你只需要将这个对象

1603
00:55:08,516 --> 0:55:10,876
传输到 the detectionObjects 的特征下

1604
00:55:13,206 --> 0:55:15,566
一旦你开始运行这个会话

1605
00:55:15,566 --> 0:55:17,236
你会收到结果反馈

1606
00:55:18,306 --> 0:55:19,356
在这里 你会需要

1607
00:55:19,356 --> 0:55:21,656
检查 ARObjectAnchor

1608
00:55:22,386 --> 0:55:23,656
它能为你提供现实生活中

1609
00:55:23,786 --> 0:55:25,436
物体的位置

1610
00:55:25,436 --> 0:55:27,866
以及方向

1611
00:55:28,716 --> 0:55:30,346
物体的名字也会

1612
00:55:30,346 --> 0:55:35,246
在资产目录中得到展示

1613
00:55:35,246 --> 0:55:36,516
你可能已经注意到

1614
00:55:36,706 --> 0:55:38,316
物体检测与

1615
00:55:38,316 --> 0:55:40,506
世界匹配重新定位之间

1616
00:55:40,506 --> 0:55:42,726
有着某些类似之处

1617
00:55:43,426 --> 0:55:44,806
但它们也有着一些不同

1618
00:55:44,856 --> 0:55:46,316
在物体检测时

1619
00:55:46,316 --> 0:55:48,406
我们是根据

1620
00:55:48,746 --> 0:55:50,946
现实世界 赋予物体位置

1621
00:55:51,506 --> 0:55:52,426
而在世界地图的重新定位时

1622
00:55:52,426 --> 0:55:53,966
是相机本身

1623
00:55:53,966 --> 0:55:56,066
调整了之前的

1624
00:55:56,066 --> 0:55:56,816
世界地图

1625
00:55:58,286 --> 0:56:01,336
而且 你还可以检测多个物体

1626
00:55:58,286 --> 0:56:01,336
而且 你还可以检测多个物体

1627
00:56:01,966 --> 0:56:03,626
物体检测十分适合

1628
00:56:03,736 --> 0:56:05,306
那些在放置在桌面上

1629
00:56:05,496 --> 0:56:06,376
家具大小的物体

1630
00:56:07,146 --> 0:56:08,446
而世界地图

1631
00:56:08,446 --> 0:56:10,266
获取的则是整个场景

1632
00:56:10,776 --> 0:56:14,576
以上便是有关物品检测的信息

1633
00:56:14,686 --> 0:56:16,266
让我们来总结一下我们

1634
00:56:17,796 --> 0:56:19,336
今天所讲过的东西

1635
00:56:19,516 --> 0:56:21,876
方向跟踪只能跟踪

1636
00:56:21,876 --> 0:56:23,696
设备的旋转

1637
00:56:23,696 --> 0:56:25,756
可以用于探索

1638
00:56:25,756 --> 0:56:26,416
静止的环境

1639
00:56:27,946 --> 0:56:29,266
世界跟踪是

1640
00:56:29,266 --> 0:56:30,776
全特征的位置和

1641
00:56:30,776 --> 0:56:32,246
方向跟踪

1642
00:56:32,246 --> 0:56:33,926
它能根据世界原点为你

1643
00:56:33,926 --> 0:56:36,286
提供设备的位置

1644
00:56:37,016 --> 0:56:38,776
并启用有关理解场景的所有

1645
00:56:38,776 --> 0:56:40,486
相关的功能

1646
00:56:41,166 --> 0:56:43,366
比如平面检测

1647
00:56:44,476 --> 0:56:46,256
平面检测让你可以与真实的

1648
00:56:46,256 --> 0:56:50,226
水平或垂直的平面互动

1649
00:56:50,226 --> 0:56:51,636
你可以在这些平面上放置虚拟物体

1650
00:56:51,996 --> 0:56:55,306
我们也了解了你可以通过

1651
00:56:55,656 --> 0:56:57,036
框架中的保存和上传

1652
00:56:57,036 --> 0:56:58,926
地图特征的功能创造

1653
00:56:58,926 --> 0:57:00,566
一个持续的或者多用户的体验

1654
00:56:58,926 --> 0:57:00,566
一个持续的或者多用户的体验

1655
00:57:01,516 --> 0:57:03,036
还有你如何才能用图像跟踪来

1656
00:57:03,036 --> 0:57:04,816
检测物理图像并以

1657
00:57:04,816 --> 0:57:06,096
每秒 60 帧的速度来跟踪它们

1658
00:57:06,096 --> 0:57:08,596
以及你是如何通过物体跟踪

1659
00:57:08,596 --> 0:57:11,176
来检测更多常用物体

1660
00:57:12,576 --> 0:57:15,216
我希望通过这个演讲

1661
00:57:15,216 --> 0:57:17,496
你们现在对 ARkit 中的所有

1662
00:57:17,496 --> 0:57:19,126
不同的跟踪技术以及

1663
00:57:19,586 --> 0:57:20,816
它们是如何运作的

1664
00:57:20,816 --> 0:57:22,016
有更清晰的理解

1665
00:57:23,006 --> 0:57:24,666
以及更了解你如何才能

1666
00:57:24,806 --> 0:57:25,976
获得最佳的跟踪质量

1667
00:57:26,436 --> 0:57:28,106
我们非常期待

1668
00:57:28,106 --> 0:57:29,226
你即将利用

1669
00:57:29,226 --> 0:57:29,656
ARKit 所创造出来的作品

1670
00:57:30,976 --> 0:57:32,416
更多的信息可以在

1671
00:57:32,486 --> 0:57:33,636
开发者网站上的

1672
00:57:33,636 --> 0:57:34,496
演讲链接中获得

1673
00:57:34,496 --> 0:57:36,226
明天早上 9 点

1674
00:57:36,536 --> 0:57:37,586
将会有 ARKit 实验室的活动

1675
00:57:38,356 --> 0:57:39,806
我和 Marion 都会

1676
00:57:39,806 --> 0:57:41,786
在现场回答有关

1677
00:57:41,786 --> 0:57:42,666
ARKit 的任何问题

1678
00:57:43,756 --> 0:57:44,976
最后 十分感谢大家

1679
00:57:44,976 --> 0:57:46,976
希望大家能享受这次知识冲击

1680
00:57:47,516 --> 0:57:53,506
[ 掌声 ]
