1
00:00:16,917 --> 0:00:19,887
（Vision与Core ML

2
00:00:19,953 --> 0:00:20,988
（演讲717）

3
00:00:25,292 --> 0:00:28,061
下午好 欢迎来到WWDC

4
00:00:28,328 --> 0:00:30,831
我知道我现正与咖啡和饼干竞争

5
00:00:30,898 --> 0:00:33,100
但我不知道咖啡是否是无麸质的

6
00:00:33,166 --> 0:00:34,701
以及饼干是否不含咖啡因

7
00:00:34,768 --> 0:00:35,869
所以坚持下去

8
00:00:36,203 --> 0:00:39,373
我叫Frank Doepke

9
00:00:39,439 --> 0:00:43,310
和Vision框架做的

10
00:00:44,244 --> 0:00:45,779
那么今天我们要谈什么呢？

11
00:00:46,580 --> 0:00:49,183
首先 你可能听说过

12
00:00:49,249 --> 0:00:52,152
为你准备了一些特别的东西

13
00:00:53,220 --> 0:00:55,389
然后我们将谈一下对象识别

14
00:00:56,356 --> 0:00:59,693
最后但同样重要的是

15
00:00:59,760 --> 0:01:01,862
来提升你对Vision的理解水平

16
00:00:59,760 --> 0:01:01,862
来提升你对Vision的理解水平

17
00:01:04,164 --> 0:01:06,066
现在先看看自定义图像分类

18
00:01:07,267 --> 0:01:11,638
我们已经在之前的演示中

19
00:01:11,705 --> 0:01:14,141
我们看到比如

20
00:01:14,208 --> 0:01:17,311
我和其他人一样喜欢鲜花和水果

21
00:01:17,411 --> 0:01:20,214
抱歉 Lizzie

22
00:01:20,848 --> 0:01:25,352
我的意思是 如果我们创建一个商店

23
00:01:25,419 --> 0:01:27,421
所以让我们创建一个

24
00:01:27,487 --> 0:01:29,690
因此我们需要识别一些零件

25
00:01:30,357 --> 0:01:33,727
我认为如果我有一个

26
00:01:33,794 --> 0:01:36,797
这些物体是什么的app会很棒

27
00:01:38,232 --> 0:01:40,901
所以我们必须为此

28
00:01:41,602 --> 0:01:42,436
然后

29
00:01:43,170 --> 0:01:47,040
一旦我们有了分类器

30
00:01:47,307 --> 0:01:49,309
其可以在我们的设备上运行

31
00:01:50,110 --> 0:01:54,281
在经历这些过程时

32
00:01:54,348 --> 0:01:56,216
遇到的常见陷阱

33
00:01:56,517 --> 0:01:58,151
并尝试引导你避开它们

34
00:02:00,854 --> 0:02:02,356
让我们从训练开始吧

35
00:02:03,323 --> 0:02:05,526
我们如何训练呢？

36
00:02:07,060 --> 0:02:10,229
当然 我们要做的第一步是

37
00:02:11,231 --> 0:02:13,567
然后我们将照片放在文件夹中

38
00:02:13,634 --> 0:02:16,870
我们使用文件夹名称作为分类标签

39
00:02:18,805 --> 0:02:21,341
现在 每个人都会问的一个问题是

40
00:02:21,408 --> 0:02:22,743
“我需要多少数据”

41
00:02:23,977 --> 0:02:29,850
首先 我们每个类别

42
00:02:29,917 --> 0:02:33,487
但这是偏低的 你肯定想拥有更多

43
00:02:33,554 --> 0:02:36,657
越多越好

44
00:02:38,425 --> 0:02:41,528
需要注意的另一件事是

45
00:02:41,595 --> 0:02:42,629
这是什么意思呢？

46
00:02:42,930 --> 0:02:46,033
当数据集中的一个类别

47
00:02:46,099 --> 0:02:49,469
而在另一个类别中只有十个时

48
00:02:49,536 --> 0:02:51,905
因此你想在大多数类别之间

49
00:02:51,972 --> 0:02:53,574
拥有一个更为均匀的分布

50
00:02:55,609 --> 0:02:59,413
我们引入的另一件事是增强

51
00:03:00,013 --> 0:03:03,217
增强将帮助你使此模型更加强大

52
00:03:04,017 --> 0:03:05,919
但它并没有取代多样性

53
00:03:05,986 --> 0:03:10,691
所以你仍然希望拥有许多

54
00:03:11,258 --> 0:03:13,694
但通过增强 我们要做的是

55
00:03:13,760 --> 0:03:15,596
我们拍摄一张照片并扰乱它

56
00:03:15,996 --> 0:03:19,399
我们对其添加噪声 模糊它

57
00:03:19,466 --> 0:03:22,503
所以当我们训练它时

58
00:03:24,738 --> 0:03:27,474
我们来看一下训练的实际运作方式

59
00:03:29,343 --> 0:03:32,346
你可能已经听过

60
00:03:32,412 --> 0:03:35,616
这正是当我们训练分类器时

61
00:03:36,283 --> 0:03:38,385
我们从一个预训练模型开始

62
00:03:38,452 --> 0:03:40,554
这是所有繁重过程发生的地方

63
00:03:40,621 --> 0:03:44,958
这些模型通常训练数周

64
00:03:45,158 --> 0:03:48,562
这是你需要实际使用它的第一个起点

65
00:03:50,364 --> 0:03:53,800
我们可以将该模型当做特征提取器

66
00:03:53,867 --> 0:03:56,670
这可以给我们一个特征向量

67
00:03:56,737 --> 0:03:58,305
它是我们图像中物体的数字描述

68
00:03:59,373 --> 0:04:02,242
现在你引入自己的数据

69
00:03:59,373 --> 0:04:02,242
现在你引入自己的数据

70
00:04:02,309 --> 0:04:05,078
我们称之为最后一层

71
00:04:05,579 --> 0:04:09,183
最终输出你的自定义模型

72
00:04:11,285 --> 0:04:14,588
我提到了这个大型的首次预训练模型

73
00:04:15,322 --> 0:04:17,257
本次演讲中

74
00:04:17,324 --> 0:04:20,127
我们称之为面向场景的

75
00:04:20,961 --> 0:04:22,696
它可在Create ML中使用

76
00:04:23,363 --> 0:04:25,866
并允许你训练图像分类器

77
00:04:27,668 --> 0:04:30,304
它已经在一个非常大的

78
00:04:31,371 --> 0:04:35,042
它能够分类超过一千个类别

79
00:04:35,309 --> 0:04:39,046
这是一个你可以使用的

80
00:04:40,681 --> 0:04:41,782
我们已经使用过它了

81
00:04:41,849 --> 0:04:44,484
在过去三年中

82
00:04:44,551 --> 0:04:48,522
一些面向用户的功能

83
00:04:49,923 --> 0:04:53,493
我们也将继续改进该模型

84
00:04:53,560 --> 0:04:55,329
我想在此强调一下

85
00:04:56,630 --> 0:04:59,333
当我们推出该模型的新版本时

86
00:04:59,399 --> 0:05:01,768
你不一定会自动获得它的改进

87
00:04:59,399 --> 0:05:01,768
你不一定会自动获得它的改进

88
00:05:01,835 --> 0:05:04,071
除非你使用新模型重新训练

89
00:05:04,605 --> 0:05:06,840
所以如果你今年开始用它来开发

90
00:05:06,907 --> 0:05:07,774
并且你想

91
00:05:07,841 --> 0:05:10,611
利用我们未来几年推出的特性

92
00:05:10,944 --> 0:05:14,581
保留你的数据集

93
00:05:16,950 --> 0:05:19,920
再谈一些关于我们的

94
00:05:21,889 --> 0:05:23,357
它已经存在在设备上

95
00:05:23,423 --> 0:05:25,859
这对我们来说是一个重要的决定

96
00:05:26,460 --> 0:05:31,265
因为它使你的模型的

97
00:05:32,032 --> 0:05:33,500
让我们稍微比较一下

98
00:05:34,001 --> 0:05:38,505
我选择了一些我们今天常用的模型

99
00:05:38,872 --> 0:05:42,676
首先是Resnet

100
00:05:42,743 --> 0:05:45,746
我的模型有多大呢？

101
00:05:47,281 --> 0:05:50,083
如果我使用Squeezenet

102
00:05:50,150 --> 0:05:53,086
它无法区分很多的类别

103
00:05:53,754 --> 0:05:55,289
它的大小为5兆字节

104
00:05:55,355 --> 0:05:58,859
所以这很不错

105
00:05:59,493 --> 0:06:00,594
Vision怎样？

106
00:05:59,493 --> 0:06:00,594
Vision怎样？

107
00:06:01,728 --> 0:06:03,931
在大多数情况下

108
00:06:06,233 --> 0:06:09,403
我们相信这对你来说

109
00:06:10,070 --> 0:06:11,338
它已经经过优化

110
00:06:11,638 --> 0:06:15,375
我们了解我们的硬件

111
00:06:15,442 --> 0:06:17,044
我们对该模型进行了大量优化

112
00:06:17,878 --> 0:06:21,415
因此它在我们的设备上表现最佳

113
00:06:24,318 --> 0:06:25,853
我们如何使用它来训练呢？

114
00:06:26,954 --> 0:06:31,058
我们从一些带标签的图像开始

115
00:06:31,391 --> 0:06:35,395
而Create ML知道如何从中提取

116
00:06:36,563 --> 0:06:38,198
它训练我们的分类器

117
00:06:38,465 --> 0:06:42,402
并且该分类器实际上就是最终

118
00:06:42,469 --> 0:06:43,837
这就是它如此之小的原因

119
00:06:45,372 --> 0:06:48,141
当我真正想要分析图像的时候

120
00:06:48,809 --> 0:06:53,747
我所要做的就是使用我的图像和模型

121
00:06:53,814 --> 0:06:57,618
在Vision或Core ML中

122
00:06:57,885 --> 0:07:01,855
使用我们的

123
00:06:57,885 --> 0:07:01,855
使用我们的

124
00:07:01,922 --> 0:07:04,124
并向我们返回分类

125
00:07:07,227 --> 0:07:09,997
这就是我们需要了解的

126
00:07:10,864 --> 0:07:14,101
但我说过当我们处理app时

127
00:07:14,168 --> 0:07:15,636
还有一些注意事项

128
00:07:16,637 --> 0:07:17,471
所以

129
00:07:18,605 --> 0:07:20,474
首先 我们只想

130
00:07:20,541 --> 0:07:22,676
在必要的时候才运行我们的分类器

131
00:07:24,444 --> 0:07:26,513
分类器是深度卷积网络

132
00:07:26,580 --> 0:07:28,615
它是计算密集型的

133
00:07:28,682 --> 0:07:32,219
所以当我们运行它时

134
00:07:32,452 --> 0:07:35,155
CPU和GPU上运行的一些电子

135
00:07:35,422 --> 0:07:38,058
所以除非你真的需要

136
00:07:38,892 --> 0:07:41,862
在我稍后演示的例子中

137
00:07:42,696 --> 0:07:46,366
我实际上只想在用户

138
00:07:46,433 --> 0:07:48,235
而不是在相机四处移动时

139
00:07:50,037 --> 0:07:53,073
所以我问的问题是

140
00:07:53,140 --> 0:07:55,075
然后我才运行我的分类器

141
00:07:56,009 --> 0:07:59,847
我该怎么做？

142
00:08:00,080 --> 0:08:03,917
配准意味着我可以拍摄两张图像

143
00:08:03,984 --> 0:08:06,954
它会告诉我

144
00:08:07,020 --> 0:08:09,022
它们就能完全匹配”

145
00:08:09,523 --> 0:08:13,460
这是一个非常便宜且快速的算法

146
00:08:13,527 --> 0:08:17,164
我是否拿稳了相机

147
00:08:19,666 --> 0:08:22,536
我使用了VNTranslationalImage

148
00:08:22,603 --> 0:08:27,674
我知道这很绕口

149
00:08:28,108 --> 0:08:31,011
为了将其可视化

150
00:08:31,445 --> 0:08:35,082
我在这个视频中所做的是

151
00:08:35,148 --> 0:08:37,017
它会告诉我在过去几帧中

152
00:08:37,083 --> 0:08:39,919
相机如何移动

153
00:08:39,986 --> 0:08:41,621
或者说配准请求如何求移动

154
00:08:41,688 --> 0:08:44,925
所以注意看那条黄线

155
00:08:44,992 --> 0:08:47,327
移动了很多

156
00:08:47,394 --> 0:08:48,962
则它应该是一条很短的线

157
00:08:50,964 --> 0:08:53,734
你可以看到相机正在移动

158
00:08:54,434 --> 0:08:55,602
它变得很短

159
00:08:56,937 --> 0:08:59,239
这是个好主意

160
00:08:59,306 --> 0:09:01,041
现在可以运行我的分类器了”

161
00:08:59,306 --> 0:09:01,041
现在可以运行我的分类器了”

162
00:09:03,677 --> 0:09:05,312
要记住的下一件事是

163
00:09:05,946 --> 0:09:07,047
制定备用计划

164
00:09:07,614 --> 0:09:09,049
有一个备用计划总是好的

165
00:09:09,750 --> 0:09:11,852
分类可能有误

166
00:09:13,554 --> 0:09:14,421
这意味着

167
00:09:14,488 --> 0:09:17,457
即使我的分类具有很高的置信度

168
00:09:17,524 --> 0:09:21,094
我还是需要为它无法

169
00:09:22,462 --> 0:09:24,431
我在这个例子中做的一件事是

170
00:09:24,498 --> 0:09:26,500
正如你稍后将看到的那样

171
00:09:26,567 --> 0:09:29,236
其中没有实体对象

172
00:09:29,603 --> 0:09:31,772
我的例子中

173
00:09:31,839 --> 0:09:33,507
条形码检测器

174
00:09:33,574 --> 0:09:36,343
来读取条形码标签以识别它

175
00:09:37,211 --> 0:09:38,645
我们看的幻灯片足够多了

176
00:09:39,313 --> 0:09:40,447
谁想看演示

177
00:09:52,993 --> 0:09:56,997
你在屏幕右侧看到的是我的设备

178
00:09:57,464 --> 0:10:00,767
我现在启动我的小机器人商店app

179
00:09:57,464 --> 0:10:00,767
我现在启动我的小机器人商店app

180
00:10:01,568 --> 0:10:04,438
你可以看到当我四处移动时

181
00:10:04,738 --> 0:10:07,474
当我静止不动并且对准某些东西时

182
00:10:07,674 --> 0:10:09,877
我可以看到一条黄线

183
00:10:09,943 --> 0:10:11,512
没错 这的确是一个步进电机

184
00:10:12,112 --> 0:10:14,381
让我们看看这张桌子上还有什么

185
00:10:18,418 --> 0:10:19,953
那是我的微控制器

186
00:10:23,323 --> 0:10:24,625
这是一个步进电机驱动器

187
00:10:25,092 --> 0:10:27,661
我们也可以拿起一件东西

188
00:10:29,596 --> 0:10:31,365
是的 这是一个闭环带

189
00:10:35,269 --> 0:10:37,137
这是什么

190
00:10:37,638 --> 0:10:40,741
如我所说 你也可以查看条形码

191
00:10:41,408 --> 0:10:43,143
我需要把数据线拉长一点

192
00:10:46,013 --> 0:10:47,848
这是我的培训课程

193
00:10:48,382 --> 0:10:49,383
-嘿 Frank

194
00:10:49,750 --> 0:10:50,617
Frank？

195
00:10:51,685 --> 0:10:52,519
发生了什么事？

196
00:10:53,420 --> 0:10:54,254
Frank

197
00:10:56,256 --> 0:10:59,927
我需要你在演示中添加

198
00:11:00,060 --> 0:11:01,728
这是Brett 他是我的经理

199
00:11:04,331 --> 0:11:05,165
那样就太好了

200
00:11:08,468 --> 0:11:11,338
像往常一样 管理层

201
00:11:12,272 --> 0:11:14,174
我将确保你得到该备忘录的

202
00:11:15,275 --> 0:11:17,444
我可不打算周六来干这事儿

203
00:11:18,212 --> 0:11:20,948
好吧 这是什么？

204
00:11:21,982 --> 0:11:24,318
好吧 让我们看看

205
00:11:25,586 --> 0:11:26,753
让我试一试

206
00:11:28,088 --> 0:11:32,259
我能搞定它吗？

207
00:11:32,326 --> 0:11:33,160
这样呢？

208
00:11:33,827 --> 0:11:36,797
不 它不是步进电机

209
00:11:37,831 --> 0:11:39,132
我想我们需解决这个问题

210
00:11:39,800 --> 0:11:40,901
谁想解决这个问题

211
00:11:42,536 --> 0:11:43,403
噢

212
00:11:44,371 --> 0:11:47,374
谁想解决这个问题

213
00:11:49,376 --> 0:11:52,246
所以我现在要做的就是

214
00:11:52,312 --> 0:11:54,948
拍摄一些该伺服电机的照片

215
00:11:55,449 --> 0:11:59,219
我可能需要去工作室

216
00:11:59,820 --> 0:12:03,991
或者 我可以使用

217
00:11:59,820 --> 0:12:03,991
或者 我可以使用

218
00:12:05,259 --> 0:12:06,827
现在 让我们看看

219
00:12:06,894 --> 0:12:09,730
我们将拍摄一堆伺服电机的照片

220
00:12:12,366 --> 0:12:13,700
而且我需要

221
00:12:13,767 --> 0:12:16,870
从不同角度拍摄它

222
00:12:23,110 --> 0:12:26,980
我需要至少十张不同的图像

223
00:12:27,447 --> 0:12:30,784
很好的选择

224
00:12:35,189 --> 0:12:37,391
并确保拍到这些角落

225
00:12:37,457 --> 0:12:39,960
再把它放在手上

226
00:12:46,834 --> 0:12:49,503
好的 我们现在有了很多图像

227
00:12:51,271 --> 0:12:52,472
现在我要打开Mac

228
00:12:52,539 --> 0:12:55,609
并实际向你展示如何对其进行训练

229
00:12:57,144 --> 0:12:57,978
好的

230
00:12:59,546 --> 0:13:02,015
我打开Image Capture app

231
00:12:59,546 --> 0:13:02,015
我打开Image Capture app

232
00:13:02,082 --> 0:13:06,253
让我暂时隐藏

233
00:13:09,089 --> 0:13:10,757
如果我现在查看我的Finder

234
00:13:10,824 --> 0:13:12,693
你可以看到这是我的训练集

235
00:13:12,759 --> 0:13:15,796
我之前用它们来训练

236
00:13:15,863 --> 0:13:17,164
那个模型

237
00:13:17,764 --> 0:13:19,600
我现在需要创建一个新文件夹

238
00:13:21,168 --> 0:13:22,736
让我们称之为Servo

239
00:13:25,772 --> 0:13:30,644
在Image Capture中

240
00:13:34,481 --> 0:13:37,584
拖入我的Servo文件夹

241
00:13:41,088 --> 0:13:44,191
好的 现在我们已经添加了它们

242
00:13:44,558 --> 0:13:46,293
现在我需要再次训练我的模型

243
00:13:46,860 --> 0:13:51,031
因为我的经理刚刚打乱了

244
00:13:52,165 --> 0:13:55,135
我在这里使用了一个

245
00:13:55,536 --> 0:13:57,538
而不是使用用户界面

246
00:13:57,604 --> 0:14:00,040
我稍后想将它作为构建步骤

247
00:13:57,604 --> 0:14:00,040
我稍后想将它作为构建步骤

248
00:14:00,107 --> 0:14:01,441
合并到我的app中

249
00:14:02,342 --> 0:14:06,113
我将其指向我们刚添加到

250
00:14:06,613 --> 0:14:08,949
然后只是简单地训练我的分类器

251
00:14:09,349 --> 0:14:11,318
最后保存我的模型

252
00:14:12,686 --> 0:14:16,757
现在发生的是 你可以看到

253
00:14:17,324 --> 0:14:23,263
它将遍历我们文件夹中的

254
00:14:23,330 --> 0:14:25,432
并从中提取场景打印

255
00:14:25,532 --> 0:14:28,068
它进行了所有必须的缩放操作

256
00:14:28,335 --> 0:14:32,139
最后将基于此训练出一个模型

257
00:14:32,339 --> 0:14:35,042
这是一项非常复杂的任务

258
00:14:35,108 --> 0:14:38,645
它只需要一行代码

259
00:14:38,712 --> 0:14:41,048
你可以在app中实际使用的模型

260
00:14:41,648 --> 0:14:43,250
它刚刚结束 我们来看看

261
00:14:44,751 --> 0:14:46,053
马上就完成了

262
00:14:47,087 --> 0:14:48,856
瞧 这就是我们的模型

263
00:14:53,060 --> 0:14:54,127
那个模型

264
00:14:54,194 --> 0:14:57,531
我已在我的机器人商店app中

265
00:14:57,598 --> 0:14:58,999
这就是我们现在看到的内容

266
00:15:00,033 --> 0:15:01,768
如你所见 这是我的图像分类器

267
00:15:01,835 --> 0:15:03,937
它的大小为148千字节

268
00:15:04,438 --> 0:15:07,741
这实际上比我的小启动画面还要小

269
00:15:15,849 --> 0:15:17,951
这里有一件我想强调的事

270
00:15:18,018 --> 0:15:19,786
这个图像

271
00:15:21,321 --> 0:15:26,593
也就是我需要传给它的图像

272
00:15:26,660 --> 0:15:30,163
奇怪的参数

273
00:15:31,365 --> 0:15:35,602
好的 现在我有了一个

274
00:15:35,936 --> 0:15:39,039
现在我要打开我复杂的产品数据库

275
00:15:39,106 --> 0:15:40,741
它只是一个键列表

276
00:15:42,109 --> 0:15:44,411
我要把我的Servo添加到那里

277
00:15:48,081 --> 0:15:50,817
我对它进行重命名

278
00:15:52,686 --> 0:15:56,056
给它一个标签

279
00:15:56,390 --> 0:15:59,326
这里该填伺服电机

280
00:16:00,761 --> 0:16:04,798
对于描述 我们可以这样写

281
00:16:04,865 --> 0:16:10,604
一个会嗖嗖响的电机

282
00:16:12,105 --> 0:16:15,843
非常具有技术性

283
00:16:17,211 --> 0:16:18,712
我现在要运行我的app

284
00:16:21,481 --> 0:16:22,316
那是…

285
00:16:28,322 --> 0:16:30,324
好的 我们来试试吧

286
00:16:32,759 --> 0:16:34,027
这就是我们的伺服电机

287
00:16:42,336 --> 0:16:45,239
从另一个角度看

288
00:16:45,305 --> 0:16:47,474
在舞台上现场训练的分类器

289
00:16:47,541 --> 0:16:49,943
从照片开始

290
00:16:50,711 --> 0:16:52,079
我对这个演示着实捏了把汗

291
00:16:55,616 --> 0:16:56,450
谢谢

292
00:17:01,788 --> 0:17:04,625
现在我们已经看到它是如何工作的

293
00:17:04,691 --> 0:17:07,160
但我想在查看代码时强调一些事情

294
00:17:08,862 --> 0:17:10,964
我保证我们将在这里现场写一些代码

295
00:17:12,199 --> 0:17:13,032
好的

296
00:17:14,134 --> 0:17:16,770
让我把一切不需要的东西都关掉

297
00:17:18,704 --> 0:17:19,873
并让它更大一些

298
00:17:22,108 --> 0:17:25,012
好的 我是如何解决这一切的呢？

299
00:17:25,345 --> 0:17:28,949
一开始 我们创建了一个

300
00:17:29,016 --> 0:17:31,885
它被用于进行配准

301
00:17:31,952 --> 0:17:34,821
正如Sergei

302
00:17:35,222 --> 0:17:36,857
它非常擅长跟踪对象

303
00:17:37,925 --> 0:17:41,528
然后创建我的请求

304
00:17:41,595 --> 0:17:43,197
你在这里看到的是 对于配准

305
00:17:43,263 --> 0:17:46,433
只保留最近15个配准结果

306
00:17:46,767 --> 0:17:48,535
然后我会对其做一些分析

307
00:17:48,602 --> 0:17:50,204
来看看我是否真的拿稳了相机

308
00:17:51,238 --> 0:17:53,907
当分析它们时 我要保留一个

309
00:17:53,974 --> 0:17:55,209
用于此目的的缓冲区

310
00:17:55,275 --> 0:17:57,311
这实际上就是分类所运行的地方

311
00:17:58,645 --> 0:18:01,315
并且由于这是一个很耗时的任务

312
00:17:58,645 --> 0:18:01,315
并且由于这是一个很耗时的任务

313
00:18:01,381 --> 0:18:03,217
我将在一个单独的队列上运行它

314
00:18:05,853 --> 0:18:08,689
这是我用来打开

315
00:18:08,755 --> 0:18:11,024
我们刚刚看到的小面板的一些代码

316
00:18:11,458 --> 0:18:14,494
但最重要的一点是

317
00:18:15,095 --> 0:18:16,530
我要完成两项任务

318
00:18:16,597 --> 0:18:18,498
首先是条形码请求

319
00:18:18,632 --> 0:18:20,934
然后是我的分类请求

320
00:18:21,001 --> 0:18:22,970
我设置了我的条形码请求

321
00:18:23,904 --> 0:18:25,672
在我的完成处理程序中

322
00:18:25,739 --> 0:18:28,041
我只是查看

323
00:18:28,909 --> 0:18:32,312
而且因为我只期待一个条形码

324
00:18:32,379 --> 0:18:34,181
所以我只看第一个

325
00:18:34,915 --> 0:18:35,983
我能解码它吗

326
00:18:36,049 --> 0:18:38,519
如果我从中得到一个字符串

327
00:18:38,585 --> 0:18:40,854
这实际上就是我刚才如何用条码打开

328
00:18:40,921 --> 0:18:42,589
“哦 没错 那是我的训练课程”

329
00:18:43,957 --> 0:18:47,027
我将其添加为我想要运行的请求之一

330
00:18:47,294 --> 0:18:49,596
现在我开始设置我的分类

331
00:18:49,930 --> 0:18:53,200
这里我所做的是 我使用分类器

332
00:18:53,267 --> 0:18:56,403
我从包中加载它

333
00:18:57,237 --> 0:18:59,173
并以此创建我的模型

334
00:18:59,339 --> 0:19:02,176
在这种情况下 我没使用

335
00:18:59,339 --> 0:19:02,176
在这种情况下 我没使用

336
00:19:02,242 --> 0:19:05,479
因为这是我的app中使用的

337
00:19:05,546 --> 0:19:09,249
它允许我进行自定义类型的错误处理

338
00:19:09,316 --> 0:19:12,619
但你也可以选择使用

339
00:19:12,686 --> 0:19:16,857
两者都绝对有效

340
00:19:17,124 --> 0:19:20,460
Vision Core ML模型

341
00:19:20,661 --> 0:19:23,697
同样 当请求返回时

342
00:19:23,764 --> 0:19:26,233
这意味着会执行我的完成处理程序

343
00:19:26,733 --> 0:19:30,838
我只是检查

344
00:19:31,705 --> 0:19:33,740
然后我在这里设置此阈值

345
00:19:34,141 --> 0:19:37,277
这是我凭经验设定的置信度目标

346
00:19:37,344 --> 0:19:42,316
我使用的是0.98

347
00:19:43,383 --> 0:19:44,685
为什么我要这样做？

348
00:19:45,185 --> 0:19:46,854
这允许我过滤掉

349
00:19:46,920 --> 0:19:49,256
当我在看某物体时

350
00:19:49,323 --> 0:19:51,491
我们稍后会看到这是什么意思

351
00:19:52,893 --> 0:19:54,595
现在我准备好了所有的请求

352
00:19:55,562 --> 0:19:57,764
当我真正想要执行它们的时候

353
00:19:57,831 --> 0:19:59,466
我在这里创建了一个小函数

354
00:19:59,533 --> 0:20:01,902
它的作用是分析当前图像

355
00:19:59,533 --> 0:20:01,902
它的作用是分析当前图像

356
00:20:02,503 --> 0:20:07,307
在分析它的时候

357
00:20:07,374 --> 0:20:10,944
这对于了解我正如何拿着手机

358
00:20:11,845 --> 0:20:13,280
我创建了图像请求处理程序

359
00:20:13,347 --> 0:20:16,717
在我们当前想要处理的缓冲区上

360
00:20:17,885 --> 0:20:22,022
并且我让它异步地执行它的工作

361
00:20:24,291 --> 0:20:26,860
这就是为了使用Core ML

362
00:20:26,927 --> 0:20:29,530
我所需要做的所有事情

363
00:20:30,264 --> 0:20:34,535
我如何实现场景稳定性部分的呢？

364
00:20:34,601 --> 0:20:36,303
我有一个可以重置的队列

365
00:20:36,770 --> 0:20:39,139
我可以将我的点添加进去

366
00:20:39,940 --> 0:20:44,044
然后我创建了一个函数

367
00:20:44,111 --> 0:20:46,313
它允许我遍历所记录的点队列

368
00:20:46,480 --> 0:20:49,216
并且设定“如果它们全部加起来

369
00:20:49,449 --> 0:20:51,418
只有不到20像素的距离”

370
00:20:51,485 --> 0:20:53,654
同样 这也是我选择的经验值

371
00:20:53,820 --> 0:20:56,256
然后我就知道场景是稳定的

372
00:20:56,323 --> 0:20:58,225
我的相机前面没有东西在移动

373
00:20:59,826 --> 0:21:01,662
然后是我们捕捉输出的部分

374
00:20:59,826 --> 0:21:01,662
然后是我们捕捉输出的部分

375
00:21:01,728 --> 0:21:04,298
这就是

376
00:21:04,364 --> 0:21:05,832
相机的缓冲区对我们的调用

377
00:21:06,567 --> 0:21:08,001
我需要确保

378
00:21:08,068 --> 0:21:10,137
获取到前一个像素缓冲区

379
00:21:10,204 --> 0:21:13,507
因为这是我为了配准

380
00:21:13,974 --> 0:21:17,077
并在完成之后将它们换掉

381
00:21:17,845 --> 0:21:20,347
我使用当前的缓冲区创建了一个

382
00:21:20,414 --> 0:21:21,648
TranslationalImage

383
00:21:22,616 --> 0:21:24,351
然后我在序列请求处理程序上

384
00:21:24,418 --> 0:21:26,520
执行我的请求

385
00:21:27,287 --> 0:21:31,124
现在我得到了我的观察

386
00:21:31,625 --> 0:21:33,227
并将它们添加到我的数组中

387
00:21:34,428 --> 0:21:37,898
最后但同样重要的是

388
00:21:38,498 --> 0:21:43,370
然后我将显示我的小黄框

389
00:21:44,404 --> 0:21:46,974
我知道这是我当前分析的缓冲区

390
00:21:47,708 --> 0:21:52,446
并要求它对此进行分析

391
00:21:54,581 --> 0:21:59,753
我在当前分析的缓冲区中的

392
00:21:59,820 --> 0:22:02,322
我释放了该缓冲区

393
00:21:59,820 --> 0:22:02,322
我释放了该缓冲区

394
00:22:02,856 --> 0:22:06,560
你在这里可以看到我正在检查

395
00:22:06,860 --> 0:22:10,230
这可以让我确保

396
00:22:10,297 --> 0:22:11,532
并且我不会总是在它们

397
00:22:11,598 --> 0:22:13,767
还在后台运行时

398
00:22:13,834 --> 0:22:16,236
因为这会使相机得不到足够的帧

399
00:22:17,371 --> 0:22:18,739
好的 当我们运行的时候

400
00:22:18,805 --> 0:22:20,841
我想要强调一些事情

401
00:22:20,908 --> 0:22:23,210
我们先来讨论第一件事

402
00:22:24,444 --> 0:22:26,480
我们看看底部的控制台

403
00:22:26,680 --> 0:22:30,484
你可以看到 当我刚开始运行时

404
00:22:31,118 --> 0:22:32,886
我现在要在这里运行

405
00:22:35,889 --> 0:22:37,491
以便你能看到它

406
00:22:38,325 --> 0:22:41,261
你可以看到置信度得分很低

407
00:22:41,328 --> 0:22:42,596
我只是在拍摄白色背景

408
00:22:42,663 --> 0:22:45,032
它不确定我正在看的是什么

409
00:22:45,499 --> 0:22:48,735
而当我对准它应该能识别的东西时

410
00:22:50,237 --> 0:22:51,905
我们的置信度得分非常高

411
00:22:51,972 --> 0:22:55,609
这就是我如何确定

412
00:22:56,810 --> 0:22:58,779
我想要演示的另一件事…

413
00:23:01,782 --> 0:23:04,885
让我们看一下CPU的实际情况

414
00:23:07,087 --> 0:23:08,655
好的 现在我什么都没做

415
00:23:08,722 --> 0:23:11,458
我只是在显示我的屏幕

416
00:23:12,159 --> 0:23:14,795
当我四处移动相机时

417
00:23:15,629 --> 0:23:19,533
场景不稳定

418
00:23:19,900 --> 0:23:22,336
现在如果我保持稳定

419
00:23:22,402 --> 0:23:24,037
你可以看到CPU使用率上升

420
00:23:24,671 --> 0:23:28,675
这就是为什么我总是建议

421
00:23:31,044 --> 0:23:33,914
好的 我们刚才讲了很多东西

422
00:23:34,081 --> 0:23:35,549
让我们回到幻灯片

423
00:23:35,949 --> 0:23:38,452
并回顾一下我们刚刚看到的内容

424
00:23:40,254 --> 0:23:41,255
回到幻灯片

425
00:23:48,161 --> 0:23:52,733
好的 回顾一下

426
00:23:53,867 --> 0:23:55,602
我们使用了序列请求处理程序

427
00:23:55,669 --> 0:23:58,739
以及VNTranslationalImage

428
00:24:00,107 --> 0:24:02,075
来与前一帧进行比较

429
00:24:03,343 --> 0:24:08,282
除此之外 我们从alignmentTransform中

430
00:24:08,348 --> 0:24:11,885
它告诉我前一帧如何移动X和Y

431
00:24:12,786 --> 0:24:14,221
从而达到当前这帧

432
00:24:15,923 --> 0:24:19,359
然后我们谈到

433
00:24:20,194 --> 0:24:22,262
为此 我们创建了

434
00:24:22,329 --> 0:24:25,032
当前缓冲区的

435
00:24:25,933 --> 0:24:27,568
并且我们一起传入

436
00:24:27,701 --> 0:24:31,538
条码检测请求和分类请求

437
00:24:32,105 --> 0:24:35,142
这允许Vision在底层进行优化

438
00:24:35,309 --> 0:24:39,079
这比将它们作为单独的请求运行

439
00:24:39,146 --> 0:24:40,547
要快得多

440
00:24:43,083 --> 0:24:47,287
下一部分关于我们在运行时

441
00:24:47,588 --> 0:24:49,656
所以说要“管理你的缓冲区”

442
00:24:50,858 --> 0:24:54,094
有些Vision请求

443
00:24:54,161 --> 0:24:55,362
可能耗费较长时间

444
00:24:55,896 --> 0:24:58,465
这些运行时间更长的任务最好

445
00:24:58,532 --> 0:25:02,803
运行在后台队列上 以便你的UI

446
00:24:58,532 --> 0:25:02,803
运行在后台队列上 以便你的UI

447
00:25:02,870 --> 0:25:04,471
可以继续运行而不会卡死

448
00:25:04,972 --> 0:25:06,840
但在做这点时 特别是对于相机

449
00:25:06,907 --> 0:25:10,143
你不想持续将来自相机的缓冲区

450
00:25:10,210 --> 0:25:14,381
所以你应该放弃繁忙的缓冲区

451
00:25:14,448 --> 0:25:16,950
这实际上在我的用例场景中

452
00:25:17,184 --> 0:25:20,621
我用的是大小为1的队列

453
00:25:20,687 --> 0:25:23,624
只要它正在运行

454
00:25:23,891 --> 0:25:28,161
当其处理完成后

455
00:25:31,131 --> 0:25:32,399
现在你可能会问

456
00:25:32,699 --> 0:25:35,569
当我能在Core ML中运行

457
00:25:35,636 --> 0:25:36,770
这毕竟是Core ML模型啊

458
00:25:38,138 --> 0:25:41,842
以下是使用Vision的

459
00:25:42,242 --> 0:25:45,279
让我们回过头看看我们的模型

460
00:25:45,579 --> 0:25:49,149
这是奇怪的299x299像素参数

461
00:25:49,550 --> 0:25:51,518
这就是这个模型的训练方式

462
00:25:51,585 --> 0:25:53,253
这就是它想要作为输入的内容

463
00:25:53,787 --> 0:25:56,790
但是相机给我们的是

464
00:25:56,857 --> 0:25:58,825
或更高的分辨率

465
00:25:59,793 --> 0:26:03,964
而Vision将完成所有这些工作

466
00:25:59,793 --> 0:26:03,964
而Vision将完成所有这些工作

467
00:26:04,031 --> 0:26:07,234
将其转换为RGB并缩放图像

468
00:26:07,301 --> 0:26:09,036
而你不必为此编写任何代码

469
00:26:09,436 --> 0:26:12,773
这使得用Vision驱动这些

470
00:26:12,840 --> 0:26:14,541
来处理图像请求变得更加容易

471
00:26:17,411 --> 0:26:21,682
以上就是图像分类的内容

472
00:26:24,518 --> 0:26:27,120
一点警告

473
00:26:27,187 --> 0:26:28,455
可能会在舞台上受伤

474
00:26:28,522 --> 0:26:30,424
所以对于那些娇气的人

475
00:26:34,528 --> 0:26:38,699
我们用于对象识别的模型

476
00:26:38,765 --> 0:26:42,703
基于这种YOLO技术

477
00:26:43,036 --> 0:26:45,072
这是一个运行非常快速的模型

478
00:26:45,138 --> 0:26:50,310
它允许我们获取对象的边界框

479
00:26:50,844 --> 0:26:52,513
并且它可以在屏幕上找到多个对象

480
00:26:52,579 --> 0:26:54,014
如这幅截图所示

481
00:26:57,451 --> 0:27:01,054
它的优势在于我可以得到

482
00:26:57,451 --> 0:27:01,054
它的优势在于我可以得到

483
00:27:01,121 --> 0:27:02,990
但我不会得到整体图像分类器

484
00:27:03,056 --> 0:27:06,126
能够得到的那么多的分类

485
00:27:07,361 --> 0:27:10,664
这与训练也有一定关系

486
00:27:10,731 --> 0:27:14,301
对于这个话题 我推荐你

487
00:27:14,368 --> 0:27:15,469
它是在昨天举办的

488
00:27:15,903 --> 0:27:18,505
其向你展示了如何训练这类模型

489
00:27:18,672 --> 0:27:19,973
这些模型也有点大

490
00:27:21,708 --> 0:27:24,645
那么这看起来效果如何

491
00:27:30,584 --> 0:27:32,152
机器人商店打烊了

492
00:27:35,422 --> 0:27:36,490
现在是早餐时间

493
00:27:42,162 --> 0:27:43,030
好的

494
00:27:43,864 --> 0:27:45,599
打开QuickTime播放器

495
00:27:45,666 --> 0:27:49,469
这是我的新app

496
00:27:52,239 --> 0:27:54,641
我们看到了什么

497
00:27:55,042 --> 0:27:58,612
我们有一个百吉饼

498
00:28:00,180 --> 0:28:02,416
现在它们都显示在一个屏幕中

499
00:28:02,482 --> 0:28:03,750
我来识别它们

500
00:28:04,785 --> 0:28:07,087
正如我所提到的

501
00:28:07,154 --> 0:28:08,922
它们通常会告诉你如何操作

502
00:28:09,122 --> 0:28:11,391
但却将预烘烤的东西从烤箱中拿出来

503
00:28:12,426 --> 0:28:18,031
而这个模型实际上在这个羊角面包

504
00:28:18,098 --> 0:28:19,233
我可以证明这一点

505
00:28:21,668 --> 0:28:25,239
它很新鲜

506
00:28:34,281 --> 0:28:35,115
好的

507
00:28:38,385 --> 0:28:40,521
这很新鲜 但我还是要好好咀嚼

508
00:28:44,525 --> 0:28:47,160
让我们快速看一下代码的样子

509
00:28:53,133 --> 0:28:54,768
那么我做了哪些不同的事情呢？

510
00:28:56,069 --> 0:28:59,039
在设置我的请求方面

511
00:28:59,673 --> 0:29:03,277
我所要做的就是

512
00:28:59,673 --> 0:29:03,277
我所要做的就是

513
00:29:03,343 --> 0:29:09,416
就像我在上一个示例中所做的那样

514
00:29:09,483 --> 0:29:12,152
然后这段代码就是简单的

515
00:29:17,357 --> 0:29:21,128
这里有一些新的东西

516
00:29:21,795 --> 0:29:26,767
当我们看这些代码时

517
00:29:26,834 --> 0:29:30,971
即VNRecognizedObjectObservation

518
00:29:31,538 --> 0:29:36,877
此外 这是我的边界框

519
00:29:37,945 --> 0:29:39,947
我还想向你展示一件事

520
00:29:40,781 --> 0:29:43,584
让我们从这里运行我们的app

521
00:29:51,291 --> 0:29:52,192
好的

522
00:29:53,927 --> 0:29:55,662
好的 我们现在正处于断点

523
00:29:56,129 --> 0:29:58,098
我只查看第一个标签

524
00:29:58,165 --> 0:30:01,468
当我们实际处理这些结果时

525
00:29:58,165 --> 0:30:01,468
当我们实际处理这些结果时

526
00:30:04,471 --> 0:30:05,606
我们看看这个

527
00:30:11,612 --> 0:30:14,648
查看一下

528
00:30:16,416 --> 0:30:17,351
labels属性

529
00:30:19,520 --> 0:30:22,990
你可以看到我实际上

530
00:30:23,290 --> 0:30:26,960
这里有百吉饼 香蕉 咖啡

531
00:30:27,027 --> 0:30:30,797
不好意思

532
00:30:30,864 --> 0:30:34,902
它们按照置信度进行排序

533
00:30:34,968 --> 0:30:37,070
它通常是你所感兴趣的

534
00:30:37,137 --> 0:30:40,107
这就是为什么我在这里

535
00:30:40,240 --> 0:30:43,043
但你总是得到所有的分类

536
00:30:43,110 --> 0:30:46,914
它们保存在一个数组中

537
00:30:48,515 --> 0:30:51,418
这就是我们的早餐查找器

538
00:30:51,485 --> 0:30:53,854
让我们回到幻灯片

539
00:30:54,021 --> 0:30:54,855
好

540
00:30:58,458 --> 0:31:00,360
我们通过使用一个新的API

541
00:30:58,458 --> 0:31:00,360
我们通过使用一个新的API

542
00:31:00,661 --> 0:31:05,165
即我们的VNRecognized

543
00:31:07,234 --> 0:31:11,471
当我们执行

544
00:31:11,538 --> 0:31:17,144
且若该模型实际上基于对象检测器时

545
00:31:20,848 --> 0:31:23,851
如该示例一样

546
00:31:23,917 --> 0:31:27,154
现在你可能会说

547
00:31:27,221 --> 0:31:29,022
我在网上看到了很多相关文章”

548
00:31:29,756 --> 0:31:32,626
但是看看他们实际需要编写多少代码

549
00:31:32,693 --> 0:31:34,561
才能获取此模型的输出

550
00:31:34,628 --> 0:31:36,530
并将它放入你可以使用的东西中

551
00:31:36,663 --> 0:31:38,899
而我们在这里只用了几行代码

552
00:31:38,966 --> 0:31:41,401
因此它使YOLO模型变得

553
00:31:42,102 --> 0:31:43,837
我们再来看一次这些代码

554
00:31:44,238 --> 0:31:48,208
我创建了我的模型

555
00:31:48,909 --> 0:31:52,913
在我的完成处理程序中

556
00:31:52,980 --> 0:31:55,449
因为我们看到

557
00:31:55,916 --> 0:31:58,385
我从中获取标签和边界框

558
00:31:58,452 --> 0:32:00,387
然后就可以将其显示到早餐查找器中

559
00:31:58,452 --> 0:32:00,387
然后就可以将其显示到早餐查找器中

560
00:32:04,091 --> 0:32:06,927
我想在这个例子中强调一件事

561
00:32:08,328 --> 0:32:10,531
你可以看到这些边界框有点抖动

562
00:32:10,597 --> 0:32:13,634
因为我一帧一帧一帧地运行检测器

563
00:32:14,935 --> 0:32:17,137
跟踪往往是这里更好的选择

564
00:32:17,271 --> 0:32:21,508
为什么呢？即使在运行方面

565
00:32:21,575 --> 0:32:23,310
跟踪也比运行这些模型要快得多

566
00:32:25,179 --> 0:32:29,750
因此重新检测比运行跟踪请求

567
00:32:31,218 --> 0:32:35,689
只要我想在屏幕上跟踪某对象

568
00:32:37,057 --> 0:32:39,393
因为它是一种更轻量级的算法

569
00:32:39,826 --> 0:32:43,030
最重要的是 我们有时间平滑

570
00:32:43,096 --> 0:32:45,065
这样这些框就不再抖动了

571
00:32:45,132 --> 0:32:47,534
如果你查看我们的一些跟踪示例

572
00:32:47,601 --> 0:32:50,237
它们实际上在屏幕上移动得非常顺畅

573
00:32:50,871 --> 0:32:52,606
如果你想了解有关跟踪的更多信息

574
00:32:54,074 --> 0:32:56,643
我的同事Sergei的上一次演讲

575
00:32:56,877 --> 0:32:59,813
讨论了如何实现这一点

576
00:33:01,982 --> 0:33:04,251
好的 最后但同样重要的

577
00:33:04,318 --> 0:33:07,721
让我们加强对Vision的掌握

578
00:33:08,889 --> 0:33:11,959
在使用Vision框架时

579
00:33:12,893 --> 0:33:15,963
首先是很多问题的常见根源

580
00:33:16,997 --> 0:33:18,165
图像方向

581
00:33:19,933 --> 0:33:23,704
并非所有的Vision算法

582
00:33:24,004 --> 0:33:26,139
你可能早些时候听说过

583
00:33:26,206 --> 0:33:28,575
与方向无关的人脸检测器

584
00:33:29,042 --> 0:33:30,777
但上一个则不是

585
00:33:32,079 --> 0:33:35,682
这意味着我们需要知道

586
00:33:36,316 --> 0:33:37,985
这可能有些让人迷惑

587
00:33:38,051 --> 0:33:40,454
因为如果你看一下它并预览最终结果

588
00:33:40,687 --> 0:33:44,057
我的图片看起来是直立的

589
00:33:44,925 --> 0:33:48,228
有样东西可以告诉我们

590
00:33:48,295 --> 0:33:49,963
这被称为EXIF方向

591
00:33:51,198 --> 0:33:55,402
如果捕获了一张图像

592
00:33:55,469 --> 0:33:57,771
通过EXIF 我们可以知道

593
00:33:58,138 --> 0:34:02,276
如果你将URL作为输入

594
00:33:58,138 --> 0:34:02,276
如果你将URL作为输入

595
00:34:02,643 --> 0:34:05,179
Vision实际上为你做了所有事

596
00:34:05,245 --> 0:34:07,848
并从文件中读取此EXIF信息

597
00:34:09,149 --> 0:34:11,685
但正如我们之前在演示中展示的那样

598
00:34:11,752 --> 0:34:13,620
如果我使用实时捕获源

599
00:34:13,687 --> 0:34:15,755
我将需要传递此信息

600
00:34:16,255 --> 0:34:17,491
因此我必须查看

601
00:34:17,558 --> 0:34:21,161
我的UI设备的当前方向是什么

602
00:34:21,562 --> 0:34:24,130
并将其转换为

603
00:34:24,197 --> 0:34:26,900
因为我们需要它的

604
00:34:29,402 --> 0:34:32,072
接下来我们谈谈坐标系

605
00:34:33,440 --> 0:34:36,443
对于Vision

606
00:34:37,744 --> 0:34:43,016
并且所有处理都会假设

607
00:34:43,382 --> 0:34:45,185
因此方向很重要

608
00:34:47,254 --> 0:34:50,489
我们所有的处理都是在

609
00:34:50,891 --> 0:34:54,360
除了配准 我们需要知道它

610
00:34:54,828 --> 0:34:58,665
标准化坐标意味着我们的坐标是

611
00:34:58,732 --> 0:35:00,767
到右上角的(1,1)

612
00:34:58,732 --> 0:35:00,767
到右上角的(1,1)

613
00:35:02,336 --> 0:35:03,270
这是一个

614
00:35:03,337 --> 0:35:07,040
执行了面部和特征点检测请求的图片

615
00:35:07,474 --> 0:35:10,210
你可以看到我得到了脸上的边界框

616
00:35:10,277 --> 0:35:12,379
并且特征点以

617
00:35:12,446 --> 0:35:15,549
该边界框的相对坐标进行表示

618
00:35:17,251 --> 0:35:19,620
如果你需要回到图像坐标空间

619
00:35:19,686 --> 0:35:24,791
可以使用在VNUtils中的工具函数

620
00:35:24,858 --> 0:35:27,661
来在这些坐标之间转换

621
00:35:31,331 --> 0:35:33,000
接下来我们谈谈置信度分数

622
00:35:33,066 --> 0:35:36,703
在我们的机器人商店示例中

623
00:35:38,739 --> 0:35:42,876
我们的许多算法都可以表示

624
00:35:43,810 --> 0:35:48,982
这在稍后分析我想要从结果中

625
00:35:49,049 --> 0:35:50,450
是一个很重要的部分

626
00:35:50,517 --> 0:35:54,354
我是得到一个为0的低置信度呢

627
00:35:58,859 --> 0:35:59,693
遥控器

628
00:36:02,296 --> 0:36:03,197
好了

629
00:36:03,463 --> 0:36:07,935
不幸的是 并非所有算法

630
00:36:08,001 --> 0:36:11,071
来衡量它们的置信度分数

631
00:36:11,605 --> 0:36:14,274
例如 如果看看我们的文本检测器

632
00:36:14,341 --> 0:36:17,878
返回一个值为1的置信度得分

633
00:36:17,945 --> 0:36:20,347
它一开始就不会返回边界框

634
00:36:21,215 --> 0:36:25,219
但正如我们所看到的

635
00:36:25,285 --> 0:36:28,856
则是一个较大的范围

636
00:36:30,624 --> 0:36:31,892
在我的第一个例子中

637
00:36:32,793 --> 0:36:35,395
我使用了一张

638
00:36:36,096 --> 0:36:37,698
我在它上面运行了自己的模型

639
00:36:38,332 --> 0:36:40,767
果然 它有很高的置信度分数

640
00:36:40,834 --> 0:36:42,169
并认为这是一个步进电机

641
00:36:43,670 --> 0:36:45,172
在下面这个例子中

642
00:36:45,839 --> 0:36:48,876
我将使用我们模型库中的一些模型

643
00:36:49,743 --> 0:36:53,080
不要误会我的意思

644
00:36:53,146 --> 0:36:55,849
而是关于它们返回的置信度

645
00:36:55,916 --> 0:36:57,584
以及我们实际想要做的事情

646
00:36:58,552 --> 0:37:01,955
当我们想要对这张图片进行分类时

647
00:36:58,552 --> 0:37:01,955
当我们想要对这张图片进行分类时

648
00:37:03,390 --> 0:37:07,794
嗯 它并没有那么糟糕

649
00:37:08,095 --> 0:37:12,566
只有0.395的置信度得分

650
00:37:12,633 --> 0:37:14,968
但其中的确有沙滩

651
00:37:15,502 --> 0:37:19,673
所以当我想搜索它时

652
00:37:19,740 --> 0:37:22,809
但我可以用它来标记图像吗

653
00:37:24,211 --> 0:37:25,512
让我们看看下一个例子

654
00:37:26,980 --> 0:37:30,117
摩托车上的女孩

655
00:37:31,752 --> 0:37:34,354
嗯 我不确定她是否愿意被称为红薯

656
00:37:39,426 --> 0:37:43,096
让我们再看一个例子

657
00:37:44,131 --> 0:37:45,799
分类器对此做了什么

658
00:37:47,334 --> 0:37:49,937
它认为这是一个网站

659
00:37:53,507 --> 0:37:56,343
这是关于我们的置信度得分的

660
00:37:58,245 --> 0:38:02,416
1.0总是意味着100%正确吗？

661
00:37:58,245 --> 0:38:02,416
1.0总是意味着100%正确吗？

662
00:38:02,816 --> 0:38:05,018
它符合算法的标准

663
00:38:05,252 --> 0:38:09,756
但我们的感知 正如我们看到的红薯

664
00:38:11,425 --> 0:38:14,962
所以当你创建一个

665
00:38:15,028 --> 0:38:18,765
请记住这一点 好好想一想

666
00:38:18,832 --> 0:38:23,003
并说“哦 你有癌症”

667
00:38:23,070 --> 0:38:25,272
你可能想要软化一点点

668
00:38:25,339 --> 0:38:28,041
取决于你对结果的确切程度

669
00:38:30,577 --> 0:38:32,946
为此你可以使用两种技术

670
00:38:33,413 --> 0:38:37,417
正如我们在机器人商店示例中所见

671
00:38:37,484 --> 0:38:40,621
因为我已经标记了图像

672
00:38:40,687 --> 0:38:42,956
并且过滤掉置信度分数较低的

673
00:38:43,590 --> 0:38:45,926
另一方面

674
00:38:45,993 --> 0:38:51,198
我可能会使用我拥有的一些图像

675
00:38:51,632 --> 0:38:54,368
搜索结果下面

676
00:38:54,434 --> 0:38:56,069
仍然可能有一个正确的选择

677
00:39:01,108 --> 0:39:04,611
像往常一样 你可以在我们的网站上

678
00:39:04,878 --> 0:39:07,514
并且我们明天下午3点有个实验室

679
00:39:07,581 --> 0:39:10,651
欢迎到访 咨询你的问题

680
00:39:11,552 --> 0:39:14,388
接着我首先要感谢你们所有人

681
00:39:14,454 --> 0:39:16,990
能够使用我们的技术创造出

682
00:39:17,057 --> 0:39:19,526
我期待看到你能用这些技术

683
00:39:19,927 --> 0:39:23,363
最后谢谢大家来到WWDC
