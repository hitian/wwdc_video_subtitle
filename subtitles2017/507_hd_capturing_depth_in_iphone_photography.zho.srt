1
00:00:17,551 --> 00:00:20,420
（关于iPhone摄影中的

2
00:00:23,924 --> 00:00:26,627
欢迎来到演讲507

3
00:00:26,693 --> 00:00:30,264
我来自相机软件团队

4
00:00:30,330 --> 00:00:32,466
能在今天下午和你们

5
00:00:33,834 --> 00:00:36,570
看到我要做什么了吗？好了

6
00:00:38,572 --> 00:00:39,406
本次演讲

7
00:00:39,473 --> 00:00:41,341
是两个演讲的第一部分

8
00:00:41,408 --> 00:00:43,477
这两个演讲会介绍Apple

9
00:00:43,544 --> 00:00:46,380
那就是包含深度信息的多媒体内容

10
00:00:47,014 --> 00:00:49,183
我会从概念层面上介绍一下景深

11
00:00:49,249 --> 00:00:51,485
我会带着你们熟悉一些关键的术语

12
00:00:51,852 --> 00:00:55,722
我会教你们如何在iPhone上

13
00:00:55,789 --> 00:00:58,192
你会在本次演讲中看到

14
00:00:59,593 --> 00:01:00,594
我们的计划是这样的

15
00:00:59,593 --> 00:01:00,594
我们的计划是这样的

16
00:01:01,929 --> 00:01:03,430
首先我们会介绍深度和视差

17
00:01:03,497 --> 00:01:06,233
它们出现在iPhone 7 Plus上

18
00:01:07,100 --> 00:01:09,937
接着我们会介绍下如何从

19
00:01:11,004 --> 00:01:12,673
拍摄带有深度数据的照片

20
00:01:13,507 --> 00:01:16,143
最后我们会介绍一个有点离题的主题

21
00:01:16,210 --> 00:01:18,779
也就是双照片拍摄

22
00:01:18,846 --> 00:01:21,181
关于双摄像头的功能

23
00:01:21,381 --> 00:01:22,883
我很激动能介绍一下它

24
00:01:23,450 --> 00:01:28,355
你要做的就是听一下所有这些

25
00:01:28,422 --> 00:01:30,524
我会不断在本次演讲中

26
00:01:30,958 --> 00:01:32,793
让我们一起做个游戏吧

27
00:01:33,126 --> 00:01:35,929
每当你听到一个关于深度的双关语时

28
00:01:36,163 --> 00:01:37,664
这样我就知道你们在专心听了

29
00:01:37,965 --> 00:01:41,401
让我们试一下

30
00:01:43,237 --> 00:01:45,138
谢谢

31
00:01:46,740 --> 00:01:47,808
好了

32
00:01:48,976 --> 00:01:51,778
你们今天之所以来这里都是因为它

33
00:01:51,945 --> 00:01:53,614
这就是iPhone 7 Plus

34
00:01:53,981 --> 00:01:56,216
这个产品卖得

35
00:01:56,650 --> 00:01:58,719
甚至比上代plus更好

36
00:01:59,286 --> 00:02:02,789
这在很大程度上要归功于

37
00:01:59,286 --> 00:02:02,789
这在很大程度上要归功于

38
00:02:04,291 --> 00:02:06,660
它是一个双主镜头系统

39
00:02:06,727 --> 00:02:10,163
由一个28毫米的广角摄像头

40
00:02:10,497 --> 00:02:13,300
和一个56毫米的

41
00:02:13,800 --> 00:02:15,669
它们都是1200万像素的

42
00:02:16,003 --> 00:02:18,605
它们共享同样的功能项

43
00:02:19,039 --> 00:02:21,241
你可以独立运行其中的一个摄像头

44
00:02:21,675 --> 00:02:26,079
或是利用一个虚拟的第三个摄像头

45
00:02:26,146 --> 00:02:28,182
这是我们首次将这项技术

46
00:02:28,248 --> 00:02:30,017
我们把它叫作双摄像头

47
00:02:30,551 --> 00:02:33,954
它以同步的方式

48
00:02:34,388 --> 00:02:37,291
并且通过共同运行双摄像头

49
00:02:38,292 --> 00:02:40,160
第一个功能是双摄像头变焦

50
00:02:41,128 --> 00:02:43,797
它会在广角和长焦之间进行自动切换

51
00:02:43,864 --> 00:02:45,098
当你在变焦的时候

52
00:02:45,632 --> 00:02:49,403
它会适配曝光 对焦 还有帧率

53
00:02:49,469 --> 00:02:51,605
你甚至意识不到我们切换了摄像头

54
00:02:51,872 --> 00:02:53,640
但所有这些事件都无缝地发生了

55
00:02:54,007 --> 00:02:56,176
我们还会视差偏移进行了补偿

56
00:02:56,243 --> 00:02:59,313
使其能够平滑地过渡

57
00:02:59,379 --> 00:03:00,714
广角和长焦之间进行切换时

58
00:02:59,379 --> 00:03:00,714
广角和长焦之间进行切换时

59
00:03:01,915 --> 00:03:04,785
第二个选框功能

60
00:03:05,152 --> 00:03:10,123
也就是双摄像头系统锁定在

61
00:03:11,258 --> 00:03:14,494
但会同时使用广角和长焦的图像

62
00:03:14,862 --> 00:03:17,664
来生成一幅漂亮的浅景深效果的图像

63
00:03:17,898 --> 00:03:20,100
通常你要用一个贵得多的照相机

64
00:03:20,167 --> 00:03:22,169
带有快速 广角的镜头

65
00:03:23,403 --> 00:03:25,305
前景在聚焦时看起来很锐利

66
00:03:25,372 --> 00:03:27,774
而背景会逐渐变得模糊

67
00:03:27,941 --> 00:03:30,043
像是这些好看的小花束圈

68
00:03:31,578 --> 00:03:34,781
景深效果甚至在iOS 11中

69
00:03:35,148 --> 00:03:38,418
我们还针对渲染失焦的区域进行了改进

70
00:03:38,886 --> 00:03:42,489
这样可以更准确地表现一个

71
00:03:42,789 --> 00:03:45,425
还带有锐利清晰可辨的花束圈

72
00:03:45,959 --> 00:03:48,662
我们还改进了对边缘进行渲染的方法

73
00:03:48,729 --> 00:03:52,266
也就是前景和背景之间的边缘

74
00:03:52,332 --> 00:03:55,169
我估计你会很惊讶地发现

75
00:03:55,402 --> 00:03:57,905
浅景深效果

76
00:03:59,473 --> 00:04:00,307
要生成

77
00:03:59,473 --> 00:04:00,307
要生成

78
00:04:00,374 --> 00:04:04,077
这样的效果你需要区分

79
00:04:04,144 --> 00:04:06,046
换句话说 你需要用到深度

80
00:04:06,547 --> 00:04:09,483
截至目前 深度信息

81
00:04:09,550 --> 00:04:11,618
Apple相机应用的人像模式中

82
00:04:13,086 --> 00:04:17,391
不过现在iOS 11中

83
00:04:18,625 --> 00:04:20,994
这里是一个灰度可视化的深度图

84
00:04:21,060 --> 00:04:22,930
它被嵌入到这个图片文件里

85
00:04:24,698 --> 00:04:28,769
有了深度信息就拥有了

86
00:04:29,036 --> 00:04:32,739
像是将不同的滤镜应用到背景和前景中

87
00:04:32,806 --> 00:04:33,807
就像是这样

88
00:04:35,442 --> 00:04:39,112
这里我将一个黑白滤镜应用到背景

89
00:04:39,179 --> 00:04:40,981
还将一个淡入淡出滤镜应用到前景

90
00:04:41,448 --> 00:04:43,550
你就能发现 这小女孩

91
00:04:43,617 --> 00:04:45,586
但是裤子后面的物体都是黑白的了

92
00:04:46,119 --> 00:04:48,488
了解了深度的渐变 我就能

93
00:04:48,555 --> 00:04:51,491
实现更加复杂的功能 将转换点

94
00:04:51,558 --> 00:04:53,427
像这样前后移动

95
00:04:53,594 --> 00:04:54,995
这样就能让你的注意力集中在花上面

96
00:04:56,663 --> 00:04:59,233
注意下

97
00:04:59,299 --> 00:05:00,934
而其他的物体都是黑白的

98
00:04:59,299 --> 00:05:00,934
而其他的物体都是黑白的

99
00:05:02,436 --> 00:05:05,639
你甚至可以对前景和背景的曝光

100
00:05:05,706 --> 00:05:06,707
像是这样

101
00:05:08,308 --> 00:05:11,478
她现在看起来就像是被

102
00:05:12,179 --> 00:05:14,748
我不是说你非得这么做

103
00:05:16,583 --> 00:05:18,519
好了 闲话少叙

104
00:05:18,785 --> 00:05:20,454
我想把这部分叫作

105
00:05:20,821 --> 00:05:22,256
深度学习

106
00:05:22,322 --> 00:05:23,490
（深度学习）

107
00:05:23,557 --> 00:05:24,591
谢谢你们

108
00:05:24,658 --> 00:05:27,027
首先我们需要定义一下什么是深度图

109
00:05:27,594 --> 00:05:32,666
在现实世界中 深度指的是

110
00:05:33,300 --> 00:05:36,637
而深度图就是将一个三维场景转换

111
00:05:36,803 --> 00:05:39,039
成二维场景的表现形式

112
00:05:39,373 --> 00:05:42,376
你可以通过将深度设定为一个

113
00:05:43,010 --> 00:05:44,111
我来解释下我说的是什么意思

114
00:05:44,678 --> 00:05:46,980
我会用到一个针孔相机的图

115
00:05:47,214 --> 00:05:48,782
它被用来说明此类问题

116
00:05:49,383 --> 00:05:50,817
如果你研究过计算机视觉

117
00:05:50,884 --> 00:05:52,920
你就会对针孔相机非常熟悉

118
00:05:53,120 --> 00:05:56,857
针孔照相机其实就是

119
00:05:56,924 --> 00:05:58,792
它只有一个小的孔洞

120
00:05:59,126 --> 00:06:02,663
一个能让光进入的小孔

121
00:05:59,126 --> 00:06:02,663
一个能让光进入的小孔

122
00:06:03,096 --> 00:06:05,399
并将自己以一个倒转的图像投射到

123
00:06:05,632 --> 00:06:09,236
像平面的另一边或者是传感器上

124
00:06:09,303 --> 00:06:10,737
（深度图）

125
00:06:11,171 --> 00:06:13,907
另一边也叫作像平面或是传感器

126
00:06:14,942 --> 00:06:16,677
而这个光线所通过的孔

127
00:06:16,743 --> 00:06:18,345
被称为焦点

128
00:06:18,879 --> 00:06:20,848
而所拍摄图像的视野

129
00:06:20,914 --> 00:06:23,483
取决于焦距

130
00:06:23,851 --> 00:06:27,654
焦距就是从焦点到像平面的距离

131
00:06:28,388 --> 00:06:31,391
较短的焦距就意味着更宽的视野

132
00:06:32,092 --> 00:06:35,629
而较长的焦距 还有较长的盒子

133
00:06:36,396 --> 00:06:39,132
焦距就是一个恒定距离

134
00:06:39,199 --> 00:06:42,769
它将现实世界的距离扁平化

135
00:06:43,437 --> 00:06:47,207
简单来说 深度图就是

136
00:06:47,374 --> 00:06:49,843
2D的单通道图像

137
00:06:50,043 --> 00:06:52,045
其中每个像素值都有不同的深度

138
00:06:52,112 --> 00:06:54,815
像是5米 4米 3米

139
00:06:56,517 --> 00:07:00,821
为了测量深度

140
00:06:56,517 --> 00:07:00,821
为了测量深度

141
00:07:00,888 --> 00:07:02,723
就像是一个渡越时间摄像头

142
00:07:02,923 --> 00:07:03,757
例如

143
00:07:03,824 --> 00:07:07,427
有一个从物体反射光信号系统

144
00:07:07,494 --> 00:07:10,664
接着它能测量其返回传感器的时间

145
00:07:11,565 --> 00:07:15,335
iPhone 7的双摄像头不是

146
00:07:16,103 --> 00:07:18,906
它是一个基于视差的系统

147
00:07:19,339 --> 00:07:20,607
视差是一种用来测量

148
00:07:20,674 --> 00:07:23,844
物体偏移量的量度

149
00:07:23,911 --> 00:07:26,580
当你从两个不同镜头观察物体时

150
00:07:27,080 --> 00:07:28,982
Disparity就是视差的别称

151
00:07:29,883 --> 00:07:32,753
你只要稳定住头

152
00:07:33,187 --> 00:07:36,356
将你的目光固定在近处的某个物体上

153
00:07:36,690 --> 00:07:40,093
然后就不要再移动头了

154
00:07:40,160 --> 00:07:43,230
比方说 就是

155
00:07:44,164 --> 00:07:46,300
左眼 右眼

156
00:07:46,900 --> 00:07:49,837
这时候你就可以发现

157
00:07:49,903 --> 00:07:52,139
比后面的记号笔要多

158
00:07:52,206 --> 00:07:54,208
这就是视差效果

159
00:07:55,776 --> 00:07:57,344
现在接着回来介绍针孔相机模型

160
00:07:57,945 --> 00:07:59,713
我拍摄一幅鸟瞰图

161
00:07:59,880 --> 00:08:03,817
是由两个立体纠正相机所拍摄的

162
00:07:59,880 --> 00:08:03,817
是由两个立体纠正相机所拍摄的

163
00:08:04,251 --> 00:08:07,254
也就是说 第一

164
00:08:07,321 --> 00:08:08,822
它们指向同一个方向

165
00:08:09,456 --> 00:08:13,193
第二 它们有相同的焦距

166
00:08:13,894 --> 00:08:16,997
那就是从焦点到像平面或是

167
00:08:17,598 --> 00:08:21,869
每个摄像头都有一个测量好的

168
00:08:22,603 --> 00:08:26,773
如果你画一条从针孔到像平面的垂直线

169
00:08:27,474 --> 00:08:30,410
那么光学中心就是它

170
00:08:30,477 --> 00:08:31,478
与像平面相交的点

171
00:08:31,945 --> 00:08:33,780
另外还有一个你应该熟悉的术语

172
00:08:33,847 --> 00:08:35,115
那就是基线

173
00:08:35,682 --> 00:08:37,351
基线指的是

174
00:08:37,417 --> 00:08:40,020
两个镜头的光学中心之间的距离

175
00:08:40,087 --> 00:08:41,655
是在立体纠正系统里的两个镜头

176
00:08:42,322 --> 00:08:43,323
它是这么工作的

177
00:08:43,690 --> 00:08:48,829
来自一个被观察对象的光线

178
00:08:49,596 --> 00:08:51,865
或是通过孔径

179
00:08:52,299 --> 00:08:56,170
并且落在两个摄像头

180
00:08:57,371 --> 00:09:00,340
我想要给你们讲的第四个术语是Z

181
00:08:57,371 --> 00:09:00,340
我想要给你们讲的第四个术语是Z

182
00:09:00,407 --> 00:09:04,278
Z是用于深度

183
00:09:06,213 --> 00:09:09,016
现在看看像平面上的点会怎么样吧

184
00:09:09,216 --> 00:09:11,785
随着被观察的点距离变得更远

185
00:09:14,121 --> 00:09:15,322
像平面上的点离得更近了

186
00:09:15,389 --> 00:09:16,857
我会给你们再展示一次

187
00:09:17,858 --> 00:09:20,093
随着现实点离得更远

188
00:09:20,994 --> 00:09:23,096
它们在像平面上离得更近了

189
00:09:23,864 --> 00:09:26,466
而随着对象离得更近

190
00:09:26,667 --> 00:09:28,502
像平面上的点会互相离得更远

191
00:09:29,036 --> 00:09:33,507
所以当使用立体纠正摄像头的时候

192
00:09:34,007 --> 00:09:36,877
它们要么互相离得更近 要么离得更远

193
00:09:36,944 --> 00:09:40,180
但它们都位于一条线

194
00:09:41,782 --> 00:09:44,318
在了解了基线后你就

195
00:09:44,384 --> 00:09:46,320
像是这样沿着它们的光学中心

196
00:09:46,687 --> 00:09:49,623
接着减去像平面上被观察点之间的距离

197
00:09:49,890 --> 00:09:52,793
就可以获得视差

198
00:09:53,227 --> 00:09:54,361
这就是视差

199
00:09:55,028 --> 00:09:57,130
你可以将这个距离以任何单位表示

200
00:09:57,197 --> 00:09:58,832
只要对你的处理过程有帮助就行

201
00:09:58,899 --> 00:10:01,502
它可以是像素 米 微米

202
00:09:58,899 --> 00:10:01,502
它可以是像素 米 微米

203
00:10:02,069 --> 00:10:05,672
通常我们会以像素为单位保存它

204
00:10:05,739 --> 00:10:07,441
因为RGB图像是以像素表示的

205
00:10:08,242 --> 00:10:09,943
我们就可以正常保存像素偏移了

206
00:10:10,010 --> 00:10:12,980
只要它们对应的图像没有改变

207
00:10:13,046 --> 00:10:14,047
大小就可以

208
00:10:14,982 --> 00:10:17,618
你最好不要编辑这个图像

209
00:10:17,784 --> 00:10:19,753
因为如果你缩小了这个图像

210
00:10:20,020 --> 00:10:22,422
你就会同时改变像素的大小

211
00:10:22,689 --> 00:10:24,191
你就得检查整个图

212
00:10:24,258 --> 00:10:27,227
然后缩放深度图中的每个值

213
00:10:27,561 --> 00:10:29,296
这是一个非常不友好的实现方式

214
00:10:30,597 --> 00:10:32,032
我们Apple

215
00:10:32,099 --> 00:10:36,036
选择使用标准化的值来表达视差

216
00:10:36,103 --> 00:10:39,339
这些值对于缩放操作是有弹性的

217
00:10:40,174 --> 00:10:42,009
再回到我们的被观察点

218
00:10:42,442 --> 00:10:45,846
你就会发现两个相似三角形

219
00:10:46,113 --> 00:10:47,214
我来给你们着重标出

220
00:10:47,281 --> 00:10:49,583
（消除视差）

221
00:10:49,650 --> 00:10:53,153
这些三角形具有

222
00:10:54,221 --> 00:10:56,423
如果我抹掉相机

223
00:10:56,857 --> 00:11:00,694
现实世界三角形的边就是Z

224
00:10:56,857 --> 00:11:00,694
现实世界三角形的边就是Z

225
00:11:00,894 --> 00:11:03,697
以及基线 也就是两个

226
00:11:04,932 --> 00:11:07,501
在这个光盒

227
00:11:08,769 --> 00:11:12,706
同一个三角形会被表示为

228
00:11:12,773 --> 00:11:14,975
和以像素为单位的视差

229
00:11:15,909 --> 00:11:18,011
你们感觉要开始讲数学了吗？

230
00:11:18,612 --> 00:11:20,614
请听我接着说

231
00:11:21,415 --> 00:11:22,249
基线

232
00:11:22,716 --> 00:11:25,719
比上以像素为单位的Z

233
00:11:28,722 --> 00:11:29,590
就等于

234
00:11:30,524 --> 00:11:33,060
视差比上焦距的长度

235
00:11:33,560 --> 00:11:37,497
如果我们将两边都除以基线的话

236
00:11:38,198 --> 00:11:44,338
左边的b就会被消去

237
00:11:45,239 --> 00:11:48,976
这很不错

238
00:11:49,643 --> 00:11:51,445
视差差不多就是这个意思

239
00:11:51,879 --> 00:11:55,148
当某个物体向远处移动的时候

240
00:11:55,749 --> 00:11:59,686
而当它向近处移动时 视差就会增长

241
00:12:01,355 --> 00:12:04,958
而我们把右边所剩下的部分

242
00:12:05,025 --> 00:12:11,064
它不再是像素偏移了

243
00:12:11,498 --> 00:12:12,833
基线就被内置了

244
00:12:12,900 --> 00:12:15,202
所以你不需要再单独携带此信息了

245
00:12:15,269 --> 00:12:16,570
当你

246
00:12:16,637 --> 00:12:17,838
处理深度图的时候

247
00:12:18,205 --> 00:12:21,408
视差的单位是1/米

248
00:12:21,742 --> 00:12:24,077
它可以处理缩放操作

249
00:12:24,411 --> 00:12:28,315
如你所见

250
00:12:28,382 --> 00:12:30,184
因为它只用1除以什么

251
00:12:31,985 --> 00:12:34,688
有人觉得超出接受范围了么？

252
00:12:35,856 --> 00:12:36,823
（视差对比深度）

253
00:12:36,890 --> 00:12:40,327
它听起来挺麻烦的

254
00:12:41,929 --> 00:12:45,265
我们有一个基于视差的系统

255
00:12:45,899 --> 00:12:48,268
不过视差是个不错的深度代理

256
00:12:48,836 --> 00:12:52,472
而且规范化的视差就是深度的倒数

257
00:12:54,274 --> 00:12:55,242
（沉思）

258
00:12:55,309 --> 00:12:58,812
提到规范化视差

259
00:12:59,012 --> 00:13:00,080
这里我们好好想一下

260
00:12:59,012 --> 00:13:00,080
这里我们好好想一下

261
00:13:03,250 --> 00:13:05,385
这幅图片有一个视差图

262
00:13:06,920 --> 00:13:10,490
我觉得这就是一个反深度的跳跃

263
00:13:13,026 --> 00:13:14,795
谢谢你们

264
00:13:15,062 --> 00:13:19,266
我们在深度API集中使用了

265
00:13:19,700 --> 00:13:22,236
这是个用来表示任何

266
00:13:23,170 --> 00:13:26,807
它可以指代一个真正的深度图

267
00:13:26,874 --> 00:13:29,243
它们都是与深度相关的

268
00:13:29,309 --> 00:13:31,044
因此它们都是深度数据

269
00:13:32,446 --> 00:13:35,315
而且我们专门给它创建了一个对象

270
00:13:35,382 --> 00:13:37,818
根据我们平台上对于深度的定义

271
00:13:37,885 --> 00:13:40,053
被称为AVDepthData

272
00:13:40,454 --> 00:13:43,490
它能被用于iOS macOS

273
00:13:43,957 --> 00:13:45,993
这是AVFoundation框架上的一个类

274
00:13:46,326 --> 00:13:49,062
它可以表示深度图

275
00:13:49,463 --> 00:13:53,734
它还提供一些不错的功能来转化

276
00:13:54,668 --> 00:13:57,404
让我们来看看

277
00:13:58,071 --> 00:14:00,941
深度图就是一些图像

278
00:13:58,071 --> 00:14:00,941
深度图就是一些图像

279
00:14:01,008 --> 00:14:04,111
它们有点像RGB图像

280
00:14:04,578 --> 00:14:07,080
不过它们仍然能被表示成

281
00:14:07,614 --> 00:14:10,484
而现在CoreVideo定义了

282
00:14:11,051 --> 00:14:13,487
给那些我们在前面的幻灯片

283
00:14:13,554 --> 00:14:14,955
它们都是浮点型的

284
00:14:15,656 --> 00:14:20,794
头两种格式是用于规范化视差的

285
00:14:20,861 --> 00:14:24,198
注意下有16位

286
00:14:24,898 --> 00:14:28,235
后两种格式是用于深度的

287
00:14:29,136 --> 00:14:31,471
它们也是有16位或者32位可选的

288
00:14:31,538 --> 00:14:32,539
我们为何要这么做？

289
00:14:33,173 --> 00:14:35,676
如果你想要在GPU上对深度进行处理

290
00:14:35,742 --> 00:14:38,312
你就应该请求16位

291
00:14:38,378 --> 00:14:40,380
或者说半浮点的深度值

292
00:14:40,681 --> 00:14:42,149
如果你是在CPU上进行处理

293
00:14:42,416 --> 00:14:46,019
你就应该使用全32位的浮点变量

294
00:14:46,086 --> 00:14:47,087
它们更加合适

295
00:14:48,488 --> 00:14:50,557
我们会在后面介绍下

296
00:14:50,624 --> 00:14:54,328
是从何而来的 不过现在让我们

297
00:14:55,095 --> 00:14:58,799
有了AVDepthData对象

298
00:14:59,166 --> 00:15:01,168
也就是四种像素格式之一

299
00:14:59,166 --> 00:15:01,168
也就是四种像素格式之一

300
00:15:01,869 --> 00:15:04,571
你可以访问

301
00:15:04,638 --> 00:15:07,674
它是个CV像素缓存

302
00:15:07,741 --> 00:15:11,044
行和列来遍历它

303
00:15:11,111 --> 00:15:12,279
（引入AVDEPTHDATA）

304
00:15:12,346 --> 00:15:14,548
而最后两种我想在这里着重说明

305
00:15:14,615 --> 00:15:18,151
它们跟捕捉深度数据的内在问题有关

306
00:15:18,785 --> 00:15:21,121
我们会一个个地说明这些问题

307
00:15:21,188 --> 00:15:22,623
并且讨论下解决办法

308
00:15:24,091 --> 00:15:27,427
第一个问题是洞

309
00:15:28,228 --> 00:15:32,332
为了计算视差 两个相机都需要观察

310
00:15:32,399 --> 00:15:33,734
不过是从两个不同的角度

311
00:15:34,134 --> 00:15:36,803
如果它们观测不到这个点

312
00:15:37,204 --> 00:15:38,906
为什么它们可能看不到这个点呢？

313
00:15:40,073 --> 00:15:43,410
首先可能是堵塞问题

314
00:15:43,477 --> 00:15:45,812
突然挡住了你的其中一个相机

315
00:15:46,513 --> 00:15:48,682
如果它变得有点模糊了

316
00:15:48,749 --> 00:15:53,921
你就不能再看到两个点了

317
00:15:55,022 --> 00:15:58,659
另一个更常见的原因就是

318
00:15:59,493 --> 00:16:02,496
当相机一和相机二

319
00:15:59,493 --> 00:16:02,496
当相机一和相机二

320
00:16:02,563 --> 00:16:06,300
你应该记得 两个相机通过光学中心

321
00:16:07,134 --> 00:16:08,435
这些特征是匹配关键点的

322
00:16:08,969 --> 00:16:09,970
假如说变暗了

323
00:16:10,571 --> 00:16:14,007
那么被观察点可能就没有

324
00:16:14,741 --> 00:16:18,145
颜色变得有点嘈杂

325
00:16:18,745 --> 00:16:19,847
另一个例子就是

326
00:16:19,913 --> 00:16:24,318
如果你将照相机指向一面

327
00:16:24,785 --> 00:16:29,323
墙上基本上没有特征 因此就很难找到

328
00:16:29,823 --> 00:16:32,459
无论是以上哪一个原因

329
00:16:32,860 --> 00:16:36,129
是根本找不到视差的

330
00:16:37,798 --> 00:16:41,835
洞在深度数据图中被表示为非数字

331
00:16:42,236 --> 00:16:45,772
标准的浮点表示形式

332
00:16:46,473 --> 00:16:50,043
深度图可能也会被处理来填补这些洞

333
00:16:50,477 --> 00:16:54,815
我们可以通过基于周围深度数据的

334
00:16:55,249 --> 00:16:58,151
或是通过使用RGB图像中

335
00:16:58,819 --> 00:17:02,122
isDepthDataFiltered属性

336
00:16:58,819 --> 00:17:02,122
isDepthDataFiltered属性

337
00:17:02,189 --> 00:17:05,358
它会告诉你是不是用这种方式

338
00:17:06,292 --> 00:17:08,896
若你接收到了一个未过滤的

339
00:17:08,962 --> 00:17:12,199
你在这个图中应该只能找到非数字的值

340
00:17:13,267 --> 00:17:17,671
我们一会儿再介绍如何请求过滤的内容

341
00:17:19,006 --> 00:17:22,542
第二个牵涉到准确视差生成的问题就是

342
00:17:22,608 --> 00:17:24,077
校准错误

343
00:17:24,511 --> 00:17:26,680
有许多校准错误

344
00:17:26,747 --> 00:17:30,450
是我们可以修正的

345
00:17:30,884 --> 00:17:34,021
那就是计算错误的光学中心

346
00:17:34,087 --> 00:17:36,323
不管是两个摄像头中的哪一个出了问题

347
00:17:36,957 --> 00:17:40,294
这里我是将针孔相机

348
00:17:40,360 --> 00:17:41,962
向下偏移了90度

349
00:17:42,029 --> 00:17:43,697
这样就可以在顶部留下更多的空间

350
00:17:44,331 --> 00:17:46,533
在一个理想的立体纠正系统中

351
00:17:46,834 --> 00:17:50,571
远景只会在一个方向上偏移

352
00:17:51,171 --> 00:17:52,673
随着这些相同的线

353
00:17:53,073 --> 00:17:55,943
如果有条光线是从相机一观察到的

354
00:17:56,810 --> 00:18:01,181
它就可以被看成是

355
00:17:56,810 --> 00:18:01,181
它就可以被看成是

356
00:18:01,248 --> 00:18:02,850
一条来自相机二线上的

357
00:18:02,916 --> 00:18:04,051
（校准错误）

358
00:18:04,117 --> 00:18:05,786
（相机1

359
00:18:05,853 --> 00:18:08,055
要精确地测量视差

360
00:18:09,022 --> 00:18:11,225
你必须要有一条精确的基线

361
00:18:11,758 --> 00:18:14,528
基线也就是两个光学中心之间的距离

362
00:18:15,028 --> 00:18:16,830
如果你没有精确的基线

363
00:18:16,897 --> 00:18:19,600
你就不能对齐两个相机的光学中心

364
00:18:19,666 --> 00:18:22,069
那么你就不能知道有多少的视差了

365
00:18:23,303 --> 00:18:28,242
那如果光学中心被算错了或者

366
00:18:28,876 --> 00:18:31,545
假如说真正的光学中心在这

367
00:18:32,112 --> 00:18:36,149
但是因为某些原因

368
00:18:36,884 --> 00:18:40,354
突然间我们相机二像平面上的

369
00:18:40,420 --> 00:18:44,258
都向左偏移了相同的固定数值

370
00:18:44,858 --> 00:18:49,129
现在所有的对象

371
00:18:50,063 --> 00:18:51,732
如果错误是发生在反方向的话

372
00:18:51,798 --> 00:18:54,301
这些物体

373
00:18:54,968 --> 00:18:57,271
我们可以探测到并修复许多问题

374
00:18:57,337 --> 00:18:59,873
但这个问题是我们不能探测并且修复的

375
00:18:59,940 --> 00:19:03,110
因为所有这些点看上去

376
00:18:59,940 --> 00:19:03,110
因为所有这些点看上去

377
00:19:03,377 --> 00:19:05,646
我们区分不出来是基线有问题

378
00:19:06,013 --> 00:19:08,949
还是人移动得更远或是更近了

379
00:19:10,117 --> 00:19:12,019
这是怎么发生的呢

380
00:19:12,085 --> 00:19:15,088
光学中心的计算会出问题呢？

381
00:19:16,056 --> 00:19:18,625
因为iPhone的摄像头

382
00:19:18,692 --> 00:19:23,730
它们用的是透镜

383
00:19:24,331 --> 00:19:25,799
如果使用了OIS

384
00:19:26,233 --> 00:19:29,970
那么透镜可能会横向移动

385
00:19:31,004 --> 00:19:34,208
重力也会发挥作用

386
00:19:35,075 --> 00:19:39,646
聚焦致动器实际上

387
00:19:40,113 --> 00:19:44,017
所有这些原因都可能会

388
00:19:44,418 --> 00:19:46,620
而这些光学中心位置的细微的错误

389
00:19:46,687 --> 00:19:49,690
会导致视差的巨大错误

390
00:19:50,457 --> 00:19:54,261
当发生这种情况时

391
00:19:54,661 --> 00:19:56,129
图中的每个像素点上

392
00:19:57,297 --> 00:20:01,335
视差值相对于彼此仍然是可用的

393
00:19:57,297 --> 00:20:01,335
视差值相对于彼此仍然是可用的

394
00:20:01,735 --> 00:20:05,873
但是它们不能再反映现实世界的距离了

395
00:20:07,474 --> 00:20:12,079
出于这个原因 AVDepthData对象

396
00:20:12,880 --> 00:20:16,416
绝对的精度值意味着单位

397
00:20:16,483 --> 00:20:20,120
确实能反映现实世界的距离

398
00:20:20,721 --> 00:20:24,424
相对精度意味着

399
00:20:25,092 --> 00:20:27,528
但是现实世界的比例已经丢失了

400
00:20:28,395 --> 00:20:30,998
对于从第三方相机捕捉的深度数据

401
00:20:31,064 --> 00:20:34,334
它可以被报告为绝对或是相对的

402
00:20:34,768 --> 00:20:38,939
但是iPhone 7 Plus

403
00:20:39,173 --> 00:20:41,608
这是因为我刚才提到过的校准错误

404
00:20:41,875 --> 00:20:43,810
但是我不想让你们被它吓到

405
00:20:43,877 --> 00:20:47,447
相对精度其实并不糟糕

406
00:20:47,948 --> 00:20:52,319
双摄像头的深度其实是完全够用的

407
00:20:53,587 --> 00:20:55,055
棒极了

408
00:20:55,889 --> 00:20:57,357
这里又有一些数学的问题

409
00:20:57,424 --> 00:21:01,461
比如

410
00:20:57,424 --> 00:21:01,461
比如

411
00:21:01,828 --> 00:21:04,565
也就是那个上面带有一个小帽子符号的

412
00:21:04,998 --> 00:21:09,670
因为它不是很好 它等同于

413
00:21:09,736 --> 00:21:11,872
加上一个固定量的错误

414
00:21:12,406 --> 00:21:14,908
我们不知道固定量的错误是多少

415
00:21:15,943 --> 00:21:19,446
现在让我们进行一个常见的操作

416
00:21:19,513 --> 00:21:22,149
同一个图中两个视差的不同之处

417
00:21:22,816 --> 00:21:24,718
这就像是减去不同的部分

418
00:21:24,785 --> 00:21:28,021
比方说

419
00:21:28,755 --> 00:21:33,961
这里你有两个不好的数据

420
00:21:34,194 --> 00:21:35,829
这就如同两个好的视差有着

421
00:21:35,896 --> 00:21:37,264
同样的固定错误

422
00:21:37,431 --> 00:21:38,765
如果我们把它们重新排列

423
00:21:38,832 --> 00:21:41,969
就会发现

424
00:21:42,035 --> 00:21:44,338
因为它们相互抵消了

425
00:21:45,172 --> 00:21:48,008
而我们剩下的

426
00:21:49,676 --> 00:21:52,513
这个令人开心的发现就是

427
00:21:53,046 --> 00:21:56,283
不管你的视差是

428
00:21:56,850 --> 00:21:59,620
这个公式在某种程度上证明了

429
00:21:59,686 --> 00:22:03,257
如果你要实现的效果只依赖于

430
00:21:59,686 --> 00:22:03,257
如果你要实现的效果只依赖于

431
00:22:03,323 --> 00:22:04,992
相同图中的不同之处的话

432
00:22:05,058 --> 00:22:08,662
这就是为什么

433
00:22:08,729 --> 00:22:09,963
看起来仍然很棒

434
00:22:12,299 --> 00:22:15,369
介绍完这个我觉得我们已经讲完了

435
00:22:15,435 --> 00:22:17,571
或者说我们已经差不多结束了

436
00:22:19,239 --> 00:22:21,441
是时候接着讲我们

437
00:22:21,508 --> 00:22:23,076
也就是流式深度

438
00:22:23,610 --> 00:22:25,612
我觉得这时候应该做个演示

439
00:22:31,718 --> 00:22:35,656
我们要做的演示叫

440
00:22:36,423 --> 00:22:40,227
这个应用是我们去年

441
00:22:40,394 --> 00:22:43,931
它会为你们展示如何将一个效果

442
00:22:44,464 --> 00:22:48,635
并且将同样的效果渲染到照片上

443
00:22:48,969 --> 00:22:50,938
去年它只有顶部的一个按钮

444
00:22:51,004 --> 00:22:53,340
它是用来过滤视频的

445
00:22:53,407 --> 00:22:56,743
有些劣质的玫瑰色

446
00:22:56,810 --> 00:22:59,179
不过它还是实时为你呈现了预览

447
00:22:59,246 --> 00:23:02,449
并且它还在你拍照的时候

448
00:22:59,246 --> 00:23:02,449
并且它还在你拍照的时候

449
00:23:02,883 --> 00:23:06,486
今年我们将一些深度加到了这个示例中

450
00:23:07,054 --> 00:23:12,059
借以向你们展示下

451
00:23:12,559 --> 00:23:15,495
现在我们要做的就是

452
00:23:15,729 --> 00:23:22,069
并且通过混合

453
00:23:22,302 --> 00:23:23,270
（深度）

454
00:23:24,204 --> 00:23:26,340
我要把我可爱的助手凡娜叫上来

455
00:23:26,406 --> 00:23:28,075
其实他叫埃瑞克

456
00:23:28,242 --> 00:23:29,843
他会上来给我们展示

457
00:23:30,477 --> 00:23:32,913
一些动态的东西

458
00:23:32,980 --> 00:23:33,847
我很喜欢它

459
00:23:34,014 --> 00:23:37,551
需要注意的是

460
00:23:37,618 --> 00:23:40,220
你肯定是能看到镜头里是什么

461
00:23:40,287 --> 00:23:42,756
有许多临时的问题

462
00:23:42,823 --> 00:23:47,060
不过我可以点击下Smooth按钮

463
00:23:47,127 --> 00:23:49,997
深度 填补了洞

464
00:23:50,063 --> 00:23:52,466
现在 这就是个看起来不错的视差了

465
00:23:52,533 --> 00:23:54,535
我要拍一张照片

466
00:23:56,570 --> 00:23:59,239
如果我现在回到照片应用中

467
00:23:59,907 --> 00:24:03,243
就能找到我们刚刚拍摄这张

468
00:23:59,907 --> 00:24:03,243
就能找到我们刚刚拍摄这张

469
00:24:03,710 --> 00:24:05,479
非常不错的深度展示

470
00:24:05,812 --> 00:24:07,915
现在这就是个有启发性的应用了

471
00:24:08,182 --> 00:24:12,019
因为我们终于能回答这个问题了：

472
00:24:12,386 --> 00:24:13,820
你的手套有多深呢？

473
00:24:14,321 --> 00:24:15,656
你们真的要学会啊

474
00:24:16,557 --> 00:24:18,125
好的 让我们回到幻灯片上

475
00:24:18,525 --> 00:24:19,760
（关于AVCAMPHOTOFILTER

476
00:24:20,861 --> 00:24:23,230
我知道已经很晚了

477
00:24:24,631 --> 00:24:26,266
我们是如何实现的呢

478
00:24:27,601 --> 00:24:29,803
AVFoundation框架的

479
00:24:29,870 --> 00:24:32,372
分为三个主要部分

480
00:24:32,706 --> 00:24:35,976
第一个是AVCaptureSession

481
00:24:36,476 --> 00:24:38,979
你可以让它开始或停止运行

482
00:24:39,046 --> 00:24:43,450
除非你给它某些输入

483
00:24:43,717 --> 00:24:45,686
例如AVCaptureDeviceInput

484
00:24:45,953 --> 00:24:48,956
这里我已经创建了一个

485
00:24:49,289 --> 00:24:51,191
它会给这个会话提供输入

486
00:24:51,491 --> 00:24:53,894
不过现在你需要将其

487
00:24:54,328 --> 00:24:58,398
现在我们就有了一种新的输出 叫作

488
00:24:59,166 --> 00:25:03,370
它在我们的团队中

489
00:24:59,166 --> 00:25:03,370
它在我们的团队中

490
00:25:03,971 --> 00:25:06,707
它的功能类似于

491
00:25:06,773 --> 00:25:09,910
除了VideoDataOutput是向

492
00:25:10,277 --> 00:25:13,347
而它提供的是

493
00:25:13,714 --> 00:25:16,383
也就是我刚才讲过的

494
00:25:16,450 --> 00:25:18,519
它以流的形式将它们送达

495
00:25:18,585 --> 00:25:20,454
（引入AVCAPTUREDEPTHDATAOUTPUT）

496
00:25:22,956 --> 00:25:26,059
AVCaptureDepthDataOutput

497
00:25:26,126 --> 00:25:28,962
你当然可以将它添加到

498
00:25:29,029 --> 00:25:32,266
不过这样你得不到深度

499
00:25:32,332 --> 00:25:35,669
因为这是唯一的双系统

500
00:25:35,736 --> 00:25:38,005
是我们用来计算视差的

501
00:25:39,106 --> 00:25:42,109
当你将一个DepthDataOutput

502
00:25:42,442 --> 00:25:43,310
会发生一些事情

503
00:25:43,677 --> 00:25:46,747
双摄像头会自动地放大到2倍

504
00:25:46,947 --> 00:25:49,683
也就是长焦的全部视野

505
00:25:50,017 --> 00:25:52,452
这是因为为了计算视差

506
00:25:52,519 --> 00:25:56,156
焦距必须相同

507
00:25:56,223 --> 00:25:59,259
广角与长焦摄像头的焦距是一致的

508
00:26:00,527 --> 00:26:03,764
与此同时 当你计算深度的时候

509
00:26:04,631 --> 00:26:05,632
我们添加了

510
00:26:05,699 --> 00:26:08,268
一些新的访问器

511
00:26:08,902 --> 00:26:13,340
在双摄像头系统中

512
00:26:13,407 --> 00:26:16,043
通过查询supported

513
00:26:17,144 --> 00:26:20,614
还有一个新的属性叫作

514
00:26:20,681 --> 00:26:23,584
它可以让你明白

515
00:26:23,650 --> 00:26:26,753
或是选择一个新的

516
00:26:28,288 --> 00:26:29,723
目前我们支持

517
00:26:29,790 --> 00:26:33,627
三种深度的视频分辨率

518
00:26:33,694 --> 00:26:35,729
让我来一个个介绍下

519
00:26:35,796 --> 00:26:38,031
第一个是广受欢迎的

520
00:26:38,665 --> 00:26:42,703
在照片预设中 你可以获得一个

521
00:26:42,769 --> 00:26:46,640
该预览来自于VideoDataOutput

522
00:26:46,840 --> 00:26:48,408
其来自于photoOutput

523
00:26:48,909 --> 00:26:52,212
这里你可以发现VideoDataOutput

524
00:26:52,279 --> 00:26:53,413
也就是屏幕尺寸的图像

525
00:26:53,847 --> 00:26:56,283
除此之外 如果你使用了

526
00:26:56,450 --> 00:27:00,387
你就可以得到一个320x240

527
00:26:56,450 --> 00:27:00,387
你就可以得到一个320x240

528
00:27:00,454 --> 00:27:01,488
为什么这么小呢？

529
00:27:01,688 --> 00:27:06,226
因为你需要消耗很多性能

530
00:27:06,827 --> 00:27:09,930
如果你愿意的话

531
00:27:09,997 --> 00:27:11,365
只有160x120的视差图

532
00:27:11,665 --> 00:27:15,469
第二个是16x9格式

533
00:27:16,003 --> 00:27:20,107
去年我们引入了一个720p

534
00:27:20,340 --> 00:27:23,610
该新格式最高能支持30fps的帧率

535
00:27:23,810 --> 00:27:26,813
它也支持

536
00:27:26,880 --> 00:27:29,449
以320x180或是

537
00:27:29,950 --> 00:27:34,555
最后 我们有个非常小的

538
00:27:34,621 --> 00:27:37,858
如果你想要个非常小而且快的视差图

539
00:27:41,361 --> 00:27:42,696
让我们来谈下帧率

540
00:27:42,763 --> 00:27:47,000
AVCaptureDevice能让你设置

541
00:27:47,067 --> 00:27:50,103
但是它不允许你

542
00:27:50,170 --> 00:27:51,738
独立于视频的帧率

543
00:27:52,206 --> 00:27:56,043
这是因为深度需要

544
00:27:56,109 --> 00:28:01,648
或者说保持在同等分割的

545
00:27:56,109 --> 00:28:01,648
或者说保持在同等分割的

546
00:28:02,316 --> 00:28:07,054
比如说

547
00:28:07,821 --> 00:28:11,225
深度能跟得上视频的最大帧率

548
00:28:11,658 --> 00:28:14,561
但是如果你选择的是30fps的视频

549
00:28:15,095 --> 00:28:18,932
深度就跟不上了

550
00:28:18,999 --> 00:28:20,868
这样就能得到

551
00:28:21,869 --> 00:28:23,403
（AVCaptureDepthDataOutput

552
00:28:23,470 --> 00:28:25,973
DepthDataOutput支持

553
00:28:26,039 --> 00:28:28,375
就像是我刚才在

554
00:28:28,742 --> 00:28:31,912
这样就可以填满洞

555
00:28:31,979 --> 00:28:36,149
随着你的移动 这样你就看不到

556
00:28:38,185 --> 00:28:41,188
好了

557
00:28:41,255 --> 00:28:43,790
四种数据输出

558
00:28:43,857 --> 00:28:46,793
我们现有四种了

559
00:28:46,860 --> 00:28:48,595
它在iOS 4中就已经出现了

560
00:28:49,029 --> 00:28:52,432
它会一个接一个地将视频帧

561
00:28:52,499 --> 00:28:56,403
以30fps或是60fps的流媒体方式

562
00:28:57,070 --> 00:28:58,939
还有一种是

563
00:28:59,239 --> 00:29:05,412
它通常一次会以44.1的速度

564
00:28:59,239 --> 00:29:05,412
它通常一次会以44.1的速度

565
00:29:06,580 --> 00:29:09,082
我们还有MetadataOutput

566
00:29:09,249 --> 00:29:13,720
面部 检测到的面部 或是条形码

567
00:29:14,121 --> 00:29:17,758
它们在寻找面部信息的时候

568
00:29:18,458 --> 00:29:20,727
我们新添加了

569
00:29:20,794 --> 00:29:23,597
也就是我刚刚提过的

570
00:29:23,664 --> 00:29:28,168
或是以视频可整除的帧率来送达

571
00:29:28,535 --> 00:29:30,871
这样就变得有点荒谬了

572
00:29:31,338 --> 00:29:33,173
要处理所有这些数据输出

573
00:29:33,240 --> 00:29:36,376
你必须要有一个非常成熟的缓冲机制

574
00:29:36,710 --> 00:29:38,812
来追踪所有进来的对象

575
00:29:38,879 --> 00:29:41,748
如果你要同时处理所有这些数据

576
00:29:41,982 --> 00:29:44,718
或是一并处理某个特定的表现时间

577
00:29:45,485 --> 00:29:51,925
我们发现这个问题已经有一阵子了

578
00:29:51,992 --> 00:29:54,928
证明它就是解决问题的桥梁

579
00:29:57,164 --> 00:29:59,800
掌声不是很响亮呀

580
00:30:00,000 --> 00:30:03,303
在iOS 11中 我们添加了一个

581
00:30:03,370 --> 00:30:06,807
叫作

582
00:30:07,341 --> 00:30:10,777
它可以在给定的呈现时间内

583
00:30:10,944 --> 00:30:13,280
在单个统一的回调函数中

584
00:30:13,547 --> 00:30:16,183
它会提供一个集合对象叫作

585
00:30:16,250 --> 00:30:18,819
AVCaptureSynchronized

586
00:30:19,353 --> 00:30:22,089
它能让你指定一个主输出

587
00:30:22,356 --> 00:30:24,992
也就是对你来说最重要的输出

588
00:30:25,058 --> 00:30:26,426
都同步到主输出上

589
00:30:26,660 --> 00:30:29,296
然后它就会保留所有多媒体信息

590
00:30:29,363 --> 00:30:32,165
只要你需要的话

591
00:30:32,232 --> 00:30:35,035
在给定的呈现时间内都是可用的

592
00:30:35,102 --> 00:30:36,637
回调函数之前

593
00:30:37,604 --> 00:30:40,474
它要么会给你所有输出的数据

594
00:30:40,541 --> 00:30:43,610
要么就是能确保

595
00:30:43,677 --> 00:30:46,547
它就会给你提供一个与它有关的集合

596
00:30:47,681 --> 00:30:49,049
这里有一个代码段

597
00:30:49,116 --> 00:30:52,519
它给你们展示了数据输出同步器的

598
00:30:52,786 --> 00:30:55,989
该回调函数会传给你一个

599
00:30:56,456 --> 00:30:58,892
这很酷

600
00:30:58,959 --> 00:31:01,895
或者是像词典那样使用

601
00:30:58,959 --> 00:31:01,895
或者是像词典那样使用

602
00:31:01,962 --> 00:31:05,232
你可以像是遍历数组一样遍历它

603
00:31:05,299 --> 00:31:08,135
你可以使用快速枚举来获取

604
00:31:08,202 --> 00:31:09,536
当前集合中所有对象的列表

605
00:31:10,437 --> 00:31:14,942
或者说如果你想将它

606
00:31:15,275 --> 00:31:18,812
你就可以通过索引数据输出的下标

607
00:31:18,879 --> 00:31:20,214
来得到你想要的结果

608
00:31:20,280 --> 00:31:22,850
例如

609
00:31:22,916 --> 00:31:25,652
该结果来自于DepthDataOutput

610
00:31:25,953 --> 00:31:26,854
它就会把结果给你

611
00:31:27,087 --> 00:31:29,957
你需要在代码中

612
00:31:30,023 --> 00:31:32,693
因为你可能在给定的呈现时间内

613
00:31:34,494 --> 00:31:35,329
好了

614
00:31:35,395 --> 00:31:38,599
这里举个如何使用AVCaptureData

615
00:31:38,665 --> 00:31:42,703
这里又用到了AVCamPhotoFilter

616
00:31:42,769 --> 00:31:45,239
它被关联到这个演讲中了

617
00:31:47,140 --> 00:31:49,276
在iOS 11中

618
00:31:49,343 --> 00:31:50,844
它有点偏离今天的主题

619
00:31:51,278 --> 00:31:56,350
那就是对于每个视频帧的

620
00:31:56,650 --> 00:31:58,151
当你在使用

621
00:31:58,719 --> 00:32:00,754
回忆下我们前面讲到的针孔相机

622
00:31:58,719 --> 00:32:00,754
回忆下我们前面讲到的针孔相机

623
00:32:01,455 --> 00:32:04,491
它可以将3D空间中的点

624
00:32:04,558 --> 00:32:08,228
我们需要两种信息

625
00:32:08,562 --> 00:32:11,665
或者说主点

626
00:32:12,065 --> 00:32:14,568
在计算机视觉中

627
00:32:14,835 --> 00:32:18,305
来将2D图像重新投影回3D空间

628
00:32:18,505 --> 00:32:20,974
通过使用逆转换

629
00:32:21,208 --> 00:32:24,344
这也是新的ARKit的重点

630
00:32:25,212 --> 00:32:30,450
作为iOS 11中的新功能

631
00:32:30,884 --> 00:32:33,720
此函数适用于你所送达的每个视频帧

632
00:32:33,921 --> 00:32:36,757
并且你可以通过调用

633
00:32:37,157 --> 00:32:40,360
isCameraIntrinsicMatrixDelivery

634
00:32:41,094 --> 00:32:45,465
当你这么做的时候就可以

635
00:32:45,532 --> 00:32:46,533
该附件包含内在功能

636
00:32:47,000 --> 00:32:48,702
让我展示矩阵本身是什么样子的

637
00:32:48,769 --> 00:32:52,239
它可能看上去挺复杂的

638
00:32:52,973 --> 00:32:55,576
相机内在函数是一个3x3的矩阵

639
00:32:55,642 --> 00:32:59,246
它描述了相机的几何属性

640
00:32:59,813 --> 00:33:02,983
fx和fy是以像素表示的焦距

641
00:32:59,813 --> 00:33:02,983
fx和fy是以像素表示的焦距

642
00:33:03,617 --> 00:33:06,486
它们是单独的x和y值

643
00:33:06,553 --> 00:33:09,756
具有变形镜头或者变形像素

644
00:33:10,324 --> 00:33:14,027
在iOS设备上 我们的相机

645
00:33:14,094 --> 00:33:16,697
所以fx和fy总是相等的

646
00:33:17,764 --> 00:33:21,802
x0和y0是两个像素坐标

647
00:33:21,869 --> 00:33:24,571
它们是镜头主点

648
00:33:25,172 --> 00:33:28,308
它们都是以像素为单位表示的数值

649
00:33:28,375 --> 00:33:30,711
提供它们的视频缓冲区的

650
00:33:31,345 --> 00:33:34,615
所以一旦你选择了

651
00:33:34,681 --> 00:33:37,784
而且你会从它们那得到这个附件

652
00:33:38,285 --> 00:33:43,023
其有效载荷是一个C/F数据

653
00:33:43,357 --> 00:33:44,691
也就是SIMD数据类型

654
00:33:45,225 --> 00:33:48,996
如果你在做计算机视觉相关的工作

655
00:33:50,531 --> 00:33:54,434
好了

656
00:33:55,802 --> 00:33:56,970
这次不错

657
00:33:57,037 --> 00:33:59,339
接下来让我们介绍下照片的拍摄

658
00:33:59,806 --> 00:34:01,008
让我们从演示开始吧

659
00:33:59,806 --> 00:34:01,008
让我们从演示开始吧

660
00:34:01,074 --> 00:34:02,843
AVCam

661
00:34:07,714 --> 00:34:10,551
这里有两个内容要讲

662
00:34:11,118 --> 00:34:13,587
AVCam是示例代码的重要部分

663
00:34:13,654 --> 00:34:14,855
它会展示如何

664
00:34:15,054 --> 00:34:18,425
使用AVFoundation

665
00:34:19,826 --> 00:34:22,696
这里要注意的是 尽管我们已经

666
00:34:22,763 --> 00:34:24,764
你还是看不到任何与深度有关的东西

667
00:34:25,165 --> 00:34:29,870
这是因为当我拍摄这里的铅笔时

668
00:34:29,937 --> 00:34:33,739
你实际上是看不到深度的表现的

669
00:34:34,541 --> 00:34:37,744
但是它已经被保存在照片中了

670
00:34:38,745 --> 00:34:42,081
让我来看一下 比方说

671
00:34:42,748 --> 00:34:44,851
上面弹出了什么

672
00:34:44,918 --> 00:34:48,789
现在如果我点击Depth按钮

673
00:34:48,856 --> 00:34:50,858
应用到背景当中

674
00:34:51,225 --> 00:34:55,629
所以现在你在应用中

675
00:34:55,696 --> 00:35:00,033
也可以使用这个浅深度域的效果了

676
00:34:55,696 --> 00:35:00,033
也可以使用这个浅深度域的效果了

677
00:35:00,367 --> 00:35:01,502
这真的很酷

678
00:35:01,969 --> 00:35:05,272
我们还可以利用深度

679
00:35:05,506 --> 00:35:07,941
既然现在所有的照片都有深度数据了

680
00:35:08,008 --> 00:35:11,311
另外 在iOS 11中

681
00:35:11,378 --> 00:35:15,449
人像模式拍摄的照片现在

682
00:35:15,516 --> 00:35:18,752
它们就是你

683
00:35:19,520 --> 00:35:21,722
我要用这个叫作

684
00:35:22,089 --> 00:35:24,992
来展示一些

685
00:35:26,894 --> 00:35:30,631
我挑个简单的开始讲

686
00:35:31,098 --> 00:35:33,233
此功能实现的是将一些平面的东西

687
00:35:33,867 --> 00:35:37,337
重新投影到3D空间

688
00:35:37,771 --> 00:35:42,576
使其看起来有点滚动的效果

689
00:35:42,643 --> 00:35:45,746
我还可以使用陀螺仪来移动我的手机

690
00:35:46,813 --> 00:35:48,015
这难道不是精妙的效果吗？

691
00:35:48,081 --> 00:35:49,449
它看起来栩栩如生的

692
00:35:50,017 --> 00:35:51,652
我想再挑个功能来讲讲

693
00:35:52,953 --> 00:35:54,221
我真的很喜欢这只狗

694
00:35:55,055 --> 00:35:56,089
这只狗看上去很不错

695
00:35:56,156 --> 00:35:59,960
现在他看上去就在

696
00:36:00,027 --> 00:36:02,429
左右地移动了

697
00:36:02,496 --> 00:36:05,832
强制让透视发生变化

698
00:36:05,899 --> 00:36:09,803
深度数据在哪里之后

699
00:36:10,804 --> 00:36:11,705
放大吧多利

700
00:36:13,140 --> 00:36:14,808
放大吧多利 挑衅吧狗狗

701
00:36:15,008 --> 00:36:17,711
我希望在多利放大的时候

702
00:36:17,978 --> 00:36:19,980
因为它有点像是条黑帮狗

703
00:36:22,182 --> 00:36:23,951
我觉得最适合这里的音乐

704
00:36:24,017 --> 00:36:26,253
应该就是“Rolling in the Deep”了

705
00:36:28,488 --> 00:36:30,824
你们做得很好 我很感谢

706
00:36:30,891 --> 00:36:31,959
我真的很感谢

707
00:36:37,030 --> 00:36:39,433
在拍摄带有深度的照片时

708
00:36:39,600 --> 00:36:42,269
我们提供了很多的拍摄选项

709
00:36:43,103 --> 00:36:45,305
你可以使用深度进行带闪光灯的拍摄

710
00:36:45,372 --> 00:36:47,975
你可以使用深度进行静态图像防抖

711
00:36:48,442 --> 00:36:53,280
你甚至可以实现自动包围曝光

712
00:36:53,881 --> 00:36:58,619
你可以利用保存在照片中的深度数据

713
00:37:00,687 --> 00:37:05,225
你可以使用AVCapturePhotoOutput

714
00:37:05,659 --> 00:37:08,629
这个类是我们去年引入的

715
00:37:08,695 --> 00:37:10,130
AVCaptureStillImageOutput

716
00:37:10,898 --> 00:37:14,368
它可以非常出色地处理复杂的照片请求

717
00:37:14,735 --> 00:37:18,071
我所说的是可以获得多个资源的请求

718
00:37:18,605 --> 00:37:23,243
而这些资源是必须被追踪和送达

719
00:37:23,710 --> 00:37:27,581
JPEG图片

720
00:37:27,648 --> 00:37:30,651
你会在不同的时间点得到很多东西

721
00:37:30,717 --> 00:37:33,220
这里的编程模型要你填写一个请求

722
00:37:33,787 --> 00:37:35,923
该请求叫作

723
00:37:36,156 --> 00:37:38,792
你通过传递请求来开始照片的拍摄

724
00:37:39,092 --> 00:37:40,794
你还要传递稍后会被调用的代理

725
00:37:41,562 --> 00:37:43,330
因为你的photoOutput是

726
00:37:43,397 --> 00:37:48,669
唯一用来拍摄Live Photo

727
00:37:49,436 --> 00:37:53,440
裸RAW图像

728
00:37:54,374 --> 00:37:58,045
它还是iOS 11中唯一可以

729
00:37:58,111 --> 00:38:02,516
拍摄HEIF文件格式的方法

730
00:37:58,111 --> 00:38:02,516
拍摄HEIF文件格式的方法

731
00:38:04,051 --> 00:38:05,853
你需要对

732
00:38:05,919 --> 00:38:08,789
AVCapturePhotoOutput进行许多修改

733
00:38:09,356 --> 00:38:13,594
所以在iOS 11中

734
00:38:13,660 --> 00:38:16,163
我们添加了一个新的代理回调函数

735
00:38:17,197 --> 00:38:18,832
它很简单

736
00:38:19,066 --> 00:38:22,002
它用来代替

737
00:38:22,069 --> 00:38:24,371
你获取样本缓冲区的回调函数

738
00:38:24,438 --> 00:38:27,508
现在你会得到一个叫作

739
00:38:28,308 --> 00:38:32,212
AVCapturePhoto是

740
00:38:32,479 --> 00:38:35,315
因此如果你想用深度的话

741
00:38:35,482 --> 00:38:37,684
就得实现这个新的代理回调函数

742
00:38:39,553 --> 00:38:42,523
另外 你需要

743
00:38:42,589 --> 00:38:45,492
在会话开始前明确选择

744
00:38:46,126 --> 00:38:48,862
为什么呢？如果你们还记得的话

745
00:38:48,929 --> 00:38:51,098
在处理深度的时候

746
00:38:51,164 --> 00:38:54,835
它需要放大到2倍

747
00:38:55,335 --> 00:38:58,205
它需要锁定变焦功能

748
00:38:59,106 --> 00:39:02,309
想要实现这个功能

749
00:38:59,106 --> 00:39:02,309
想要实现这个功能

750
00:39:02,376 --> 00:39:06,146
你就需要告诉photoOutput

751
00:39:06,613 --> 00:39:10,551
然后根据每张照片的请求

752
00:39:10,617 --> 00:39:14,087
拍照的时候 你就会填写一个设置对象

753
00:39:14,288 --> 00:39:16,290
“我想在这张照片中包含深度”

754
00:39:17,824 --> 00:39:21,762
然后你就可以使用返回的

755
00:39:22,162 --> 00:39:25,199
它有一个叫作

756
00:39:25,265 --> 00:39:28,068
哇 这个AVDepthData

757
00:39:28,535 --> 00:39:31,205
它就像是深入集成到了API中

758
00:39:34,441 --> 00:39:36,310
iOS上绝大多数的

759
00:39:36,376 --> 00:39:38,111
格式都具有

760
00:39:38,178 --> 00:39:41,849
更高的静态分辨率

761
00:39:42,316 --> 00:39:43,450
看下我们

762
00:39:43,517 --> 00:39:45,752
在iPhone 7 Plus上

763
00:39:46,253 --> 00:39:49,690
你就可以发现流式视频分辨率

764
00:39:49,756 --> 00:39:52,426
可以和你获得的高分辨率照片相媲美

765
00:39:52,726 --> 00:39:55,062
例如 对于照片来说

766
00:39:55,128 --> 00:39:58,999
你只能得到屏幕大小的缓冲区

767
00:39:59,700 --> 00:40:01,368
对于深度也是这样

768
00:39:59,700 --> 00:40:01,368
对于深度也是这样

769
00:40:02,569 --> 00:40:04,471
我说过当我们在用流处理深度数据时

770
00:40:04,538 --> 00:40:09,776
有很多需要实时完成的工作

771
00:40:09,843 --> 00:40:12,613
但是当处理照片的时候

772
00:40:12,679 --> 00:40:14,915
因为它不需要实时送达

773
00:40:14,982 --> 00:40:17,417
所以我们可以就可以给你

774
00:40:17,518 --> 00:40:20,687
好看的图

775
00:40:21,822 --> 00:40:24,825
其长宽比总是与视频的长宽比保持一致

776
00:40:24,892 --> 00:40:27,928
如果你处理的是16x9的视频

777
00:40:30,297 --> 00:40:35,369
好了

778
00:40:36,236 --> 00:40:41,175
我们捕捉并且嵌入到照片中的深度图

779
00:40:41,942 --> 00:40:45,412
我很抱歉给你们带来这个消息

780
00:40:45,846 --> 00:40:46,914
让我来解释下为什么

781
00:40:47,781 --> 00:40:52,186
所有我给你们展示过的相机图

782
00:40:52,853 --> 00:40:54,588
针孔相机没有镜头

783
00:40:55,189 --> 00:40:57,524
所以图像就是直线的

784
00:40:58,225 --> 00:41:02,462
也就是说 光会以直线形式穿过小孔

785
00:40:58,225 --> 00:41:02,462
也就是说 光会以直线形式穿过小孔

786
00:41:02,896 --> 00:41:06,500
并且呈现一个几何完美的

787
00:41:06,567 --> 00:41:09,903
复制倒置物体在像平面上

788
00:41:10,671 --> 00:41:13,340
如果你有这样一个完美的正方形网格

789
00:41:13,407 --> 00:41:16,109
并且用针孔相机给它拍了张照片的话

790
00:41:16,176 --> 00:41:18,946
它在像平面上看起来就是这样的

791
00:41:19,479 --> 00:41:22,216
直线会一直保持直的

792
00:41:23,884 --> 00:41:26,687
不幸的是 在现实世界中

793
00:41:26,753 --> 00:41:30,090
所以我们需要镜头

794
00:41:30,824 --> 00:41:33,627
这些失真也会存在于所拍摄的图像中

795
00:41:33,694 --> 00:41:35,762
因为它们会以

796
00:41:35,963 --> 00:41:40,100
有点奇怪的方式弯曲

797
00:41:40,400 --> 00:41:41,902
在一种极端情况下

798
00:41:41,969 --> 00:41:45,205
由一个坏镜头所捕获的直线

799
00:41:45,739 --> 00:41:48,041
这样不利于寻找视差

800
00:41:48,108 --> 00:41:51,144
因为两个图像需要匹配得上

801
00:41:51,512 --> 00:41:54,348
如果相机一有一组失真

802
00:41:54,448 --> 00:41:56,817
而相机二有一组不同的失真

803
00:41:56,884 --> 00:42:00,687
你要如何才能

804
00:41:56,884 --> 00:42:00,687
你要如何才能

805
00:42:00,754 --> 00:42:02,356
因为它们失真的地方不一样呢？

806
00:42:04,625 --> 00:42:08,662
我在描述如何计算视差的时候

807
00:42:09,062 --> 00:42:10,564
我现在就把它补充一下

808
00:42:10,964 --> 00:42:13,967
在比较长焦和广角图像之前

809
00:42:14,034 --> 00:42:15,335
我们还需要做一件事

810
00:42:15,969 --> 00:42:19,239
我们要让这些失真的图像直线化

811
00:42:19,573 --> 00:42:24,144
也就是说 我们要利用一组校准系数

812
00:42:24,411 --> 00:42:27,447
这些系数代表了镜头的失真标准

813
00:42:28,549 --> 00:42:30,083
在每个图像都被修正后

814
00:42:30,551 --> 00:42:32,219
看起来就是这样的

815
00:42:32,519 --> 00:42:33,654
笔直的直线

816
00:42:34,421 --> 00:42:37,324
我们现在就可以确定地

817
00:42:37,391 --> 00:42:42,763
并且能够找到一个完美的 现实世界的

818
00:42:43,197 --> 00:42:44,431
看起来就像是这样

819
00:42:45,666 --> 00:42:47,201
我们有了一个完全相反的问题

820
00:42:47,668 --> 00:42:50,037
视差图匹配了物理世界

821
00:42:50,537 --> 00:42:52,973
但是它跟我们刚刚拍摄的图像并不匹配

822
00:42:53,040 --> 00:42:55,142
也就是由于镜头而变得失真的图像

823
00:42:55,475 --> 00:42:57,244
所以现在我们要完成另外一个步骤

824
00:42:57,444 --> 00:43:02,349
就是要将视差图通过失真

825
00:42:57,444 --> 00:43:02,349
就是要将视差图通过失真

826
00:43:02,416 --> 00:43:06,720
我们要使用一组

827
00:43:07,020 --> 00:43:10,624
这样最终的视差图

828
00:43:10,691 --> 00:43:12,192
跟它对应的图像一样

829
00:43:13,293 --> 00:43:15,596
要我说的话 这是个好事

830
00:43:16,129 --> 00:43:17,764
这就意味着我们有了开箱即用的

831
00:43:18,065 --> 00:43:23,604
深度图

832
00:43:23,670 --> 00:43:24,571
和进行特效处理

833
00:43:24,638 --> 00:43:27,341
它们总是与相关的图像匹配

834
00:43:27,774 --> 00:43:29,443
如果你想要实现某种特效

835
00:43:29,510 --> 00:43:33,213
如果你想实现一些

836
00:43:33,280 --> 00:43:36,950
或是想让照片实现一些有趣的效果

837
00:43:37,017 --> 00:43:38,719
就像是我最开始时所展示的那样

838
00:43:38,986 --> 00:43:40,254
它们就能完美实现这一点

839
00:43:40,654 --> 00:43:43,924
但它们不适用于重建3D场景

840
00:43:44,424 --> 00:43:47,194
如果你想实现该功能

841
00:43:47,995 --> 00:43:48,862
你是可以做到的

842
00:43:48,929 --> 00:43:50,330
我马上就会介绍一下

843
00:43:52,032 --> 00:43:53,667
我想要简单地介绍一下

844
00:43:53,734 --> 00:43:57,938
我们图像文件中

845
00:43:58,572 --> 00:44:02,509
在iOS 11中 我们支持两种

846
00:43:58,572 --> 00:44:02,509
在iOS 11中 我们支持两种

847
00:44:02,609 --> 00:44:05,746
第一种是HEIF HEVC

848
00:44:06,146 --> 00:44:08,215
也叫作HEIC文件

849
00:44:08,815 --> 00:44:11,785
而它有着对于深度最好的支持

850
00:44:12,352 --> 00:44:15,589
在此文件中有一个区域

851
00:44:15,656 --> 00:44:19,393
它可以保存视差 深度

852
00:44:19,860 --> 00:44:21,195
我们会把它保存在这里

853
00:44:21,595 --> 00:44:24,798
我们将其编码为单色HEVC

854
00:44:25,465 --> 00:44:30,470
我们还保存了一些

855
00:44:30,537 --> 00:44:34,508
例如关于它是否被过滤的信息

856
00:44:34,741 --> 00:44:36,476
它的精度

857
00:44:37,211 --> 00:44:39,947
诸如镜头失真这样的相机校准信息

858
00:44:40,214 --> 00:44:42,282
以及一些渲染指令

859
00:44:42,549 --> 00:44:47,154
所有这些元数据

860
00:44:48,622 --> 00:44:50,591
我们支持的第二种格式是JPEG

861
00:44:51,391 --> 00:44:55,495
虽然JPEG不是个很好的方法

862
00:44:56,096 --> 00:45:00,200
如果是过滤的深度图

863
00:44:56,096 --> 00:45:00,200
如果是过滤的深度图

864
00:45:00,300 --> 00:45:02,202
而如果它里面包含了非数字

865
00:45:02,369 --> 00:45:07,975
我们就使用16位的无损JPEG编码

866
00:45:08,041 --> 00:45:11,478
我们会将它作为第二个图像

867
00:45:11,545 --> 00:45:14,314
它就像是一个多画面对象

868
00:45:15,048 --> 00:45:20,387
我们同样将元数据保存为XMP

869
00:45:23,423 --> 00:45:27,194
好了 现在让我介绍下

870
00:45:27,261 --> 00:45:29,029
它就是双照片拍摄

871
00:45:30,631 --> 00:45:31,732
这是什么意思呢

872
00:45:32,132 --> 00:45:35,702
到目前为止

873
00:45:35,769 --> 00:45:37,237
你还是只能得到一张照片

874
00:45:37,304 --> 00:45:39,506
它或者是用广角拍摄的

875
00:45:39,573 --> 00:45:40,974
取决于你缩放的倍数

876
00:45:41,041 --> 00:45:45,612
你是处于1和2X之间的区域

877
00:45:45,679 --> 00:45:48,615
的混合

878
00:45:48,682 --> 00:45:49,917
但你还是只能获得一张照片

879
00:45:50,517 --> 00:45:54,588
你们一直想要两张照片

880
00:45:54,721 --> 00:45:58,392
通过一个单一请求

881
00:45:58,458 --> 00:46:00,894
全幅1200万像素照片

882
00:45:58,458 --> 00:46:00,894
全幅1200万像素照片

883
00:46:01,094 --> 00:46:02,896
你能随心所欲

884
00:46:08,769 --> 00:46:09,670
让我说下它是如何实现的

885
00:46:09,736 --> 00:46:11,572
它和前面讲过的

886
00:46:12,439 --> 00:46:14,141
在开始拍摄会话之前

887
00:46:14,208 --> 00:46:16,710
你需要告诉photoOutput

888
00:46:17,244 --> 00:46:20,347
我需要拍摄双照片

889
00:46:21,248 --> 00:46:25,552
然后当你在拍摄基于

890
00:46:25,619 --> 00:46:27,621
你就可以完成你的设定了

891
00:46:28,088 --> 00:46:32,626
我想将特定的照片拍成是双照片

892
00:46:34,595 --> 00:46:38,599
当你这么做的时候

893
00:46:38,899 --> 00:46:40,634
这并不是说你会得到两个回调函数

894
00:46:40,901 --> 00:46:45,739
假如说你请求的是RAW

895
00:46:45,806 --> 00:46:48,809
那么你就会得到四个回调函数

896
00:46:48,876 --> 00:46:52,412
两张广角和两张长焦的

897
00:46:53,013 --> 00:46:55,182
所以不管你之前得到多少个回调函数

898
00:46:55,249 --> 00:46:56,917
现在数量会翻倍

899
00:46:58,952 --> 00:47:03,891
现在我们支持了所有与深度相关的功能

900
00:46:58,952 --> 00:47:03,891
现在我们支持了所有与深度相关的功能

901
00:47:03,957 --> 00:47:06,693
你可以使用闪光灯来拍摄双照片

902
00:47:06,793 --> 00:47:13,100
自动SIS 包围曝光

903
00:47:15,269 --> 00:47:16,870
我们是如何处理变焦的呢？

904
00:47:17,070 --> 00:47:21,341
这里会出现安全性和信任的问题

905
00:47:21,742 --> 00:47:25,712
假如说你的应用只显示长焦的视野

906
00:47:26,380 --> 00:47:29,116
广角摄像头有更多的信息

907
00:47:29,183 --> 00:47:30,284
所以在你拍照的时候

908
00:47:30,384 --> 00:47:34,087
你实际上会拍到可视区域之外的内容

909
00:47:34,154 --> 00:47:35,889
这就会产生隐私的问题

910
00:47:36,156 --> 00:47:37,758
所以在你变焦的时候

911
00:47:38,058 --> 00:47:42,129
我们会提供双照片

912
00:47:42,429 --> 00:47:45,332
这样它们就与预览中

913
00:47:45,766 --> 00:47:47,334
如果你想要完整的图片

914
00:47:47,401 --> 00:47:49,670
你可以将变焦设置成1倍

915
00:47:50,504 --> 00:47:53,774
你怎么知道外面是否有变黑的区域呢？

916
00:47:54,174 --> 00:47:57,411
在图片中我们保存了

917
00:47:57,477 --> 00:47:59,880
该矩形定义了有效像素的区域

918
00:48:02,115 --> 00:48:05,686
双照片也可以通过相机校准数据送达

919
00:48:06,520 --> 00:48:09,656
相机校准数据

920
00:48:09,723 --> 00:48:14,761
可以用来实现

921
00:48:15,095 --> 00:48:17,097
镜头失真修正等等

922
00:48:17,664 --> 00:48:23,136
有了广角镜头 长焦镜头

923
00:48:23,570 --> 00:48:25,405
你就可以制作自己的深度图了

924
00:48:25,806 --> 00:48:28,375
我希望你们可以做出一个

925
00:48:29,576 --> 00:48:32,746
你还可以实现增强现实

926
00:48:33,013 --> 00:48:36,650
让我来介绍下

927
00:48:36,717 --> 00:48:38,952
这是我今晚要介绍的最后一个东西了

928
00:48:39,887 --> 00:48:45,292
AVCameraCalibrationData

929
00:48:46,126 --> 00:48:47,094
它在哪里出现呢？

930
00:48:47,160 --> 00:48:50,697
如果你请求深度数据的话

931
00:48:50,797 --> 00:48:52,332
它是深度的一个属性

932
00:48:53,367 --> 00:48:57,504
你也可以通过AVCapturePhoto

933
00:48:58,172 --> 00:49:02,042
你可以说我想对照片进行相机校准

934
00:48:58,172 --> 00:49:02,042
你可以说我想对照片进行相机校准

935
00:49:02,376 --> 00:49:03,577
就可以使用这个属性了

936
00:49:03,644 --> 00:49:05,946
如果你要进行双照片拍摄

937
00:49:06,013 --> 00:49:10,250
并且请求相机校准

938
00:49:10,384 --> 00:49:12,085
并且你会获得广角效果的校准

939
00:49:12,186 --> 00:49:14,655
以及长焦效果的校准了

940
00:49:16,190 --> 00:49:18,025
intrinsicMatrix

941
00:49:18,091 --> 00:49:19,726
我希望它能有点相似性

942
00:49:19,793 --> 00:49:22,029
因为它差不多等同于我们之前见过的

943
00:49:22,095 --> 00:49:24,097
流式

944
00:49:24,698 --> 00:49:29,269
它也是一个3x3矩阵 并且包含了

945
00:49:30,437 --> 00:49:34,842
它被用来从3D空间

946
00:49:34,908 --> 00:49:38,011
转换到2D空间

947
00:49:38,078 --> 00:49:40,848
你可以在想变回3D空间的时候

948
00:49:42,349 --> 00:49:45,352
它具有像素焦距

949
00:49:45,452 --> 00:49:48,689
这又是两个不同的数字

950
00:49:48,755 --> 00:49:51,325
不过因为我们的像素是正方形像素

951
00:49:53,026 --> 00:49:56,830
它也有对应着光学中心的x和y坐标

952
00:49:58,465 --> 00:50:02,936
像素值是按照参考帧的分辨率给定的

953
00:49:58,465 --> 00:50:02,936
像素值是按照参考帧的分辨率给定的

954
00:50:04,004 --> 00:50:06,406
深度数据可能有很低的分辨率

955
00:50:06,473 --> 00:50:08,642
我们不想把这么低分辨率的像素值

956
00:50:08,709 --> 00:50:12,112
因此我们提供了一个独立的空间集

957
00:50:12,279 --> 00:50:14,448
它们通常是传感器的完整大小

958
00:50:14,515 --> 00:50:16,783
因此你可以获得很高的精度

959
00:50:16,850 --> 00:50:20,020
以及很高的分辨率

960
00:50:21,822 --> 00:50:24,191
接下来是extrinsicMatrix

961
00:50:24,725 --> 00:50:28,395
这个属性描述的是

962
00:50:29,463 --> 00:50:33,066
你会在下面的情景要到这个属性

963
00:50:33,133 --> 00:50:36,470
进行三角测量的时候

964
00:50:37,004 --> 00:50:41,875
我们的外在功能

965
00:50:41,942 --> 00:50:44,678
不过有点像是把两个矩阵压到了一起

966
00:50:45,345 --> 00:50:48,749
首先左边的那个是旋转矩阵

967
00:50:49,082 --> 00:50:53,320
它是个3x3的矩阵

968
00:50:53,387 --> 00:50:55,656
根据现实世界的初始状态

969
00:50:55,722 --> 00:50:56,957
不管原来到底是什么样的

970
00:50:57,291 --> 00:51:01,828
另外还有一个1x3的矩阵

971
00:50:57,291 --> 00:51:01,828
另外还有一个1x3的矩阵

972
00:51:01,895 --> 00:51:04,698
或者说相对于现实世界初始状态的距离

973
00:51:05,732 --> 00:51:10,771
需要注意的是

974
00:51:10,838 --> 00:51:13,473
当你使用双摄像头时候很容易就能实现

975
00:51:13,540 --> 00:51:15,175
如果你只想要一个长焦图像

976
00:51:15,943 --> 00:51:19,413
那么你会得到一个单位矩阵

977
00:51:19,947 --> 00:51:22,182
如果你要使用广角和长焦的话

978
00:51:22,482 --> 00:51:25,252
广角就不会是单位矩阵

979
00:51:25,319 --> 00:51:31,558
因为它描述了

980
00:51:31,725 --> 00:51:33,560
但是通过使用外在功能 你就可以

981
00:51:33,627 --> 00:51:36,096
计算广角和长焦之间的基线

982
00:51:38,465 --> 00:51:40,000
还有其他一些属性是用来处理

983
00:51:40,067 --> 00:51:43,337
我们前面介绍过的

984
00:51:43,770 --> 00:51:45,539
它们适用于你需要

985
00:51:45,606 --> 00:51:48,809
处理一个图像或是

986
00:51:50,677 --> 00:51:52,813
有两个你需要注意的属性

987
00:51:52,880 --> 00:51:55,782
第一个是

988
00:51:56,650 --> 00:51:58,785
它描述了传感器上

989
00:51:58,852 --> 00:52:02,823
与镜头失真中心重合的点

990
00:51:58,852 --> 00:52:02,823
与镜头失真中心重合的点

991
00:52:03,190 --> 00:52:07,060
这通常与镜头的光学中心并不相同

992
00:52:07,127 --> 00:52:09,496
如果你把所有的失真

993
00:52:09,563 --> 00:52:12,599
镜头上的径向失真

994
00:52:12,666 --> 00:52:15,269
那么它就是年轮的中心

995
00:52:16,236 --> 00:52:18,572
除了这个失真中心

996
00:52:18,906 --> 00:52:21,742
我们还有一个叫

997
00:52:22,176 --> 00:52:26,680
你可以把它想成是多个浮点

998
00:52:27,214 --> 00:52:30,951
这些浮点会将lensDistortionCenter

999
00:52:31,185 --> 00:52:34,087
如果你从这些浮点上画一些小的圆圈

1000
00:52:34,321 --> 00:52:36,123
你就可以得到一些

1001
00:52:36,190 --> 00:52:38,458
它会给你展现镜头的径向失真

1002
00:52:39,226 --> 00:52:43,964
lensDistortionLookupTable

1003
00:52:45,232 --> 00:52:49,069
如果沿着这些虚线的每个点都是0的话

1004
00:52:49,136 --> 00:52:52,039
你就会得到世界上独一无二的

1005
00:52:52,172 --> 00:52:54,308
它根本没有径向失真

1006
00:52:54,942 --> 00:52:56,810
如果这里是一个正值

1007
00:52:56,910 --> 00:52:59,947
就表明半径有延长的地方

1008
00:53:00,514 --> 00:53:04,017
如果是负值

1009
00:53:04,351 --> 00:53:06,854
但是如果总体来看整个表格

1010
00:53:06,920 --> 00:53:11,258
你就可以发现镜头的颠簸情况

1011
00:53:12,993 --> 00:53:15,262
要将失真修正应用到一个图像中

1012
00:53:15,329 --> 00:53:17,931
首先你要建立一个空的目标缓冲区

1013
00:53:18,098 --> 00:53:20,300
然后对它进行逐行遍历

1014
00:53:20,367 --> 00:53:24,037
对于每个点 你都要用

1015
00:53:24,238 --> 00:53:27,174
在在失真图像中查找对应的值

1016
00:53:27,474 --> 00:53:30,811
将该值写入到输出缓冲区中

1017
00:53:31,545 --> 00:53:33,847
这部分代码很难写

1018
00:53:33,981 --> 00:53:34,848
我们也知道

1019
00:53:34,915 --> 00:53:38,151
所以我们提供一个参考实现方法

1020
00:53:38,519 --> 00:53:43,223
你们可以在

1021
00:53:43,557 --> 00:53:45,359
我们把代码放到了一个头文件中

1022
00:53:45,893 --> 00:53:48,362
它被全部注释掉了

1023
00:53:49,162 --> 00:53:50,197
请你们去看一下

1024
00:53:50,264 --> 00:53:55,636
它描述了如何纠正一个图像

1025
00:53:55,802 --> 00:53:57,871
取决于你传给它的表格是什么

1026
00:53:59,673 --> 00:54:02,943
如你所想 此表格的逆实现

1027
00:53:59,673 --> 00:54:02,943
如你所想 此表格的逆实现

1028
00:54:03,043 --> 00:54:08,815
描述的就是如何从扭曲的图像变回

1029
00:54:11,084 --> 00:54:13,787
用一个演示来给你们解释会容易得多

1030
00:54:18,725 --> 00:54:22,229
这是我们今天的第四个

1031
00:54:22,963 --> 00:54:24,331
它叫作Straighten Up

1032
00:54:25,065 --> 00:54:26,800
我打赌你们能猜出来

1033
00:54:28,635 --> 00:54:32,639
这个应用使用了

1034
00:54:32,940 --> 00:54:35,742
特别是镜头失真特征描述

1035
00:54:35,843 --> 00:54:37,978
来让图像直线化

1036
00:54:39,646 --> 00:54:43,650
我今天早上在外面的时候

1037
00:54:44,051 --> 00:54:46,920
你们应该可以看出来这些是双照片

1038
00:54:46,987 --> 00:54:49,523
因为它们周围有黑边

1039
00:54:49,923 --> 00:54:53,193
这张当然是用长焦镜头拍的

1040
00:54:54,461 --> 00:54:56,363
而这张是失真的照片

1041
00:54:57,431 --> 00:54:59,700
现在当我点击

1042
00:55:00,067 --> 00:55:03,237
你就会发现一些细微的变化

1043
00:55:06,907 --> 00:55:08,842
你绝对是可以发现的

1044
00:55:09,343 --> 00:55:13,146
通常长焦镜头具有更小的曲率

1045
00:55:13,213 --> 00:55:19,353
这样它们就在边缘上

1046
00:55:19,520 --> 00:55:22,322
我会放大一点

1047
00:55:22,656 --> 00:55:25,359
这是直线的

1048
00:55:26,093 --> 00:55:27,461
而这条是扭曲的

1049
00:55:27,995 --> 00:55:30,531
现在如果我看下广角的照片

1050
00:55:32,799 --> 00:55:34,835
虽然我们没有角落的信息

1051
00:55:37,004 --> 00:55:40,107
但是你们还是可以看到

1052
00:55:40,274 --> 00:55:42,376
失真还是要更明显些

1053
00:55:43,944 --> 00:55:46,547
按distorted

1054
00:55:46,780 --> 00:55:48,348
按distorted

1055
00:55:48,415 --> 00:55:51,818
你绝对可以发现 在边缘附近

1056
00:55:54,621 --> 00:55:55,622
按undistorted

1057
00:55:55,689 --> 00:55:57,090
按distorted

1058
00:55:59,092 --> 00:56:00,661
好了

1059
00:55:59,092 --> 00:56:00,661
好了

1060
00:56:05,999 --> 00:56:07,534
（总结）

1061
00:56:07,601 --> 00:56:08,635
我们该总结一下了

1062
00:56:09,870 --> 00:56:14,775
iPhone 7 Plus的双摄像头

1063
00:56:14,842 --> 00:56:15,742
它是一个？

1064
00:56:16,410 --> 00:56:18,245
视差 视差系统

1065
00:56:18,312 --> 00:56:20,414
如果你今天只记住一点

1066
00:56:20,480 --> 00:56:23,183
那么我希望你们能记住

1067
00:56:24,685 --> 00:56:29,289
另外 我们平台上对于深度的规范表示

1068
00:56:30,858 --> 00:56:35,395
我们还介绍了内在功能 外在功能

1069
00:56:35,462 --> 00:56:38,732
这些都是

1070
00:56:40,367 --> 00:56:43,170
我们介绍了

1071
00:56:43,237 --> 00:56:47,641
它所提供的流式深度

1072
00:56:49,576 --> 00:56:54,481
我们还介绍了如何使用

1073
00:56:54,848 --> 00:56:57,084
并且让这些照片带有深度信息

1074
00:56:59,152 --> 00:57:02,189
最后我们花了点时间介绍双摄像头

1075
00:56:59,152 --> 00:57:02,189
最后我们花了点时间介绍双摄像头

1076
00:57:02,256 --> 00:57:03,757
双照片送达

1077
00:57:04,057 --> 00:57:07,060
双照片会给单张照片生成广角

1078
00:57:07,127 --> 00:57:10,797
这样就能实现有趣的计算机视觉任务了

1079
00:57:12,633 --> 00:57:15,102
我们有三个示例代码

1080
00:57:15,169 --> 00:57:19,640
它们都跟本次演讲相关联了

1081
00:57:21,808 --> 00:57:24,811
有关它们的更多信息

1082
00:57:26,346 --> 00:57:29,249
请再稍等一会儿

1083
00:57:29,850 --> 00:57:33,353
紧接着这个演讲的

1084
00:57:33,420 --> 00:57:36,590
与会者都是对摄影感兴趣的开发者

1085
00:57:37,024 --> 00:57:38,091
你们都是其中一员啊

1086
00:57:39,126 --> 00:57:44,198
那你们可以来跟Apple

1087
00:57:45,432 --> 00:57:49,937
你可以问些问题

1088
00:57:50,437 --> 00:57:53,907
明天11点有一个本演讲的姊妹演讲

1089
00:57:54,174 --> 00:57:56,777
你可以学到如何读取并处理

1090
00:57:56,877 --> 00:57:58,812
图像文件中的深度数据

1091
00:57:58,912 --> 00:58:00,581
今天我们只是浅显地

1092
00:57:58,912 --> 00:58:00,581
今天我们只是浅显地

1093
00:58:00,647 --> 00:58:03,650
介绍了一下

1094
00:58:04,051 --> 00:58:05,886
明天会有很多的演示

1095
00:58:05,953 --> 00:58:08,088
我真的希望你们

1096
00:58:08,288 --> 00:58:10,824
如果你们能来的话

1097
00:58:12,292 --> 00:58:15,963
最后主讲一个关于

1098
00:58:16,029 --> 00:58:18,899
如何处理HEIF的演讲

1099
00:58:18,966 --> 00:58:20,200
我希望你们也能来

1100
00:58:21,368 --> 00:58:26,373
在那个演讲中 我会深入介绍一下

1101
00:58:27,941 --> 00:58:29,843
谢谢 希望你们享受大会剩余的演讲
