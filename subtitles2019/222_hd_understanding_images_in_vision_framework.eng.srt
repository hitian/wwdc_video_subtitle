1
00:00:00,506 --> 0:00:04,500
[ Music ]

2
00:00:07,516 --> 0:00:14,196
[ Applause ]

3
00:00:14,696 --> 0:00:15,336
&gt;&gt; Good morning.

4
00:00:15,676 --> 0:00:16,976
My name is Brittany Weinert, and

5
00:00:16,976 --> 0:00:17,946
I'm a software engineer on the

6
00:00:17,946 --> 0:00:18,866
Vision Framework Team.

7
00:00:19,566 --> 0:00:20,596
This year the Vision Team has a

8
00:00:20,596 --> 0:00:22,086
lot of exciting new updates that

9
00:00:22,086 --> 0:00:23,096
we think you're all going to

10
00:00:23,096 --> 0:00:23,316
love.

11
00:00:23,916 --> 0:00:24,846
Because we have so much new

12
00:00:24,846 --> 0:00:26,076
stuff to cover, we're going to

13
00:00:26,076 --> 0:00:27,106
dive right into the new

14
00:00:27,106 --> 0:00:27,516
features.

15
00:00:27,876 --> 0:00:29,256
If you're completely new to

16
00:00:29,256 --> 0:00:30,706
Vision, don't worry.

17
00:00:30,706 --> 0:00:31,466
You should still be able to

18
00:00:31,466 --> 0:00:33,096
follow along, and our hope is

19
00:00:33,096 --> 0:00:34,216
that the new capabilities that

20
00:00:34,216 --> 0:00:35,956
we introduce today will motivate

21
00:00:35,956 --> 0:00:37,366
you to learn about Vision and to

22
00:00:37,366 --> 0:00:39,606
use it in your apps.

23
00:00:39,836 --> 0:00:40,906
Today well be covering four

24
00:00:40,906 --> 0:00:44,206
completely new topics, saliency,

25
00:00:44,416 --> 0:00:46,066
image classification, Image

26
00:00:46,066 --> 0:00:47,556
Similarity, and face quality.

27
00:00:47,556 --> 0:00:49,456
We also have some technology

28
00:00:49,456 --> 0:00:50,796
upgrades for the Object Tracker

29
00:00:50,796 --> 0:00:52,826
and Face Landmarks as well as

30
00:00:52,826 --> 0:00:54,666
new detectors and improved Core

31
00:00:54,666 --> 0:00:55,196
ML support.

32
00:00:55,196 --> 0:00:58,486
Today, I'm going to be talking

33
00:00:58,486 --> 0:00:59,276
about saliency.

34
00:00:59,736 --> 0:01:00,836
Let's start with a definition.

35
00:00:59,736 --> 0:01:00,836
Let's start with a definition.

36
00:01:01,386 --> 0:01:02,806
I'm about to show you a photo,

37
00:01:02,806 --> 0:01:06,426
and I want you to pay attention

38
00:01:06,426 --> 0:01:07,586
to where your eyes are first

39
00:01:07,586 --> 0:01:08,036
drawn.

40
00:01:08,526 --> 0:01:13,046
When you first saw this photo of

41
00:01:13,046 --> 0:01:14,126
the three puffins sitting on a

42
00:01:14,126 --> 0:01:15,696
cliff, did you notice what stood

43
00:01:15,696 --> 0:01:16,366
out to you first?

44
00:01:17,326 --> 0:01:19,956
According to our models, most of

45
00:01:19,956 --> 0:01:21,226
you looked at the puffins faces

46
00:01:21,226 --> 0:01:21,476
first.

47
00:01:22,436 --> 0:01:23,556
This is saliency.

48
00:01:24,006 --> 0:01:26,366
There are two types of saliency,

49
00:01:26,766 --> 0:01:28,156
attention based and objectness

50
00:01:28,216 --> 0:01:28,566
based.

51
00:01:29,086 --> 0:01:30,976
The overlay that you saw on the

52
00:01:30,976 --> 0:01:32,566
puffin image just now called the

53
00:01:32,566 --> 0:01:34,226
heatmap was generated by

54
00:01:34,226 --> 0:01:35,686
attention based saliency.

55
00:01:36,146 --> 0:01:37,216
But before we get into more

56
00:01:37,216 --> 0:01:38,556
visual examples, I want to go

57
00:01:38,556 --> 0:01:39,696
over the basics of each

58
00:01:39,696 --> 0:01:40,246
algorithm.

59
00:01:41,776 --> 0:01:45,186
Attention based saliency is a

60
00:01:45,186 --> 0:01:47,786
human aspected saliency, and by

61
00:01:47,786 --> 0:01:49,436
this, I mean that the attention

62
00:01:49,436 --> 0:01:51,006
based saliency models were

63
00:01:51,006 --> 0:01:53,566
generated by where people looked

64
00:01:53,906 --> 0:01:55,006
when they were shown a series of

65
00:01:55,006 --> 0:01:55,446
images.

66
00:01:56,326 --> 0:01:57,646
This means that the heatmap

67
00:01:57,646 --> 0:02:00,026
reflects and highlights where

68
00:01:57,646 --> 0:02:00,026
reflects and highlights where

69
00:02:00,026 --> 0:02:00,966
people first look when they're

70
00:02:00,966 --> 0:02:01,646
shown an image.

71
00:02:02,676 --> 0:02:04,556
Objectness based saliency on the

72
00:02:04,556 --> 0:02:06,006
other hand was trained on

73
00:02:06,056 --> 0:02:07,976
subject segmentation in an image

74
00:02:08,515 --> 0:02:10,286
with the goal to highlight the

75
00:02:10,286 --> 0:02:12,086
foreground objects or the

76
00:02:12,086 --> 0:02:13,176
subjects of an image.

77
00:02:13,766 --> 0:02:15,996
So, in the heatmap, the subjects

78
00:02:15,996 --> 0:02:17,196
or foreground objects should be

79
00:02:17,196 --> 0:02:17,656
highlighted.

80
00:02:18,226 --> 0:02:19,916
Let's look at some examples now.

81
00:02:20,496 --> 0:02:23,566
So, here are the puffins from

82
00:02:23,566 --> 0:02:23,946
earlier.

83
00:02:24,636 --> 0:02:26,956
Here's the attention based

84
00:02:26,956 --> 0:02:28,436
heatmap overlaid on the image,

85
00:02:29,446 --> 0:02:30,916
and here's the objectness based

86
00:02:32,316 --> 0:02:33,046
heatmap.

87
00:02:33,046 --> 0:02:34,806
As I said, people tend to look

88
00:02:34,806 --> 0:02:36,056
at the puffins' faces first, so

89
00:02:36,346 --> 0:02:37,686
the area around the puffins'

90
00:02:37,686 --> 0:02:38,996
heads is very salient for the

91
00:02:38,996 --> 0:02:39,956
attention based heatmap.

92
00:02:40,896 --> 0:02:42,286
For objectness, we're just

93
00:02:42,286 --> 0:02:43,676
trying to pick up the subjects,

94
00:02:43,676 --> 0:02:45,246
and in this case, it's the three

95
00:02:45,246 --> 0:02:45,636
puffins.

96
00:02:45,786 --> 0:02:46,846
So, all the puffins are

97
00:02:46,846 --> 0:02:47,266
highlighted.

98
00:02:48,486 --> 0:02:50,206
Let's look at how saliency works

99
00:02:50,206 --> 0:02:51,206
with images of people.

100
00:02:51,746 --> 0:02:56,556
For attention based saliency,

101
00:02:57,046 --> 0:02:58,656
the areas around peoples' faces

102
00:02:58,656 --> 0:02:59,926
tend to be the most salient,

103
00:03:00,536 --> 0:03:02,146
unsurprisingly because we tend

104
00:03:02,146 --> 0:03:04,146
to look at people's faces first.

105
00:03:04,846 --> 0:03:06,616
For objectness based saliency,

106
00:03:06,616 --> 0:03:08,196
if the person is the subject of

107
00:03:08,196 --> 0:03:09,646
the image, the entire person

108
00:03:09,646 --> 0:03:10,386
should be highlighted.

109
00:03:12,596 --> 0:03:14,896
So, attention based saliency

110
00:03:14,896 --> 0:03:16,266
though is the more complicated

111
00:03:16,266 --> 0:03:17,846
of the two saliencies, I'd say,

112
00:03:18,436 --> 0:03:19,926
because it is determined by a

113
00:03:19,926 --> 0:03:21,386
number of very human factors.

114
00:03:22,226 --> 0:03:23,846
And the number, the main factors

115
00:03:23,846 --> 0:03:25,126
that determine attention based

116
00:03:25,126 --> 0:03:26,236
saliency and what's salient or

117
00:03:26,236 --> 0:03:28,426
not, is contrast, faces,

118
00:03:28,916 --> 0:03:31,046
subjects, horizons, and light.

119
00:03:32,416 --> 0:03:33,666
But interestingly enough, it can

120
00:03:33,666 --> 0:03:35,516
also be affected by perceived

121
00:03:35,516 --> 0:03:36,026
motion.

122
00:03:36,556 --> 0:03:39,816
In this example, the umbrella

123
00:03:39,816 --> 0:03:42,436
colors really pop, so the area

124
00:03:42,436 --> 0:03:43,716
around the umbrella is salient,

125
00:03:44,026 --> 0:03:45,636
but the road is also salient

126
00:03:46,006 --> 0:03:47,666
because our eyes try to track

127
00:03:47,666 --> 0:03:48,796
where the umbrella is headed.

128
00:03:50,706 --> 0:03:52,216
For objectness based saliency,

129
00:03:52,216 --> 0:03:53,406
we just pick up on the umbrella

130
00:03:55,016 --> 0:03:55,096
guy.

131
00:03:55,316 --> 0:03:56,876
So, I could do this all day and

132
00:03:56,876 --> 0:03:58,456
show you more examples, but

133
00:03:58,456 --> 0:03:59,456
honestly, the best way to

134
00:03:59,456 --> 0:04:00,966
understand saliency is to try it

135
00:03:59,456 --> 0:04:00,966
understand saliency is to try it

136
00:04:00,966 --> 0:04:01,766
out for yourself.

137
00:04:02,476 --> 0:04:03,776
I encourage everybody to

138
00:04:03,776 --> 0:04:05,726
download the Saliency app and

139
00:04:05,726 --> 0:04:06,656
try it on their own photo

140
00:04:06,656 --> 0:04:07,136
libraries.

141
00:04:07,796 --> 0:04:10,536
So, let's get into what's

142
00:04:10,536 --> 0:04:12,236
returned from the saliency

143
00:04:12,236 --> 0:04:15,726
request, mainly the heatmap.

144
00:04:16,055 --> 0:04:17,375
So, the images that I've been

145
00:04:17,375 --> 0:04:19,596
showing to you up until now, the

146
00:04:19,596 --> 0:04:22,356
heatmap has been scaled,

147
00:04:22,356 --> 0:04:25,996
overlaid, and colorized and put

148
00:04:25,996 --> 0:04:27,536
onto the image, but in

149
00:04:27,536 --> 0:04:29,656
actuality, the heatmap is a very

150
00:04:29,656 --> 0:04:32,206
small CV pixel buffer that's

151
00:04:32,206 --> 0:04:34,186
made up of Floats in the range

152
00:04:34,186 --> 0:04:37,246
of 0 to 1, 0 designating

153
00:04:37,526 --> 0:04:39,356
nonsalient and 1 being most

154
00:04:39,356 --> 0:04:39,746
salient.

155
00:04:40,256 --> 0:04:44,066
And there's extra code that

156
00:04:44,066 --> 0:04:45,206
you'd have to do to get the

157
00:04:45,206 --> 0:04:46,526
exact same effect like you see

158
00:04:46,526 --> 0:04:46,746
here.

159
00:04:47,526 --> 0:04:48,896
But let's go into how to

160
00:04:48,946 --> 0:04:50,926
formulate a request at the very

161
00:04:50,926 --> 0:04:51,716
basic level.

162
00:04:53,256 --> 0:04:56,686
Okay. So, first we start out

163
00:04:56,686 --> 0:04:58,776
with a VNImageRequestHandler to

164
00:04:58,776 --> 0:05:00,206
handle a single image.

165
00:04:58,776 --> 0:05:00,206
handle a single image.

166
00:05:01,266 --> 0:05:03,126
Next, you choose the algorithm

167
00:05:03,126 --> 0:05:04,346
that you want to run, in this

168
00:05:04,346 --> 0:05:06,516
case, AttentionBasedSaliency,

169
00:05:06,566 --> 0:05:09,256
and set the revision if you

170
00:05:09,256 --> 0:05:10,586
always want to be using the same

171
00:05:10,586 --> 0:05:10,976
algorithm.

172
00:05:12,976 --> 0:05:14,896
Next, you call perform request,

173
00:05:15,196 --> 0:05:17,466
like you usually would, and if

174
00:05:17,466 --> 0:05:19,476
it's successful, the results

175
00:05:19,476 --> 0:05:20,996
property on the request should

176
00:05:20,996 --> 0:05:22,456
be populated with a

177
00:05:22,456 --> 0:05:24,266
VNSaliencyImageObservation.

178
00:05:25,326 --> 0:05:28,396
To access the heatmap, you call

179
00:05:28,396 --> 0:05:30,086
the pixelBuffer property on the

180
00:05:30,086 --> 0:05:32,906
VNSaliencyImageObservation like

181
00:05:35,256 --> 0:05:35,356
so.

182
00:05:35,596 --> 0:05:37,516
If you wanted to do objectness

183
00:05:37,516 --> 0:05:39,316
based saliency, all you would

184
00:05:39,316 --> 0:05:41,096
have to do is change the request

185
00:05:41,246 --> 0:05:43,786
name and the revision to be

186
00:05:43,786 --> 0:05:44,246
objectness.

187
00:05:45,426 --> 0:05:46,676
So, for attention, it's

188
00:05:46,736 --> 0:05:48,206
VNGenerateAttentionBased

189
00:05:48,206 --> 0:05:50,106
SaliencyImageRequest and for

190
00:05:50,106 --> 0:05:50,836
objectness, it's

191
00:05:50,836 --> 0:05:52,386
VNGenerateObjectnessBased

192
00:05:52,386 --> 0:05:53,076
SaliencyRequest.

193
00:05:53,796 --> 0:05:56,856
So, let's get into another tool

194
00:05:56,856 --> 0:05:58,226
other than at heatmap, the

195
00:05:58,226 --> 0:05:58,856
bounding box.

196
00:05:59,476 --> 0:06:03,196
The bounding boxes encapsulate

197
00:05:59,476 --> 0:06:03,196
The bounding boxes encapsulate

198
00:06:03,196 --> 0:06:04,536
all the salient regions in an

199
00:06:04,536 --> 0:06:04,796
image.

200
00:06:05,156 --> 0:06:06,726
For attention based saliency,

201
00:06:06,726 --> 0:06:08,006
you should always have one

202
00:06:08,006 --> 0:06:09,866
bounding box, and for objectness

203
00:06:09,866 --> 0:06:11,116
based saliency, you can have up

204
00:06:11,116 --> 0:06:12,616
to three bounding boxes.

205
00:06:13,726 --> 0:06:15,826
The bounding boxes are in

206
00:06:15,826 --> 0:06:17,336
normalized coordinate space with

207
00:06:17,336 --> 0:06:18,546
respect to the image, the

208
00:06:18,546 --> 0:06:21,726
original image, and the lower

209
00:06:21,726 --> 0:06:23,316
left-hand corner is the origin

210
00:06:23,316 --> 0:06:25,066
point, much like bounding boxes

211
00:06:25,066 --> 0:06:26,826
returned by other algorithms in

212
00:06:26,826 --> 0:06:27,186
Vision.

213
00:06:27,956 --> 0:06:30,716
So, I wrote up a small method to

214
00:06:30,716 --> 0:06:32,266
show how to access the bounding

215
00:06:32,266 --> 0:06:33,146
boxes and use them.

216
00:06:33,916 --> 0:06:35,346
Here we have a

217
00:06:35,346 --> 0:06:37,386
VNSaliencyImageObservation, and

218
00:06:37,826 --> 0:06:40,156
all you have to do is access the

219
00:06:40,156 --> 0:06:41,706
salientObjects property on that

220
00:06:41,706 --> 0:06:43,876
observation, and you should get

221
00:06:44,036 --> 0:06:46,456
a list of bounding boxes, and

222
00:06:46,456 --> 0:06:48,296
you can access them like so.

223
00:06:48,296 --> 0:06:50,566
Okay. So, now that you know how

224
00:06:50,566 --> 0:06:53,136
to formulate a request and now

225
00:06:53,136 --> 0:06:54,906
that you know what saliency is,

226
00:06:55,686 --> 0:06:57,306
let's get into some of the use

227
00:06:57,306 --> 0:06:57,736
cases.

228
00:06:58,346 --> 0:07:02,606
First, for a bit of fun, you can

229
00:06:58,346 --> 0:07:02,606
First, for a bit of fun, you can

230
00:07:02,816 --> 0:07:05,816
use saliency as a graphical mask

231
00:07:06,076 --> 0:07:07,056
to edit your photos with.

232
00:07:07,056 --> 0:07:09,496
So, here you have the heatmaps.

233
00:07:10,216 --> 0:07:14,306
On the left-hand side, I've

234
00:07:14,306 --> 0:07:15,886
desaturated all the nonsalient

235
00:07:15,886 --> 0:07:17,716
regions, and on the right-hand

236
00:07:17,716 --> 0:07:19,416
side, I've added a Gaussian blur

237
00:07:19,416 --> 0:07:20,636
to all the nonsalient regions.

238
00:07:21,006 --> 0:07:22,336
It really makes the subjects

239
00:07:22,336 --> 0:07:22,576
pop.

240
00:07:25,536 --> 0:07:27,246
Another use case of saliency is

241
00:07:27,246 --> 0:07:28,396
you can enhance your photo

242
00:07:28,396 --> 0:07:29,156
viewing experience.

243
00:07:30,106 --> 0:07:31,666
So, let's say that you're at

244
00:07:31,666 --> 0:07:32,036
home.

245
00:07:32,036 --> 0:07:33,846
You're sitting on the couch, and

246
00:07:33,946 --> 0:07:35,236
either your TV or your computer

247
00:07:35,236 --> 0:07:38,216
has gone into standby mode, and

248
00:07:38,216 --> 0:07:39,066
it's going through your photo

249
00:07:39,066 --> 0:07:39,906
library.

250
00:07:40,336 --> 0:07:41,706
A lot of times, these

251
00:07:41,706 --> 0:07:44,446
photo-showing algorithms can be

252
00:07:44,446 --> 0:07:45,306
a little bit awkward.

253
00:07:45,306 --> 0:07:47,006
They zoom into seemingly random

254
00:07:47,006 --> 0:07:50,506
parts of the image, and it's not

255
00:07:50,506 --> 0:07:51,346
always what you expect.

256
00:07:52,086 --> 0:07:53,546
But with saliency, you always

257
00:07:53,546 --> 0:07:56,026
know where the subjects are, so

258
00:07:56,026 --> 0:07:57,036
you can get a more

259
00:07:57,136 --> 0:07:59,366
documentary-like effect like

260
00:08:02,476 --> 0:08:02,576
this.

261
00:08:02,766 --> 0:08:04,616
Finally, saliency works really

262
00:08:04,616 --> 0:08:05,656
great with other vision

263
00:08:05,656 --> 0:08:06,206
algorithms.

264
00:08:07,486 --> 0:08:09,246
Let's say we have an image, and

265
00:08:09,246 --> 0:08:11,236
we want to classify the objects

266
00:08:11,236 --> 0:08:11,806
in the image.

267
00:08:12,966 --> 0:08:14,426
We can run objectness based

268
00:08:14,426 --> 0:08:16,226
saliency to pick up on the

269
00:08:16,226 --> 0:08:19,196
objects in the image, crop the

270
00:08:19,196 --> 0:08:21,266
image to the bounding boxes

271
00:08:21,266 --> 0:08:22,456
returned by objectness based

272
00:08:22,456 --> 0:08:25,006
saliency, and run these crops

273
00:08:25,366 --> 0:08:26,816
through the algorithm through a

274
00:08:26,886 --> 0:08:28,736
image classification algorithm

275
00:08:29,106 --> 0:08:31,286
to find out what the objects

276
00:08:31,286 --> 0:08:31,536
are.

277
00:08:32,316 --> 0:08:33,996
So, not only do you know where

278
00:08:34,285 --> 0:08:35,576
they are in the image because of

279
00:08:35,576 --> 0:08:36,885
the bounding boxes, but it

280
00:08:36,885 --> 0:08:39,905
allows you the hone in on what

281
00:08:39,905 --> 0:08:42,035
the objects are by just picking

282
00:08:42,035 --> 0:08:43,366
out the crops that have those

283
00:08:43,366 --> 0:08:44,976
objects in it.

284
00:08:45,286 --> 0:08:46,756
Now, you can already classify

285
00:08:46,756 --> 0:08:48,996
things with Core ML, but this

286
00:08:48,996 --> 0:08:50,606
year, Vision has new image

287
00:08:50,606 --> 0:08:52,296
classification technique that

288
00:08:52,296 --> 0:08:54,126
Rohan will now present to you.

289
00:08:55,516 --> 0:09:02,736
[ Applause ]

290
00:08:55,516 --> 0:09:02,736
[ Applause ]

291
00:09:03,236 --> 0:09:04,696
&gt;&gt; Good morning.

292
00:09:05,136 --> 0:09:06,806
My name is Rohan Chandra, and

293
00:09:06,806 --> 0:09:08,176
I'm a researcher on the Vision

294
00:09:08,176 --> 0:09:08,476
Team.

295
00:09:09,266 --> 0:09:10,846
Today, I'm going to be talking

296
00:09:10,846 --> 0:09:12,106
about some of the new image

297
00:09:12,106 --> 0:09:13,476
classification requests we're

298
00:09:13,476 --> 0:09:14,926
introducing to the Vision API

299
00:09:15,126 --> 0:09:15,606
this year.

300
00:09:16,806 --> 0:09:18,856
Now, image classification as a

301
00:09:18,856 --> 0:09:20,626
task is fundamentally meant to

302
00:09:20,626 --> 0:09:22,096
answer the question, what are

303
00:09:22,096 --> 0:09:23,406
the objects that appear in my

304
00:09:23,406 --> 0:09:23,916
image.

305
00:09:25,066 --> 0:09:26,546
Many of you will already be

306
00:09:26,546 --> 0:09:27,386
familiar with image

307
00:09:27,386 --> 0:09:28,206
classification.

308
00:09:28,626 --> 0:09:30,146
You may have used Create ML or

309
00:09:30,146 --> 0:09:31,776
Core ML to train your own

310
00:09:31,776 --> 0:09:33,306
classification networks on your

311
00:09:33,356 --> 0:09:35,366
own data as we showed in the

312
00:09:35,366 --> 0:09:36,846
Vision with Core ML talk last

313
00:09:36,846 --> 0:09:37,046
year.

314
00:09:38,156 --> 0:09:39,306
Others of you may have been

315
00:09:39,356 --> 0:09:40,456
interested in image

316
00:09:40,456 --> 0:09:41,636
classification but felt you

317
00:09:41,636 --> 0:09:43,246
lacked the resources or the

318
00:09:43,246 --> 0:09:44,696
expertise to develop your own

319
00:09:44,696 --> 0:09:45,116
networks.

320
00:09:45,896 --> 0:09:48,716
In practice, developing a

321
00:09:48,716 --> 0:09:50,026
large-scale classification

322
00:09:50,026 --> 0:09:51,576
network from scratch can take

323
00:09:51,676 --> 0:09:53,076
millions of images to annotate,

324
00:09:53,596 --> 0:09:55,246
thousands of hours to train, and

325
00:09:55,246 --> 0:09:56,406
very specialized domain

326
00:09:56,406 --> 0:09:57,566
expertise to develop.

327
00:09:58,736 --> 0:10:00,606
We here at Apple have already

328
00:09:58,736 --> 0:10:00,606
We here at Apple have already

329
00:10:00,606 --> 0:10:02,296
gone through this process, and

330
00:10:02,336 --> 0:10:03,646
so we wanted to share our

331
00:10:03,646 --> 0:10:05,206
large-scale, on-device

332
00:10:05,256 --> 0:10:06,706
classification network with you

333
00:10:07,176 --> 0:10:08,206
so that you can leverage this

334
00:10:08,206 --> 0:10:09,936
technology without needing to

335
00:10:09,936 --> 0:10:11,396
invest a huge amount of time or

336
00:10:11,396 --> 0:10:12,816
resources into developing it

337
00:10:12,816 --> 0:10:13,306
yourself.

338
00:10:14,296 --> 0:10:16,116
We've also strived to put tools

339
00:10:16,116 --> 0:10:17,346
in the API to help you

340
00:10:17,346 --> 0:10:19,216
contextualize and understand the

341
00:10:19,216 --> 0:10:20,646
results in a way that makes

342
00:10:20,646 --> 0:10:21,806
sense for your application.

343
00:10:22,986 --> 0:10:24,116
Now, the network we're talking

344
00:10:24,116 --> 0:10:25,796
about exposing here is in fact

345
00:10:25,846 --> 0:10:27,326
the same network we ourselves

346
00:10:27,326 --> 0:10:28,686
use to power the photo search

347
00:10:28,686 --> 0:10:29,306
experience.

348
00:10:30,096 --> 0:10:31,136
This is a network we've

349
00:10:31,136 --> 0:10:32,646
developed specifically to run

350
00:10:32,646 --> 0:10:34,256
efficiently on device without

351
00:10:34,256 --> 0:10:35,516
requiring any service side

352
00:10:35,576 --> 0:10:36,236
processing.

353
00:10:36,956 --> 0:10:38,446
We've also developed it to

354
00:10:38,446 --> 0:10:39,746
identify over a thousand

355
00:10:39,746 --> 0:10:41,136
different categories of objects.

356
00:10:42,516 --> 0:10:44,396
Now, it's also important to note

357
00:10:44,516 --> 0:10:45,776
that this is a multi-label

358
00:10:45,776 --> 0:10:47,876
network capable of identifying

359
00:10:48,236 --> 0:10:49,936
multiple objects in a single

360
00:10:49,936 --> 0:10:51,986
image, in contrast to more

361
00:10:51,986 --> 0:10:53,976
typical mono-label networks that

362
00:10:53,976 --> 0:10:55,496
try to focus on identifying a

363
00:10:55,496 --> 0:10:57,666
single large central object in

364
00:10:57,666 --> 0:10:58,226
an image.

365
00:10:59,586 --> 0:11:01,076
Now, as I talk about this new

366
00:10:59,586 --> 0:11:01,076
Now, as I talk about this new

367
00:11:01,076 --> 0:11:03,226
classification API, I think one

368
00:11:03,226 --> 0:11:04,186
of the first questions that

369
00:11:04,186 --> 0:11:05,786
comes to mind is what are the

370
00:11:05,786 --> 0:11:06,856
objects it can actually

371
00:11:06,856 --> 0:11:07,426
identify?

372
00:11:08,336 --> 0:11:09,916
Well, the set of objects that a

373
00:11:09,916 --> 0:11:11,536
classifier can predict is known

374
00:11:11,536 --> 0:11:12,426
as the taxonomy.

375
00:11:13,436 --> 0:11:15,106
The taxonomy has a hierarchical

376
00:11:15,106 --> 0:11:16,456
structure with directional

377
00:11:16,456 --> 0:11:17,936
relationships between classes.

378
00:11:19,006 --> 0:11:20,606
These relationships are based

379
00:11:20,606 --> 0:11:22,226
upon shared semantic meaning.

380
00:11:22,916 --> 0:11:24,866
For instance, a class like dog

381
00:11:24,966 --> 0:11:26,436
might have children like Beagle,

382
00:11:26,616 --> 0:11:28,176
Poodle, Husky, and other

383
00:11:28,176 --> 0:11:29,156
sub-breeds of dogs.

384
00:11:30,256 --> 0:11:31,866
In this sense, a parent class

385
00:11:31,896 --> 0:11:33,226
tends to be more general while

386
00:11:33,226 --> 0:11:34,866
child classes are more specific

387
00:11:34,866 --> 0:11:36,106
instances of their parent.

388
00:11:37,046 --> 0:11:38,466
You can of course see the entire

389
00:11:38,466 --> 0:11:39,816
taxonomy using

390
00:11:40,006 --> 0:11:40,926
ImageRequest.known

391
00:11:40,926 --> 0:11:41,756
Classifications.

392
00:11:43,136 --> 0:11:44,416
Now, when we constructed the

393
00:11:44,416 --> 0:11:46,276
taxonomy, we had a few specific

394
00:11:46,276 --> 0:11:47,226
rules that we applied.

395
00:11:48,656 --> 0:11:49,816
The first is that the classes

396
00:11:49,856 --> 0:11:51,516
must be visually identifiable.

397
00:11:52,836 --> 0:11:54,456
That is, we avoid more abstract

398
00:11:54,456 --> 0:11:55,756
concepts like holiday or

399
00:11:55,756 --> 0:11:56,306
festival.

400
00:11:57,426 --> 0:11:59,196
We also avoid any classes that

401
00:11:59,196 --> 0:11:59,876
might be considered

402
00:11:59,876 --> 0:12:01,746
controversial or offensive as

403
00:11:59,876 --> 0:12:01,746
controversial or offensive as

404
00:12:01,746 --> 0:12:03,066
well as those to do with proper

405
00:12:03,066 --> 0:12:04,866
names, nouns, excuse me,

406
00:12:04,866 --> 0:12:06,256
adjectives, or basic shapes.

407
00:12:07,346 --> 0:12:09,266
Finally, we omit occupations,

408
00:12:09,526 --> 0:12:10,686
and this might seem odd at

409
00:12:10,686 --> 0:12:11,076
first.

410
00:12:11,716 --> 0:12:12,856
But consider the range of

411
00:12:12,856 --> 0:12:14,266
answers you'd get if we asked

412
00:12:14,266 --> 0:12:15,226
something like what does an

413
00:12:15,226 --> 0:12:16,216
engineer look like.

414
00:12:16,556 --> 0:12:18,496
There probably isn't a single

415
00:12:18,496 --> 0:12:19,546
concise description you could

416
00:12:19,546 --> 0:12:20,856
give that would apply to every

417
00:12:20,856 --> 0:12:22,416
engineer aside from sleep

418
00:12:22,416 --> 0:12:23,716
deprived and usually glued to a

419
00:12:23,716 --> 0:12:24,476
computer screen.

420
00:12:25,556 --> 0:12:26,646
Let's take a look at the code

421
00:12:26,646 --> 0:12:27,816
you need to use in order to

422
00:12:27,816 --> 0:12:28,706
classify an image.

423
00:12:28,706 --> 0:12:31,636
So, as usual, you form an

424
00:12:31,636 --> 0:12:32,896
ImageRequestHandler to your

425
00:12:32,896 --> 0:12:33,526
source image.

426
00:12:34,196 --> 0:12:35,026
You then perform the

427
00:12:35,026 --> 0:12:36,836
VNClassifyImageRequest and

428
00:12:36,836 --> 0:12:38,066
retrieve your observations.

429
00:12:38,746 --> 0:12:40,036
Now, in this case, you actually

430
00:12:40,036 --> 0:12:41,406
get an array of observations,

431
00:12:41,696 --> 0:12:42,976
one for every class in the

432
00:12:42,976 --> 0:12:44,976
taxonomy and its associated

433
00:12:44,976 --> 0:12:45,456
confidence.

434
00:12:46,366 --> 0:12:47,876
In a mono-label problem, you'd

435
00:12:47,876 --> 0:12:48,946
probably expect that these

436
00:12:48,946 --> 0:12:50,906
probabilities sum up to 1, but

437
00:12:50,906 --> 0:12:51,896
this is a multi-label

438
00:12:51,936 --> 0:12:53,876
classification network, and each

439
00:12:53,876 --> 0:12:55,376
prediction is an independent

440
00:12:55,376 --> 0:12:56,756
confidence associated with a

441
00:12:56,756 --> 0:12:57,416
particular class.

442
00:12:58,416 --> 0:13:00,096
As such, they won't sum to 1,

443
00:12:58,416 --> 0:13:00,096
As such, they won't sum to 1,

444
00:13:00,406 --> 0:13:01,466
and they're meant to be compared

445
00:13:01,466 --> 0:13:02,956
within the same class, not

446
00:13:02,956 --> 0:13:04,256
across different classes.

447
00:13:04,496 --> 0:13:06,196
So we can't simply take the max

448
00:13:06,196 --> 0:13:07,486
amongst them in order to

449
00:13:07,486 --> 0:13:08,816
determine our final prediction.

450
00:13:09,696 --> 0:13:11,166
You might be wondering then, how

451
00:13:11,166 --> 0:13:12,806
do I deal with so many classes

452
00:13:12,856 --> 0:13:13,756
and so many numbers.

453
00:13:14,516 --> 0:13:15,836
Well, there are a few key tools

454
00:13:15,836 --> 0:13:17,066
in the API that we've

455
00:13:17,066 --> 0:13:18,186
implemented to help you make

456
00:13:18,256 --> 0:13:19,086
sense of the result.

457
00:13:20,376 --> 0:13:22,026
Now, in order to talk about

458
00:13:22,086 --> 0:13:23,916
these tools in the API, we first

459
00:13:23,916 --> 0:13:25,586
need to define some basic terms.

460
00:13:26,266 --> 0:13:28,716
The first is when you get a

461
00:13:28,716 --> 0:13:30,576
confidence for a class, we

462
00:13:30,576 --> 0:13:31,936
typically compare that to a

463
00:13:31,936 --> 0:13:33,606
class-specific threshold, which

464
00:13:33,606 --> 0:13:34,866
we refer to as an operating

465
00:13:34,866 --> 0:13:35,186
point.

466
00:13:35,976 --> 0:13:37,976
If the class confidence is above

467
00:13:37,976 --> 0:13:39,466
the threshold, then we say that

468
00:13:39,466 --> 0:13:40,796
class is present in the image.

469
00:13:41,266 --> 0:13:42,996
If the class confidence is below

470
00:13:42,996 --> 0:13:44,516
the class threshold, then we say

471
00:13:44,516 --> 0:13:46,316
that object is not present in

472
00:13:46,316 --> 0:13:46,756
the image.

473
00:13:47,656 --> 0:13:49,316
In this sense, we want to pick

474
00:13:49,316 --> 0:13:50,866
thresholds such that objects

475
00:13:50,866 --> 0:13:52,346
with the target class typically

476
00:13:52,346 --> 0:13:53,456
have a confidence higher than

477
00:13:53,456 --> 0:13:55,646
the threshold, and images

478
00:13:55,736 --> 0:13:56,886
without the target class

479
00:13:57,026 --> 0:13:58,556
typically have a score lower

480
00:13:58,556 --> 0:13:59,406
than the threshold.

481
00:14:00,376 --> 0:14:02,216
However, machine learning is not

482
00:14:02,216 --> 0:14:03,866
infallible, and there will be

483
00:14:03,866 --> 0:14:05,636
instances where the network is

484
00:14:05,636 --> 0:14:07,136
unsure and the confidence is

485
00:14:07,136 --> 0:14:08,276
proportionally lower.

486
00:14:09,056 --> 0:14:10,496
This can happen when objects are

487
00:14:10,536 --> 0:14:12,156
office gated, appear in odd

488
00:14:12,156 --> 0:14:14,006
lighting or at odd angles, for

489
00:14:14,006 --> 0:14:14,506
instance.

490
00:14:15,176 --> 0:14:15,866
So how do we pick our

491
00:14:15,866 --> 0:14:16,426
thresholds?

492
00:14:17,596 --> 0:14:18,306
Well, there are essentially

493
00:14:18,436 --> 0:14:19,756
three different regimes we can

494
00:14:19,756 --> 0:14:20,926
be in depending on our choice of

495
00:14:20,926 --> 0:14:22,266
threshold that yield three

496
00:14:22,266 --> 0:14:23,326
different kinds of searches.

497
00:14:24,416 --> 0:14:25,426
To make this a little more

498
00:14:25,426 --> 0:14:26,816
concrete, let's say I have a

499
00:14:26,876 --> 0:14:28,696
library of images for which I've

500
00:14:28,806 --> 0:14:30,346
already performed classification

501
00:14:30,386 --> 0:14:31,246
and stored the results.

502
00:14:32,026 --> 0:14:33,296
Let's say in this particular

503
00:14:33,296 --> 0:14:35,016
case I'm looking for images of

504
00:14:35,016 --> 0:14:35,866
motorcycles.

505
00:14:36,726 --> 0:14:37,626
Now, I want to pick my

506
00:14:37,626 --> 0:14:39,466
thresholds such that images with

507
00:14:39,466 --> 0:14:40,776
motorcycles typically have a

508
00:14:40,776 --> 0:14:41,826
confidence higher than this

509
00:14:41,876 --> 0:14:43,906
threshold and images without

510
00:14:43,906 --> 0:14:45,306
motorcycles typically have a

511
00:14:45,306 --> 0:14:46,946
score lower than this threshold.

512
00:14:47,596 --> 0:14:49,186
So, what happens if I just pick

513
00:14:49,186 --> 0:14:50,006
a low threshold.

514
00:14:50,826 --> 0:14:52,286
As you can see behind me, when I

515
00:14:52,286 --> 0:14:54,246
apply this low threshold, I do

516
00:14:54,246 --> 0:14:55,356
in fact get my motorcycle

517
00:14:55,356 --> 0:14:56,926
images, but I'm also getting

518
00:14:56,926 --> 0:14:58,696
these images of mopeds in the

519
00:14:58,696 --> 0:14:59,246
bottom right.

520
00:14:59,246 --> 0:15:01,106
And if my users are motorcycle

521
00:14:59,246 --> 0:15:01,106
And if my users are motorcycle

522
00:15:01,106 --> 0:15:02,106
enthusiasts, they might be a

523
00:15:02,106 --> 0:15:03,386
little annoyed with that result.

524
00:15:04,516 --> 0:15:06,186
When we talk about a search that

525
00:15:06,186 --> 0:15:07,806
tries to maximize the percentage

526
00:15:07,806 --> 0:15:09,346
of the target class retrieved

527
00:15:09,346 --> 0:15:11,156
amongst the entire library, and

528
00:15:11,246 --> 0:15:12,656
isn't as concerned with these

529
00:15:12,656 --> 0:15:14,196
missed predictions where we say

530
00:15:14,196 --> 0:15:15,596
the motorcycle is present when

531
00:15:15,596 --> 0:15:16,266
it actually isn't.

532
00:15:16,656 --> 0:15:18,036
We are typically talking about a

533
00:15:18,036 --> 0:15:19,076
high recall search.

534
00:15:20,156 --> 0:15:22,236
Now, I could maximize recall by

535
00:15:22,236 --> 0:15:23,806
simply returning as many images

536
00:15:23,806 --> 0:15:25,086
as possible, but I would get a

537
00:15:25,176 --> 0:15:26,526
huge number of these false

538
00:15:26,526 --> 0:15:27,516
predictions where I say my

539
00:15:27,516 --> 0:15:28,806
target class is present when it

540
00:15:28,806 --> 0:15:30,436
actually isn't, and so we need

541
00:15:30,436 --> 0:15:31,666
to find a more balanced point of

542
00:15:31,666 --> 0:15:32,676
recall to operate at.

543
00:15:33,586 --> 0:15:34,956
Let's take a look at how I need

544
00:15:34,956 --> 0:15:36,596
to change my code in order to

545
00:15:36,596 --> 0:15:38,256
perform this high recall search.

546
00:15:39,716 --> 0:15:41,566
So, here I have the same code

547
00:15:41,566 --> 0:15:43,466
snippet as before, but this time

548
00:15:43,606 --> 0:15:45,516
I'm performing a filtering with

549
00:15:45,516 --> 0:15:47,206
hasMinimumPrecision and a

550
00:15:47,206 --> 0:15:48,666
specific recall value.

551
00:15:49,496 --> 0:15:51,526
For each observation in my array

552
00:15:51,526 --> 0:15:53,646
of observations, the filter only

553
00:15:53,646 --> 0:15:55,166
retains it if the confidence

554
00:15:55,166 --> 0:15:56,336
associated with the class

555
00:15:56,656 --> 0:15:57,946
achieves the level of recall

556
00:15:57,946 --> 0:15:58,896
that I specified.

557
00:15:59,646 --> 0:16:01,526
Now, the actual operating point

558
00:15:59,646 --> 0:16:01,526
Now, the actual operating point

559
00:16:01,526 --> 0:16:02,856
needed to determine this is

560
00:16:02,856 --> 0:16:04,196
going to be different for every

561
00:16:04,196 --> 0:16:06,106
class, and it's something we've

562
00:16:06,106 --> 0:16:07,756
determined based on our internal

563
00:16:07,756 --> 0:16:08,986
tests of how the network

564
00:16:08,986 --> 0:16:10,476
performs on every class in the

565
00:16:10,476 --> 0:16:11,116
taxonomy.

566
00:16:12,086 --> 0:16:13,736
However, the filter handles this

567
00:16:13,736 --> 0:16:14,696
for you automatically.

568
00:16:15,026 --> 0:16:16,756
All you need to do is specify

569
00:16:16,756 --> 0:16:18,016
the level of recall you want to

570
00:16:18,016 --> 0:16:18,696
operate at.

571
00:16:19,736 --> 0:16:20,686
So, we talked about a high

572
00:16:20,686 --> 0:16:22,426
recall search here, but what if

573
00:16:22,426 --> 0:16:24,196
I have an application that can't

574
00:16:24,196 --> 0:16:25,646
tolerate these false predictions

575
00:16:25,646 --> 0:16:26,786
where I'm saying motorcycles are

576
00:16:26,786 --> 0:16:27,696
present when they're not.

577
00:16:28,106 --> 0:16:29,746
That is, I want to be absolutely

578
00:16:29,746 --> 0:16:31,456
sure that the images I retrieve

579
00:16:31,626 --> 0:16:32,566
actually do contain a

580
00:16:32,566 --> 0:16:33,246
motorcycle.

581
00:16:34,026 --> 0:16:35,326
Well, let's come back to our

582
00:16:35,326 --> 0:16:36,956
library of images then and see

583
00:16:36,956 --> 0:16:38,236
what would happen if we applied

584
00:16:38,266 --> 0:16:39,186
the higher threshold.

585
00:16:39,956 --> 0:16:41,406
As you can see behind me, when I

586
00:16:41,406 --> 0:16:43,186
apply my high threshold, I do in

587
00:16:43,186 --> 0:16:44,776
fact only get motorcycle images,

588
00:16:45,106 --> 0:16:46,506
but I get far fewer images

589
00:16:46,506 --> 0:16:47,166
overall.

590
00:16:48,536 --> 0:16:50,176
When we talk about a search that

591
00:16:50,176 --> 0:16:51,816
tries to maximize the percentage

592
00:16:51,876 --> 0:16:53,506
of the target class amongst the

593
00:16:53,506 --> 0:16:55,586
retrieved images and isn't as

594
00:16:55,586 --> 0:16:57,146
concerned with overlooking some

595
00:16:57,146 --> 0:16:58,486
of the more ambiguous images

596
00:16:58,536 --> 0:16:59,696
that actually do contain the

597
00:16:59,696 --> 0:17:01,216
target class, we are typically

598
00:16:59,696 --> 0:17:01,216
target class, we are typically

599
00:17:01,216 --> 0:17:03,296
talking about a high precision

600
00:17:03,296 --> 0:17:03,736
search.

601
00:17:04,445 --> 0:17:06,296
Again, like with high recall, we

602
00:17:06,296 --> 0:17:07,316
need to find a more balanced

603
00:17:07,316 --> 0:17:08,656
operating point where I have an

604
00:17:08,656 --> 0:17:10,435
acceptable likelihood about my

605
00:17:10,435 --> 0:17:11,486
target class appearing in my

606
00:17:11,486 --> 0:17:12,886
results, but I'm not getting too

607
00:17:12,986 --> 0:17:13,756
few images.

608
00:17:14,816 --> 0:17:16,116
So, let's take a look at how I

609
00:17:16,116 --> 0:17:17,816
need to modify my code in order

610
00:17:17,816 --> 0:17:19,175
to perform this high precision

611
00:17:19,175 --> 0:17:19,586
search.

612
00:17:20,175 --> 0:17:22,546
So here's the same code snippet,

613
00:17:22,715 --> 0:17:24,215
but this time my filtering is

614
00:17:24,215 --> 0:17:26,366
done with hasMinimumRecall and a

615
00:17:26,366 --> 0:17:27,935
precision value I've specified.

616
00:17:28,806 --> 0:17:30,636
Again, I only retain the

617
00:17:30,636 --> 0:17:32,186
observation if the confidence

618
00:17:32,186 --> 0:17:33,786
associated with it achieves the

619
00:17:33,786 --> 0:17:34,736
level of precision that I

620
00:17:34,736 --> 0:17:35,406
specified.

621
00:17:36,166 --> 0:17:37,546
The actual threshold needed for

622
00:17:37,546 --> 0:17:38,806
this is going to be different

623
00:17:38,806 --> 0:17:40,286
for every class, but the filter

624
00:17:40,286 --> 0:17:41,646
handles that for me

625
00:17:41,646 --> 0:17:42,256
automatically.

626
00:17:42,626 --> 0:17:44,056
All I need to do is tell it the

627
00:17:44,056 --> 0:17:45,026
level of precision I want to

628
00:17:45,026 --> 0:17:45,616
operate at.

629
00:17:46,936 --> 0:17:48,576
So we've talked about two

630
00:17:48,576 --> 0:17:49,846
different extremes here, one of

631
00:17:49,906 --> 0:17:51,236
high recall and one of high

632
00:17:51,236 --> 0:17:53,516
precision, but in practice, it

633
00:17:53,516 --> 0:17:55,216
can be better to find a balanced

634
00:17:55,416 --> 0:17:57,076
tradeoff between the two.

635
00:17:58,096 --> 0:17:59,496
So, let's see how we can go

636
00:17:59,496 --> 0:18:00,896
about doing that, and in order

637
00:17:59,496 --> 0:18:00,896
about doing that, and in order

638
00:18:00,896 --> 0:18:01,966
to understand what's happening,

639
00:18:02,246 --> 0:18:03,266
I first need to introduce

640
00:18:03,396 --> 0:18:04,836
something known as the precision

641
00:18:04,836 --> 0:18:05,646
and recall curve.

642
00:18:06,346 --> 0:18:08,976
So, in practice, there is a

643
00:18:08,976 --> 0:18:10,406
tradeoff to be made where

644
00:18:10,406 --> 0:18:11,986
increasing one of precision and

645
00:18:11,986 --> 0:18:13,846
recall can lead to a decrease in

646
00:18:13,846 --> 0:18:14,196
the other.

647
00:18:14,646 --> 0:18:16,376
I can represent this tradeoff as

648
00:18:16,376 --> 0:18:17,956
a graph, where for each

649
00:18:18,026 --> 0:18:19,586
operating point I can compute

650
00:18:19,586 --> 0:18:20,936
the corresponding precision and

651
00:18:20,936 --> 0:18:21,336
recall.

652
00:18:22,016 --> 0:18:23,476
For instance, at the operating

653
00:18:23,506 --> 0:18:25,156
point at where I achieve a

654
00:18:25,156 --> 0:18:27,116
recall of 0.7, I find that I get

655
00:18:27,116 --> 0:18:28,456
a corresponding precision of

656
00:18:28,456 --> 0:18:29,436
0.74.

657
00:18:29,436 --> 0:18:31,886
I can compute this for a

658
00:18:31,886 --> 0:18:33,486
multitude of operating points in

659
00:18:33,486 --> 0:18:35,016
order to form my full curve.

660
00:18:36,246 --> 0:18:37,956
As I said before, I want to find

661
00:18:37,956 --> 0:18:39,846
a balance point along this curve

662
00:18:40,056 --> 0:18:41,056
that achieves the level of

663
00:18:41,056 --> 0:18:42,286
recall and precision that makes

664
00:18:42,286 --> 0:18:43,536
sense for my application.

665
00:18:44,486 --> 0:18:45,726
So let's see how I need to

666
00:18:45,756 --> 0:18:47,366
change my code in order to

667
00:18:47,366 --> 0:18:48,636
accomplish it and how the

668
00:18:48,636 --> 0:18:50,376
precision and recall curve plays

669
00:18:51,106 --> 0:18:52,146
into that.

670
00:18:52,286 --> 0:18:54,276
So here I have a filtering with

671
00:18:54,276 --> 0:18:55,896
hasMinimumPrecision where I'm

672
00:18:55,896 --> 0:18:57,546
specifying the minimum precision

673
00:18:57,546 --> 0:18:57,976
and a recall value.

674
00:18:58,416 --> 0:19:00,746
When I specify a

675
00:18:58,416 --> 0:19:00,746
When I specify a

676
00:19:00,746 --> 0:19:02,486
MinimumPrecision, I'm actually

677
00:19:02,486 --> 0:19:04,286
selecting an area along the

678
00:19:04,286 --> 0:19:05,566
graph that I want to operate

679
00:19:05,566 --> 0:19:05,976
within.

680
00:19:06,866 --> 0:19:08,236
When I select a recall point

681
00:19:08,286 --> 0:19:10,226
with forRecall, I'm choosing a

682
00:19:10,296 --> 0:19:11,896
point along the curve that will

683
00:19:11,896 --> 0:19:13,006
be my operating point.

684
00:19:13,876 --> 0:19:15,516
Now, if the operating point is

685
00:19:15,576 --> 0:19:16,666
in the valid region that I

686
00:19:16,666 --> 0:19:18,166
selected, then that is the

687
00:19:18,166 --> 0:19:19,616
threshold that the filter will

688
00:19:19,616 --> 0:19:20,916
apply when looking at that

689
00:19:20,916 --> 0:19:21,626
particular class.

690
00:19:22,586 --> 0:19:24,076
If the operating point is not in

691
00:19:24,076 --> 0:19:25,636
the valid region, then there is

692
00:19:25,636 --> 0:19:27,046
no operating point that meets

693
00:19:27,046 --> 0:19:28,536
the constraints I stated, and

694
00:19:28,536 --> 0:19:29,606
the class will always be

695
00:19:29,606 --> 0:19:30,816
filtered out of my results.

696
00:19:31,856 --> 0:19:33,556
In this sense, all you need to

697
00:19:33,556 --> 0:19:35,166
do is provide the level of

698
00:19:35,166 --> 0:19:36,476
precision and recall that you

699
00:19:36,476 --> 0:19:37,686
want to operate at, and the

700
00:19:37,686 --> 0:19:38,626
filter will determine the

701
00:19:38,626 --> 0:19:39,916
necessary thresholds for you

702
00:19:40,076 --> 0:19:40,796
automatically.

703
00:19:41,386 --> 0:19:44,906
So, to summarize, the

704
00:19:44,906 --> 0:19:46,226
observation I get back when

705
00:19:46,226 --> 0:19:47,656
performing image classification

706
00:19:47,986 --> 0:19:49,106
is actually an array of

707
00:19:49,106 --> 0:19:50,656
observations, one for every

708
00:19:50,656 --> 0:19:51,736
class in the taxonomy.

709
00:19:52,876 --> 0:19:54,106
Because this is a multi-label

710
00:19:54,206 --> 0:19:55,566
problem, the confidences will

711
00:19:55,566 --> 0:19:56,456
not sum to 1.

712
00:19:57,166 --> 0:19:58,716
Instead, we have independent

713
00:19:58,716 --> 0:20:00,376
confidence values, one for every

714
00:19:58,716 --> 0:20:00,376
confidence values, one for every

715
00:20:00,376 --> 0:20:02,616
class between 0 to 1, and we

716
00:20:02,616 --> 0:20:04,096
need to understand precision and

717
00:20:04,096 --> 0:20:05,656
recall and how they apply to our

718
00:20:05,656 --> 0:20:07,676
specific use case in order to

719
00:20:07,676 --> 0:20:08,606
apply a filtering with

720
00:20:08,606 --> 0:20:10,226
hasMinimumPrecision or

721
00:20:10,226 --> 0:20:11,856
hasMinimumRecall that makes

722
00:20:11,856 --> 0:20:13,166
sense for our application.

723
00:20:13,606 --> 0:20:15,846
So, that concludes the portion

724
00:20:16,146 --> 0:20:17,496
on image classification.

725
00:20:17,986 --> 0:20:19,416
I'd like to switch gears and

726
00:20:19,416 --> 0:20:20,676
talk about a related topic,

727
00:20:21,226 --> 0:20:22,866
Image -- excuse me.

728
00:20:23,176 --> 0:20:23,976
Image Similarity.

729
00:20:26,216 --> 0:20:27,216
When we talk about Image

730
00:20:27,216 --> 0:20:29,076
Similarity, what we really mean

731
00:20:29,076 --> 0:20:30,406
is a method to describe the

732
00:20:30,406 --> 0:20:32,476
content of an image and another

733
00:20:32,476 --> 0:20:34,286
method to compare those

734
00:20:34,316 --> 0:20:34,956
descriptions.

735
00:20:36,116 --> 0:20:38,106
The most basic way in which I

736
00:20:38,106 --> 0:20:39,676
can describe the contents of an

737
00:20:39,676 --> 0:20:41,436
image is using the source pixels

738
00:20:41,436 --> 0:20:41,986
themselves.

739
00:20:43,536 --> 0:20:45,286
That is, I can search for other

740
00:20:45,286 --> 0:20:47,076
images that have close to or

741
00:20:47,076 --> 0:20:49,196
exactly the same pixel values

742
00:20:49,256 --> 0:20:50,076
and retrieve them.

743
00:20:51,146 --> 0:20:52,136
If I did a search in this

744
00:20:52,136 --> 0:20:54,046
fashion, however, it's extremely

745
00:20:54,046 --> 0:20:55,886
fragile, and it's easily fooled

746
00:20:55,986 --> 0:20:57,646
by small changes like rotations

747
00:20:57,916 --> 0:20:59,466
or lighting augmentations that

748
00:20:59,466 --> 0:21:00,656
drastically change the pixel

749
00:20:59,466 --> 0:21:00,656
drastically change the pixel

750
00:21:00,656 --> 0:21:02,256
values but not the semantic

751
00:21:02,256 --> 0:21:03,376
content in the image.

752
00:21:04,266 --> 0:21:05,536
What I really want is a more

753
00:21:05,536 --> 0:21:07,246
high-level description of what

754
00:21:07,246 --> 0:21:08,836
the content of the image is,

755
00:21:08,836 --> 0:21:09,926
perhaps something like natural

756
00:21:09,926 --> 0:21:10,426
language.

757
00:21:10,936 --> 0:21:13,126
I could make use of the image

758
00:21:13,126 --> 0:21:14,696
classification API I was

759
00:21:14,696 --> 0:21:16,426
describing previously in order

760
00:21:16,426 --> 0:21:17,636
to extract a set of words that

761
00:21:17,636 --> 0:21:18,626
describe my image.

762
00:21:19,446 --> 0:21:20,936
I could then retrieve other

763
00:21:20,936 --> 0:21:22,426
images with a similar set of

764
00:21:22,426 --> 0:21:23,316
classifications.

765
00:21:23,756 --> 0:21:25,106
I might even combine this with

766
00:21:25,106 --> 0:21:26,256
something like word vectors to

767
00:21:26,326 --> 0:21:27,716
account for similar but not

768
00:21:27,716 --> 0:21:29,386
exactly matching words like cat

769
00:21:29,386 --> 0:21:30,356
and kitten.

770
00:21:30,696 --> 0:21:31,896
Well, if I performed a search

771
00:21:31,896 --> 0:21:33,556
like this, I might get similar

772
00:21:33,556 --> 0:21:35,046
objects in a very general sense,

773
00:21:35,446 --> 0:21:36,406
but the way in which those

774
00:21:36,406 --> 0:21:37,556
objects appear and the

775
00:21:37,556 --> 0:21:39,066
relationships between them could

776
00:21:39,066 --> 0:21:40,026
be very different.

777
00:21:40,966 --> 0:21:43,036
As well, I would be limited by

778
00:21:43,036 --> 0:21:44,586
the taxonomy of my classifier.

779
00:21:45,386 --> 0:21:46,736
That is, any object that

780
00:21:46,736 --> 0:21:48,426
appeared in my image that wasn't

781
00:21:48,426 --> 0:21:49,756
in my classification networks

782
00:21:49,756 --> 0:21:51,686
taxonomy couldn't be expressed

783
00:21:51,786 --> 0:21:52,876
in a search like this.

784
00:21:54,216 --> 0:21:55,486
What I really want is a

785
00:21:55,486 --> 0:21:56,966
high-level description of the

786
00:21:56,996 --> 0:21:58,176
objects that appear in the image

787
00:21:58,386 --> 0:21:59,856
that isn't fixated on the exact

788
00:21:59,856 --> 0:22:01,096
pixel values but still cares

789
00:21:59,856 --> 0:22:01,096
pixel values but still cares

790
00:22:01,096 --> 0:22:01,556
about them.

791
00:22:02,256 --> 0:22:04,006
I also want this to apply to any

792
00:22:04,006 --> 0:22:05,896
natural image and not just those

793
00:22:05,896 --> 0:22:07,266
within a specific taxonomy.

794
00:22:07,746 --> 0:22:09,946
As it turns out, this kind of

795
00:22:09,946 --> 0:22:11,756
representation learning is

796
00:22:11,756 --> 0:22:12,676
something that's naturally

797
00:22:12,676 --> 0:22:14,016
engendered in our classification

798
00:22:14,016 --> 0:22:15,296
network as part of its training

799
00:22:15,296 --> 0:22:15,786
process.

800
00:22:16,786 --> 0:22:18,736
The upper layers of the network

801
00:22:18,976 --> 0:22:20,356
contain all of the salient

802
00:22:20,356 --> 0:22:22,166
information necessary to perform

803
00:22:22,166 --> 0:22:23,686
classification while discarding

804
00:22:23,756 --> 0:22:25,466
any redundant or unnecessary

805
00:22:25,466 --> 0:22:26,726
information that doesn't aid it

806
00:22:26,726 --> 0:22:28,336
in that task.

807
00:22:28,386 --> 0:22:29,596
We can make use of these upper

808
00:22:29,596 --> 0:22:31,136
layers then to act as our

809
00:22:31,136 --> 0:22:32,456
feature descriptor, and it's

810
00:22:32,456 --> 0:22:33,516
something we refer to as the

811
00:22:33,516 --> 0:22:34,156
feature print.

812
00:22:35,196 --> 0:22:36,706
Now, the feature print is a

813
00:22:36,706 --> 0:22:37,946
vector that describes the

814
00:22:37,946 --> 0:22:39,666
content of the image that isn't

815
00:22:39,666 --> 0:22:40,696
constrained to a particular

816
00:22:40,696 --> 0:22:42,086
taxonomy, even the one that the

817
00:22:42,086 --> 0:22:43,146
classification network was

818
00:22:43,146 --> 0:22:43,636
trained on.

819
00:22:43,896 --> 0:22:45,106
It simply leverages what the

820
00:22:45,106 --> 0:22:46,486
network has learned about images

821
00:22:46,486 --> 0:22:47,736
during its training process.

822
00:22:48,816 --> 0:22:50,296
If we look at these pairs of

823
00:22:50,296 --> 0:22:51,696
images, we can compare how

824
00:22:51,696 --> 0:22:52,786
similar their feature prints

825
00:22:52,786 --> 0:22:54,236
are, and the smaller the value

826
00:22:54,236 --> 0:22:55,486
is, the more similar the two

827
00:22:55,486 --> 0:22:57,296
images are in a semantic sense.

828
00:22:58,046 --> 0:22:59,316
We can see that even though the

829
00:22:59,316 --> 0:23:00,556
two images of the cats are

830
00:22:59,316 --> 0:23:00,556
two images of the cats are

831
00:23:00,556 --> 0:23:02,026
visually dissimilar, they have a

832
00:23:02,026 --> 0:23:03,506
much more similar feature print

833
00:23:03,746 --> 0:23:05,526
than the visually similar pairs

834
00:23:05,596 --> 0:23:06,636
of different animals.

835
00:23:07,136 --> 0:23:09,416
To make this a little more

836
00:23:09,416 --> 0:23:10,646
concrete, let's go through a

837
00:23:10,646 --> 0:23:11,686
specific example.

838
00:23:12,396 --> 0:23:13,476
Let's say I have the source

839
00:23:13,476 --> 0:23:15,186
image on screen, and I want to

840
00:23:15,186 --> 0:23:16,706
find other semantically similar

841
00:23:16,706 --> 0:23:17,476
images to it.

842
00:23:18,336 --> 0:23:19,656
I'm going to take a library of

843
00:23:19,656 --> 0:23:21,196
images and compute the feature

844
00:23:21,196 --> 0:23:22,896
print for each image and then

845
00:23:22,896 --> 0:23:24,476
retrieve those images with the

846
00:23:24,476 --> 0:23:26,296
most similar feature print to my

847
00:23:26,296 --> 0:23:26,986
source image.

848
00:23:27,746 --> 0:23:29,136
When I do it for this image of

849
00:23:29,136 --> 0:23:29,926
the gentleman in the coffee

850
00:23:29,926 --> 0:23:31,876
shop, I find I get other images

851
00:23:31,926 --> 0:23:33,176
of people in coffee shop and

852
00:23:33,176 --> 0:23:33,946
restaurant settings.

853
00:23:34,886 --> 0:23:36,266
If I focus on a crop of the

854
00:23:36,266 --> 0:23:38,246
newspaper, however, I get other

855
00:23:38,246 --> 0:23:39,586
images of newspapers.

856
00:23:40,246 --> 0:23:42,156
And if I focus on the teapot, I

857
00:23:42,236 --> 0:23:43,906
get other images of teapots.

858
00:23:45,326 --> 0:23:46,616
I'd like to now invite the

859
00:23:46,616 --> 0:23:48,616
Vision Team onstage to help me

860
00:23:48,616 --> 0:23:50,116
with a quick demonstration to

861
00:23:50,116 --> 0:23:51,546
expand a little more on how

862
00:23:51,546 --> 0:23:52,646
Image Similarity works.

863
00:23:54,516 --> 0:23:58,166
[ Applause ]

864
00:23:58,666 --> 0:23:59,466
&gt;&gt; Hello everyone.

865
00:23:59,886 --> 0:24:00,846
My name is Brett, and we have a

866
00:23:59,886 --> 0:24:00,846
My name is Brett, and we have a

867
00:24:00,846 --> 0:24:02,046
really fun way to demonstrate

868
00:24:02,046 --> 0:24:03,546
Image Similarity for you today.

869
00:24:03,546 --> 0:24:05,286
We have very creatively called

870
00:24:05,286 --> 0:24:06,846
it the Image Similarity game.

871
00:24:07,876 --> 0:24:09,086
And here is how you play.

872
00:24:09,676 --> 0:24:11,066
You draw something on a piece of

873
00:24:11,066 --> 0:24:14,026
paper, then ask a few friends to

874
00:24:14,026 --> 0:24:15,166
re-create your original as close

875
00:24:15,166 --> 0:24:15,726
as possible.

876
00:24:16,296 --> 0:24:17,596
So I will start by drawing the

877
00:24:17,596 --> 0:24:17,976
original.

878
00:24:30,096 --> 0:24:33,496
Okay. Tap continue to scan it in

879
00:24:33,496 --> 0:24:33,976
as my original.

880
00:24:42,046 --> 0:24:42,976
And then save.

881
00:24:44,526 --> 0:24:46,456
Now, my team will act as

882
00:24:46,456 --> 0:24:47,886
contestants, and they will draw

883
00:24:48,156 --> 0:24:48,976
this as best as they can.

884
00:24:54,266 --> 0:24:55,206
Now while they're drawing, I

885
00:24:55,206 --> 0:24:57,316
should tell you that this sample

886
00:24:57,316 --> 0:24:58,476
app is currently available to

887
00:24:58,536 --> 0:25:00,576
you now on the developer

888
00:24:58,536 --> 0:25:00,576
you now on the developer

889
00:25:00,576 --> 0:25:04,356
documentation website as sample

890
00:25:04,356 --> 0:25:05,986
code, and also, we are using the

891
00:25:05,986 --> 0:25:07,586
Vision kit document scanner to

892
00:25:07,586 --> 0:25:08,886
scan in our drawings, and you

893
00:25:08,886 --> 0:25:09,966
can learn more about that at our

894
00:25:09,966 --> 0:25:11,226
text recognition session.

895
00:25:12,706 --> 0:25:15,076
Let's them give a few more

896
00:25:16,636 --> 0:25:16,886
seconds.

897
00:25:16,966 --> 0:25:19,896
Five, four, three, okay, I guess

898
00:25:19,926 --> 0:25:20,186
they're done.

899
00:25:20,266 --> 0:25:22,186
Okay. Let's bring them up and

900
00:25:22,186 --> 0:25:24,266
start scanning them in.

901
00:25:24,996 --> 0:25:26,256
Contestant number one.

902
00:25:31,046 --> 0:25:31,576
Pretty good [applause].

903
00:25:32,756 --> 0:25:33,406
That might be a winner.

904
00:25:33,406 --> 0:25:35,256
Let's see contestant number two.

905
00:25:38,296 --> 0:25:39,786
Still pretty good.

906
00:25:40,096 --> 0:25:40,506
Nicely done.

907
00:25:41,256 --> 0:25:43,256
[ Applause ]

908
00:25:43,496 --> 0:25:44,706
Contestant number three please.

909
00:25:48,056 --> 0:25:50,056
[ Laughter and Applause ]

910
00:25:50,096 --> 0:25:50,976
I think that's pretty good.

911
00:25:51,016 --> 0:25:52,586
[ Applause ]

912
00:25:52,586 --> 0:25:53,696
And contestant number four.

913
00:25:57,046 --> 0:25:58,136
Well, I don't know about that,

914
00:25:58,196 --> 0:26:00,836
but we'll see how it goes.

915
00:25:58,196 --> 0:26:00,836
but we'll see how it goes.

916
00:26:01,151 --> 0:26:03,151
[ Applause ]

917
00:26:03,286 --> 0:26:03,836
All right.

918
00:26:03,836 --> 0:26:07,006
So let's save those, and we find

919
00:26:07,006 --> 0:26:08,286
out that the winner is

920
00:26:08,286 --> 0:26:09,636
contestant number one.

921
00:26:09,636 --> 0:26:10,446
Congratulations.

922
00:26:11,016 --> 0:26:12,736
[ Applause ]

923
00:26:12,736 --> 0:26:14,716
Now I can swipe over, and we can

924
00:26:14,796 --> 0:26:16,076
see that the faces are more

925
00:26:16,076 --> 0:26:17,656
semantically similar that way.

926
00:26:18,336 --> 0:26:19,996
They are closer to the original

927
00:26:20,096 --> 0:26:21,376
while the tree is semantically

928
00:26:21,376 --> 0:26:22,416
different, it was much further

929
00:26:22,416 --> 0:26:22,686
away.

930
00:26:23,336 --> 0:26:24,426
And that is the Image Similarity

931
00:26:24,426 --> 0:26:25,376
game, and background check to

932
00:26:25,376 --> 0:26:25,726
Rohan.

933
00:26:26,516 --> 0:26:31,776
[ Applause ]

934
00:26:32,276 --> 0:26:32,876
&gt;&gt; Thanks everyone.

935
00:26:33,476 --> 0:26:34,736
I want to take a quick look at a

936
00:26:34,736 --> 0:26:35,846
snippet from that demo

937
00:26:35,846 --> 0:26:37,196
application to show how we

938
00:26:37,196 --> 0:26:37,946
determined the winning

939
00:26:37,946 --> 0:26:38,546
contestant.

940
00:26:39,686 --> 0:26:41,906
So here I have the portion of

941
00:26:41,906 --> 0:26:43,506
the code that compares each of

942
00:26:43,506 --> 0:26:45,286
the contestant's drawings

943
00:26:45,396 --> 0:26:46,796
feature print to Brett's

944
00:26:46,796 --> 0:26:47,666
drawing's feature print.

945
00:26:48,326 --> 0:26:49,006
Now, I extracted the

946
00:26:49,006 --> 0:26:50,276
contestant's feature print with

947
00:26:50,276 --> 0:26:51,396
a function we have defined in

948
00:26:51,396 --> 0:26:52,566
the application called

949
00:26:52,566 --> 0:26:54,466
featureprintObservationForImage.

950
00:26:55,186 --> 0:26:56,956
Once I have each feature print,

951
00:26:57,226 --> 0:26:58,666
I then need to determine how

952
00:26:58,666 --> 0:27:00,056
similar it was to the original

953
00:26:58,666 --> 0:27:00,056
similar it was to the original

954
00:27:00,056 --> 0:27:01,846
drawing, and I can do that using

955
00:27:01,846 --> 0:27:03,246
computeDistance, which returns

956
00:27:03,246 --> 0:27:04,576
me a floating-point value.

957
00:27:05,166 --> 0:27:05,826
Now, the smaller the

958
00:27:05,826 --> 0:27:07,206
floating-point value, the more

959
00:27:07,206 --> 0:27:08,646
similar the two images are.

960
00:27:09,236 --> 0:27:10,456
And so, once I've determined

961
00:27:10,456 --> 0:27:11,636
this for every contestant, I

962
00:27:11,636 --> 0:27:13,116
simply need to sort them in

963
00:27:13,116 --> 0:27:14,276
order to determine the winner.

964
00:27:15,336 --> 0:27:17,006
Well, this concludes the portion

965
00:27:17,156 --> 0:27:18,156
on Image Similarity.

966
00:27:18,486 --> 0:27:19,536
I'd now like to hand the mic

967
00:27:19,536 --> 0:27:20,816
over to Sergey to talk about

968
00:27:20,816 --> 0:27:21,866
some of the changes coming to

969
00:27:21,866 --> 0:27:22,836
Face Technologies.

970
00:27:23,516 --> 0:27:28,500
[ Applause ]

971
00:27:33,056 --> 0:27:33,876
&gt;&gt; Good morning everybody.

972
00:27:34,196 --> 0:27:35,226
My name is Sergey Kamensky.

973
00:27:35,226 --> 0:27:36,196
I'm a software engineer on the

974
00:27:36,196 --> 0:27:36,926
Vision Framework Team.

975
00:27:37,356 --> 0:27:38,456
I'm excited to share with you

976
00:27:38,456 --> 0:27:39,786
today even more new features

977
00:27:39,876 --> 0:27:40,926
coming to the Framework this

978
00:27:40,926 --> 0:27:41,096
year.

979
00:27:41,096 --> 0:27:43,286
Let's talk about Face Technology

980
00:27:43,286 --> 0:27:43,606
first.

981
00:27:44,206 --> 0:27:45,956
Remember, two years ago when we

982
00:27:45,956 --> 0:27:47,836
introduced Vision Framework, we

983
00:27:47,836 --> 0:27:49,366
also talked about Face Landmark

984
00:27:49,416 --> 0:27:49,806
detector.

985
00:27:50,266 --> 0:27:51,596
This year, we're coming with a

986
00:27:51,596 --> 0:27:52,796
new revision for this algorithm.

987
00:27:53,176 --> 0:27:54,806
So, what are the changes?

988
00:27:55,916 --> 0:27:57,786
Well, first, we now have

989
00:27:57,926 --> 0:27:59,616
76-point cancellation, and this

990
00:27:59,616 --> 0:28:01,426
is versus 65-point cancellation

991
00:27:59,616 --> 0:28:01,426
is versus 65-point cancellation

992
00:28:01,426 --> 0:28:02,166
as we had before.

993
00:28:02,166 --> 0:28:04,116
The 76-point cancellation gives

994
00:28:04,116 --> 0:28:05,146
us a greater density to

995
00:28:05,146 --> 0:28:06,276
represent different face

996
00:28:06,276 --> 0:28:06,686
regions.

997
00:28:07,556 --> 0:28:09,556
Second, we now report confidence

998
00:28:09,556 --> 0:28:11,306
score per landmark point, and

999
00:28:11,306 --> 0:28:12,886
this is versus a single average

1000
00:28:12,886 --> 0:28:14,446
confidence score, as we reported

1001
00:28:14,446 --> 0:28:14,836
before.

1002
00:28:14,836 --> 0:28:16,646
But the biggest improvement

1003
00:28:16,646 --> 0:28:18,126
comes in the pupil detection.

1004
00:28:18,706 --> 0:28:19,996
As you can see, the image on the

1005
00:28:19,996 --> 0:28:21,296
right-hand side has pupils

1006
00:28:21,296 --> 0:28:22,406
detected with much better

1007
00:28:22,566 --> 0:28:23,066
accuracy.

1008
00:28:23,516 --> 0:28:25,886
Let's take a look at the client

1009
00:28:25,886 --> 0:28:26,416
code sample.

1010
00:28:27,986 --> 0:28:29,676
This code snippet will repeat

1011
00:28:29,676 --> 0:28:31,056
throughout the presentation so

1012
00:28:31,056 --> 0:28:32,386
the first time we're going to go

1013
00:28:32,386 --> 0:28:33,206
line by line.

1014
00:28:33,496 --> 0:28:35,516
Also, I use for [inaudible] in

1015
00:28:35,516 --> 0:28:36,246
my samples.

1016
00:28:36,246 --> 0:28:37,946
If this is just to simplify the

1017
00:28:37,946 --> 0:28:39,376
slides, when you develop your

1018
00:28:39,376 --> 0:28:40,736
apps, you probably should use

1019
00:28:40,786 --> 0:28:42,276
proper error handling to avoid

1020
00:28:42,276 --> 0:28:43,516
unwanted boundary conditions.

1021
00:28:44,256 --> 0:28:45,156
Let's get back to the sample.

1022
00:28:46,276 --> 0:28:47,276
In order to get your facial

1023
00:28:47,276 --> 0:28:48,866
landmarks, first you need to

1024
00:28:48,866 --> 0:28:49,416
create a

1025
00:28:49,416 --> 0:28:50,716
DetectFaceLandmarksRequest.

1026
00:28:51,296 --> 0:28:52,536
Then, you need to create

1027
00:28:52,536 --> 0:28:54,036
ImageRequestHandler, passing the

1028
00:28:54,036 --> 0:28:56,096
image into it the image that

1029
00:28:56,096 --> 0:28:58,346
needs to be processed, and then

1030
00:28:58,626 --> 0:28:59,626
you need to use that request

1031
00:28:59,626 --> 0:29:00,776
handler to process your request.

1032
00:28:59,626 --> 0:29:00,776
handler to process your request.

1033
00:29:01,426 --> 0:29:03,136
Finally, you need to look at the

1034
00:29:03,136 --> 0:29:03,606
results.

1035
00:29:04,386 --> 0:29:05,446
The results for everything that

1036
00:29:05,446 --> 0:29:06,736
this human face related in

1037
00:29:06,736 --> 0:29:08,156
Vision Framework will come in

1038
00:29:08,196 --> 0:29:09,946
forms of face observations.

1039
00:29:10,506 --> 0:29:12,006
Face observation derives some

1040
00:29:12,006 --> 0:29:13,486
detected object observation.

1041
00:29:13,856 --> 0:29:15,006
It inherits bounding box

1042
00:29:15,046 --> 0:29:16,396
property, and it also adds

1043
00:29:16,396 --> 0:29:17,766
several other properties on its

1044
00:29:17,766 --> 0:29:19,886
level to describe human face.

1045
00:29:20,766 --> 0:29:21,806
This time we'll be interested in

1046
00:29:21,806 --> 0:29:22,636
the landmarks property.

1047
00:29:23,136 --> 0:29:24,366
The landmarks property is of

1048
00:29:24,426 --> 0:29:25,976
FaceLandmarks2D class.

1049
00:29:26,176 --> 0:29:28,166
FaceLandmarks2D class consists

1050
00:29:28,416 --> 0:29:29,896
of the confidence score.

1051
00:29:30,086 --> 0:29:31,356
This is the average single

1052
00:29:31,356 --> 0:29:32,476
average confidence score for the

1053
00:29:32,476 --> 0:29:34,716
entire set and multiple face

1054
00:29:34,716 --> 0:29:37,296
regions where each face region

1055
00:29:37,296 --> 0:29:38,196
is represented by

1056
00:29:38,196 --> 0:29:40,176
FaceLandmarksRegion2D class.

1057
00:29:40,636 --> 0:29:41,856
Let's take a closer look at the

1058
00:29:41,856 --> 0:29:44,536
properties of this class.

1059
00:29:44,716 --> 0:29:46,166
First is pointCount.

1060
00:29:46,666 --> 0:29:48,156
PointCount will tell you how

1061
00:29:48,156 --> 0:29:49,546
many points represent a

1062
00:29:49,546 --> 0:29:50,756
particular face region.

1063
00:29:50,996 --> 0:29:52,256
This property will [inaudible] a

1064
00:29:52,256 --> 0:29:53,616
different value depending how

1065
00:29:53,616 --> 0:29:55,366
you configure your request, with

1066
00:29:55,366 --> 0:29:56,936
65-point cancellation or

1067
00:29:56,936 --> 0:29:58,376
76-point cancellation.

1068
00:29:59,646 --> 0:30:01,146
The normalizedPoints property

1069
00:29:59,646 --> 0:30:01,146
The normalizedPoints property

1070
00:30:01,916 --> 0:30:03,876
will represent the actual

1071
00:30:03,876 --> 0:30:05,596
landmarks point, and the

1072
00:30:05,596 --> 0:30:07,326
precisionEstimatesPerPoint will

1073
00:30:07,326 --> 0:30:08,916
represent the actual confidence

1074
00:30:08,916 --> 0:30:10,416
score for teach landmark point.

1075
00:30:11,456 --> 0:30:12,386
Let's take a look at the codes

1076
00:30:12,386 --> 0:30:12,666
needed.

1077
00:30:12,666 --> 0:30:14,496
This is the same code snippet as

1078
00:30:14,496 --> 0:30:16,106
in the previous slide, but now

1079
00:30:16,106 --> 0:30:17,056
we're going to look at it from a

1080
00:30:17,056 --> 0:30:18,196
slightly different perspective.

1081
00:30:18,576 --> 0:30:20,266
We want to see how revisioning

1082
00:30:20,266 --> 0:30:22,296
of the algorithm works in Vision

1083
00:30:22,296 --> 0:30:22,696
Framework.

1084
00:30:23,446 --> 0:30:25,056
If you take this code snippet

1085
00:30:25,056 --> 0:30:26,276
and recompile it with the last

1086
00:30:26,276 --> 0:30:28,076
[inaudible], what you will get

1087
00:30:28,076 --> 0:30:30,086
is that the request object will

1088
00:30:30,086 --> 0:30:31,806
be configured as follows: the

1089
00:30:31,806 --> 0:30:33,196
revision property will be set to

1090
00:30:33,196 --> 0:30:34,786
revision number 2, and the

1091
00:30:34,786 --> 0:30:36,136
cancellation property will be

1092
00:30:36,136 --> 0:30:37,426
set to cancellation of 65

1093
00:30:37,426 --> 0:30:37,816
points.

1094
00:30:38,396 --> 0:30:39,056
Technically, we didn't have

1095
00:30:39,056 --> 0:30:40,286
cancellation property last year,

1096
00:30:40,286 --> 0:30:41,456
but if we did, we could have set

1097
00:30:41,456 --> 0:30:42,616
it to a single value only.

1098
00:30:43,426 --> 0:30:45,726
Now, if on the other hand you

1099
00:30:45,726 --> 0:30:47,246
take the same code snippet and

1100
00:30:47,246 --> 0:30:49,136
recompile it with the current

1101
00:30:49,686 --> 0:30:50,336
[inaudible], what you will get

1102
00:30:50,336 --> 0:30:51,856
is that the revision property

1103
00:30:51,856 --> 0:30:52,846
will be set to revision number

1104
00:30:52,846 --> 0:30:55,056
3, and the cancellation property

1105
00:30:55,056 --> 0:30:56,726
will be set to cancellation 76

1106
00:30:56,726 --> 0:30:57,076
points.

1107
00:30:58,626 --> 0:30:59,706
This actually represents the

1108
00:30:59,706 --> 0:31:00,846
philosophy of how Vision

1109
00:30:59,706 --> 0:31:00,846
philosophy of how Vision

1110
00:31:00,846 --> 0:31:02,606
Framework handles revisions of

1111
00:31:02,606 --> 0:31:04,006
algorithms by default.

1112
00:31:04,096 --> 0:31:05,766
If you don't specify a revision,

1113
00:31:05,766 --> 0:31:07,716
what we will do is, we will give

1114
00:31:07,716 --> 0:31:09,366
the latest supported by the SDK

1115
00:31:09,366 --> 0:31:11,436
your code is compiled and linked

1116
00:31:11,436 --> 0:31:11,916
against.

1117
00:31:12,116 --> 0:31:14,066
Of course, we'll always

1118
00:31:14,066 --> 0:31:14,856
recommend to set those

1119
00:31:14,856 --> 0:31:15,836
properties explicitly.

1120
00:31:16,116 --> 0:31:17,106
This is just to guarantee

1121
00:31:17,106 --> 0:31:18,296
deterministic behavior in the

1122
00:31:18,296 --> 0:31:18,656
future.

1123
00:31:19,386 --> 0:31:22,266
Let's take a new metric that we

1124
00:31:22,266 --> 0:31:23,556
developed this year, Face

1125
00:31:23,556 --> 0:31:24,286
Capture Quality.

1126
00:31:24,786 --> 0:31:25,686
There are two images on the

1127
00:31:25,686 --> 0:31:25,946
screen.

1128
00:31:26,296 --> 0:31:27,416
You can clearly see that one

1129
00:31:27,416 --> 0:31:28,536
image was captured with better

1130
00:31:28,536 --> 0:31:29,436
lighting and focusing

1131
00:31:29,436 --> 0:31:29,996
conditions.

1132
00:31:30,846 --> 0:31:32,016
We wanted to develop the metric

1133
00:31:32,016 --> 0:31:33,376
that looks at the image as a

1134
00:31:33,376 --> 0:31:35,016
whole and gives you one score

1135
00:31:35,016 --> 0:31:36,796
back saying how bad or good the

1136
00:31:36,796 --> 0:31:37,966
capture quality was.

1137
00:31:37,966 --> 0:31:40,366
As a result, we came up with a

1138
00:31:40,366 --> 0:31:41,596
Face Capture Quality metric.

1139
00:31:42,466 --> 0:31:43,656
We trained our models for this

1140
00:31:43,656 --> 0:31:45,026
metric in such a way so they

1141
00:31:45,026 --> 0:31:46,876
tend to score lower if the image

1142
00:31:46,876 --> 0:31:48,456
was captured with low light or

1143
00:31:48,456 --> 0:31:50,206
bad focus, or for example, if a

1144
00:31:50,236 --> 0:31:51,936
person had negative expressions.

1145
00:31:52,896 --> 0:31:54,316
If we run this metric on these

1146
00:31:54,346 --> 0:31:55,626
two images, we will get our

1147
00:31:55,626 --> 0:31:56,206
scores back.

1148
00:31:57,036 --> 0:31:57,876
These are floating-point

1149
00:31:57,876 --> 0:31:58,316
numbers.

1150
00:31:58,636 --> 0:31:59,636
You can compare them against

1151
00:31:59,636 --> 0:32:00,776
each other, and you can say that

1152
00:31:59,636 --> 0:32:00,776
each other, and you can say that

1153
00:32:00,776 --> 0:32:02,856
the image that scored higher is

1154
00:32:02,856 --> 0:32:04,056
the image that was captured with

1155
00:32:04,056 --> 0:32:04,716
better quality.

1156
00:32:05,346 --> 0:32:07,466
Let's take a look at the code

1157
00:32:07,466 --> 0:32:08,136
sample.

1158
00:32:09,606 --> 0:32:10,756
This is very similar to what we

1159
00:32:10,756 --> 0:32:11,986
saw just a couple of slides ago,

1160
00:32:12,196 --> 0:32:13,416
with the differences being in

1161
00:32:13,416 --> 0:32:15,596
the request type and the

1162
00:32:15,596 --> 0:32:16,136
results.

1163
00:32:16,846 --> 0:32:18,526
Since we still with C1 faces,

1164
00:32:18,736 --> 0:32:19,556
we're going to get our face

1165
00:32:19,556 --> 0:32:20,826
observation back, but now we're

1166
00:32:20,826 --> 0:32:21,686
going to look at a different

1167
00:32:21,686 --> 0:32:22,426
property of the face

1168
00:32:22,426 --> 0:32:23,926
observation, Face Capture

1169
00:32:23,926 --> 0:32:24,626
Quality property.

1170
00:32:24,876 --> 0:32:27,546
Let's take a look at the broader

1171
00:32:27,546 --> 0:32:27,946
example.

1172
00:32:28,616 --> 0:32:29,646
Let's say I have a sequence of

1173
00:32:29,646 --> 0:32:30,586
images that could have been

1174
00:32:30,586 --> 0:32:31,976
obtained by using the burst mode

1175
00:32:31,976 --> 0:32:33,346
on the selfie camera or in the

1176
00:32:33,346 --> 0:32:34,316
photo burst, for example.

1177
00:32:34,626 --> 0:32:35,776
And you will ask yourself a

1178
00:32:35,776 --> 0:32:36,166
question.

1179
00:32:36,426 --> 0:32:37,816
Which image was captured with

1180
00:32:37,816 --> 0:32:38,566
the best quality?

1181
00:32:39,686 --> 0:32:40,966
What you can do now, you can run

1182
00:32:40,966 --> 0:32:42,166
our algorithm on each image,

1183
00:32:42,506 --> 0:32:45,366
assign scores, rank them, and

1184
00:32:45,366 --> 0:32:46,466
the image that apps on the most

1185
00:32:46,466 --> 0:32:47,876
light is the image that was

1186
00:32:47,876 --> 0:32:50,896
captured with the best quality.

1187
00:32:50,966 --> 0:32:52,436
Let's try to understand how we

1188
00:32:52,436 --> 0:32:54,026
can interpret the results that

1189
00:32:54,026 --> 0:32:55,166
are coming from the Face Capture

1190
00:32:55,196 --> 0:32:55,856
Quality metric.

1191
00:32:56,586 --> 0:32:58,206
I have two sequences of images

1192
00:32:58,356 --> 0:32:58,886
on the slide.

1193
00:32:59,426 --> 0:33:00,726
Each sequence is of the same

1194
00:32:59,426 --> 0:33:00,726
Each sequence is of the same

1195
00:33:00,726 --> 0:33:02,146
person, and each sequence is

1196
00:33:02,146 --> 0:33:03,456
represented by the images that

1197
00:33:03,456 --> 0:33:05,286
scores lowest and the highest in

1198
00:33:05,286 --> 0:33:06,346
the sequence with respect to

1199
00:33:06,346 --> 0:33:07,216
Face Capture Quality.

1200
00:33:08,116 --> 0:33:09,076
What can we say about these

1201
00:33:09,076 --> 0:33:09,496
ranges?

1202
00:33:10,816 --> 0:33:12,356
Well, there is some overlapping

1203
00:33:12,356 --> 0:33:13,696
region, but there are some also

1204
00:33:13,696 --> 0:33:15,006
regions that belong to one and

1205
00:33:15,006 --> 0:33:16,336
don't belong to the other.

1206
00:33:16,516 --> 0:33:18,216
If you had yet another sequence,

1207
00:33:18,536 --> 0:33:19,306
it could have happened that

1208
00:33:19,306 --> 0:33:20,536
there was no overlapping region

1209
00:33:20,536 --> 0:33:20,926
at all.

1210
00:33:21,856 --> 0:33:22,976
The point I'm trying to make

1211
00:33:22,976 --> 0:33:24,686
here is that the Face Capture

1212
00:33:24,686 --> 0:33:26,146
Quality should not be compared

1213
00:33:26,146 --> 0:33:26,906
against a threshold.

1214
00:33:28,126 --> 0:33:29,466
In this particular example, if I

1215
00:33:29,466 --> 0:33:32,096
picked 0.52, I would have missed

1216
00:33:32,326 --> 0:33:33,876
all the images on the left, and

1217
00:33:33,876 --> 0:33:36,556
I would pretty much can get any

1218
00:33:36,556 --> 0:33:37,406
image that's just past the

1219
00:33:37,406 --> 0:33:38,626
midpoint on the right.

1220
00:33:39,996 --> 0:33:41,286
But then what is Face Capture

1221
00:33:41,346 --> 0:33:41,676
Quality?

1222
00:33:42,706 --> 0:33:44,246
We define Face Capture Quality

1223
00:33:44,246 --> 0:33:46,286
is a comparative or ranking

1224
00:33:46,326 --> 0:33:47,766
measure of the same subject.

1225
00:33:48,186 --> 0:33:49,756
Now, comparative and same are

1226
00:33:49,756 --> 0:33:51,086
the key words in this sentence.

1227
00:33:51,526 --> 0:33:53,216
If you're thinking, cool, I have

1228
00:33:53,216 --> 0:33:54,476
this great new metric, I'm going

1229
00:33:54,476 --> 0:33:55,606
to develop my beauty contest

1230
00:33:55,606 --> 0:33:55,726
app.

1231
00:33:56,946 --> 0:33:58,126
Probably not a good idea.

1232
00:33:58,606 --> 0:33:59,636
In a beauty contest app, you

1233
00:33:59,636 --> 0:34:01,296
would have to compare faces of

1234
00:33:59,636 --> 0:34:01,296
would have to compare faces of

1235
00:34:01,296 --> 0:34:02,806
different people, and that's not

1236
00:34:02,806 --> 0:34:03,996
what this metric was developed

1237
00:34:03,996 --> 0:34:04,626
and designed for.

1238
00:34:06,266 --> 0:34:07,696
And that's Face Technology.

1239
00:34:09,295 --> 0:34:10,076
Let's take a look at the new

1240
00:34:10,076 --> 0:34:11,146
detectors we're adding this

1241
00:34:11,146 --> 0:34:11,335
year.

1242
00:34:12,896 --> 0:34:15,466
We're introducing Human Detector

1243
00:34:15,466 --> 0:34:16,726
that detects human upper body

1244
00:34:16,726 --> 0:34:18,076
that consists of human head and

1245
00:34:18,076 --> 0:34:20,386
torso and also a pet detector,

1246
00:34:20,795 --> 0:34:22,326
an Animal Detector that detects

1247
00:34:22,696 --> 0:34:23,406
cats and dogs.

1248
00:34:23,735 --> 0:34:24,755
The Animal Detector gives you

1249
00:34:24,755 --> 0:34:26,166
bounding box back, and in

1250
00:34:26,166 --> 0:34:27,286
addition to bounding boxes it

1251
00:34:27,286 --> 0:34:28,636
gives you also a label saying

1252
00:34:28,866 --> 0:34:30,235
which animal was detected.

1253
00:34:31,795 --> 0:34:32,746
Let's take a look at the client

1254
00:34:32,746 --> 0:34:33,226
code sample.

1255
00:34:35,956 --> 0:34:37,916
Two snippets, one for Human

1256
00:34:37,916 --> 0:34:39,366
Detector, one for Animal

1257
00:34:39,366 --> 0:34:39,815
Detector.

1258
00:34:40,326 --> 0:34:41,386
Very similar to what we had

1259
00:34:41,386 --> 0:34:41,835
before.

1260
00:34:42,016 --> 0:34:43,646
Again, the differences are in

1261
00:34:43,646 --> 0:34:44,596
the request types that you

1262
00:34:44,596 --> 0:34:46,386
create and in the results.

1263
00:34:47,186 --> 0:34:49,525
Now, for Human Detector, all we

1264
00:34:49,826 --> 0:34:51,275
care about is the bounding box.

1265
00:34:51,886 --> 0:34:53,166
So, we use for that

1266
00:34:53,306 --> 0:34:54,606
DetectedObjectObservation.

1267
00:34:55,726 --> 0:34:56,706
For the Animal Detector on the

1268
00:34:56,706 --> 0:34:57,706
other hand, we also need the

1269
00:34:57,706 --> 0:34:58,986
label, so we use

1270
00:34:59,676 --> 0:35:01,206
RecognizedObjectObservation that

1271
00:34:59,676 --> 0:35:01,206
RecognizedObjectObservation that

1272
00:35:01,206 --> 0:35:02,496
derives from detected object

1273
00:35:02,496 --> 0:35:03,076
observation.

1274
00:35:03,186 --> 0:35:04,536
It inherits bounding box, but it

1275
00:35:04,536 --> 0:35:06,736
also adds a label property on

1276
00:35:06,806 --> 0:35:07,296
the [inaudible].

1277
00:35:07,806 --> 0:35:10,486
And that's new detectors.

1278
00:35:11,246 --> 0:35:12,456
Let's take a look at what's new

1279
00:35:12,456 --> 0:35:13,336
in tracking this year.

1280
00:35:14,216 --> 0:35:15,126
We're coming up with a new

1281
00:35:15,126 --> 0:35:16,556
revision for the Tracker.

1282
00:35:16,556 --> 0:35:17,996
The changes are, we have

1283
00:35:17,996 --> 0:35:19,516
improvements in the bounding

1284
00:35:19,516 --> 0:35:20,396
boxes expansion area.

1285
00:35:21,266 --> 0:35:22,386
We can now handle better

1286
00:35:22,386 --> 0:35:22,976
occlusions.

1287
00:35:23,536 --> 0:35:25,716
We are machine learning based

1288
00:35:25,716 --> 0:35:26,176
this time.

1289
00:35:26,886 --> 0:35:28,086
And we can run with low power

1290
00:35:28,086 --> 0:35:29,046
consumption on multiple

1291
00:35:29,046 --> 0:35:29,476
[inaudible] devices.

1292
00:35:29,476 --> 0:35:32,516
Let's take a look at a sample.

1293
00:35:32,956 --> 0:35:35,056
I have a mini video clip where a

1294
00:35:35,056 --> 0:35:36,446
man is running in the forest,

1295
00:35:36,606 --> 0:35:38,126
and he appears sometimes behind

1296
00:35:38,126 --> 0:35:38,646
the trees.

1297
00:35:38,976 --> 0:35:40,126
As you can see, the tracker is

1298
00:35:40,126 --> 0:35:41,606
able to successfully recapture

1299
00:35:41,606 --> 0:35:42,926
the tracked object and keep

1300
00:35:42,926 --> 0:35:43,726
going with the tracking

1301
00:35:43,726 --> 0:35:44,186
sequence.

1302
00:35:46,016 --> 0:35:47,046
[ Applause ]

1303
00:35:47,046 --> 0:35:47,486
Thank you.

1304
00:35:48,516 --> 0:35:51,976
[ Applause ]

1305
00:35:52,476 --> 0:35:53,756
Let's take a look at the client

1306
00:35:53,756 --> 0:35:54,256
code sample.

1307
00:35:54,676 --> 0:35:56,046
This is exactly the same snippet

1308
00:35:56,046 --> 0:35:56,996
that we showed last year.

1309
00:35:57,256 --> 0:35:58,306
It represents probably the

1310
00:35:58,306 --> 0:35:59,416
simplest tracking sequence you

1311
00:35:59,416 --> 0:35:59,946
can imagine.

1312
00:36:00,166 --> 0:36:01,106
It tracks your object of

1313
00:36:01,106 --> 0:36:02,336
interest for five consecutive

1314
00:36:02,336 --> 0:36:02,786
frames.

1315
00:36:03,986 --> 0:36:05,356
I want to go line-by-line, but I

1316
00:36:05,356 --> 0:36:06,916
want to emphasize two points

1317
00:36:06,916 --> 0:36:07,116
here.

1318
00:36:07,486 --> 0:36:09,116
First is we use our

1319
00:36:09,116 --> 0:36:10,036
SequenceRequestHandler.

1320
00:36:11,066 --> 0:36:12,046
That is as opposite to

1321
00:36:12,046 --> 0:36:13,226
ImageRequestHandler as we have

1322
00:36:13,226 --> 0:36:14,226
used so far throughout the

1323
00:36:14,226 --> 0:36:14,816
presentation.

1324
00:36:15,336 --> 0:36:16,616
SequenceRequestHandler is used

1325
00:36:16,616 --> 0:36:17,986
in Vision when you work with a

1326
00:36:17,986 --> 0:36:19,216
sequence of frames and you need

1327
00:36:19,216 --> 0:36:20,426
to cache some information from

1328
00:36:20,426 --> 0:36:21,496
frame to frame to frame.

1329
00:36:22,826 --> 0:36:24,356
Second point is when you

1330
00:36:24,356 --> 0:36:25,106
implement your tracking

1331
00:36:25,106 --> 0:36:26,836
sequence, you need to get your

1332
00:36:26,836 --> 0:36:28,356
results from iteration number n

1333
00:36:28,356 --> 0:36:30,026
and feed it as an input to a

1334
00:36:30,026 --> 0:36:31,276
duration number n plus 1.

1335
00:36:31,976 --> 0:36:35,216
Of course, if you recompiled

1336
00:36:35,216 --> 0:36:36,106
this quote with the current

1337
00:36:36,106 --> 0:36:37,646
[inaudible] SDK, the revision of

1338
00:36:37,646 --> 0:36:38,846
the request will be set to

1339
00:36:38,846 --> 0:36:40,316
revision number 2 by default.

1340
00:36:40,496 --> 0:36:41,836
But we also recommend to set it

1341
00:36:41,836 --> 0:36:42,426
explicitly.

1342
00:36:42,966 --> 0:36:45,106
And that's the tracking.

1343
00:36:46,136 --> 0:36:47,376
Let's take a look at the news

1344
00:36:47,376 --> 0:36:49,696
with respect to Vision and Core

1345
00:36:49,696 --> 0:36:50,396
ML integration.

1346
00:36:51,146 --> 0:36:52,766
Last year, we presented

1347
00:36:52,766 --> 0:36:53,836
integration with Vision and Core

1348
00:36:53,836 --> 0:36:55,496
ML, and we showed how you can

1349
00:36:55,496 --> 0:36:57,066
run Core ML models through

1350
00:36:57,066 --> 0:36:57,596
Vision API.

1351
00:36:57,596 --> 0:36:59,776
The advantage of doing that was

1352
00:36:59,776 --> 0:37:01,706
that you can use 1 over 5

1353
00:36:59,776 --> 0:37:01,706
that you can use 1 over 5

1354
00:37:01,816 --> 0:37:03,236
different overloads of the image

1355
00:37:03,236 --> 0:37:04,916
request handler to translate the

1356
00:37:04,916 --> 0:37:06,266
image that you have in your hand

1357
00:37:06,586 --> 0:37:08,976
to the image type, size, and

1358
00:37:08,976 --> 0:37:10,416
color scheme that the Core ML

1359
00:37:10,416 --> 0:37:11,266
model requires.

1360
00:37:12,246 --> 0:37:13,256
We will run the inference for

1361
00:37:13,256 --> 0:37:15,016
you, and we'll pack the outputs

1362
00:37:15,016 --> 0:37:16,436
or results coming from Core ML

1363
00:37:16,436 --> 0:37:17,886
model into Vision observations.

1364
00:37:20,716 --> 0:37:22,426
Now, if you have a different

1365
00:37:22,426 --> 0:37:23,696
task in mind, for example, if

1366
00:37:23,696 --> 0:37:24,646
you want to do image style

1367
00:37:24,646 --> 0:37:26,066
transfer, you need to have at

1368
00:37:26,066 --> 0:37:27,416
least two images, the image

1369
00:37:27,416 --> 0:37:29,056
content and the image style.

1370
00:37:29,256 --> 0:37:30,356
You may also need to have some

1371
00:37:30,356 --> 0:37:31,816
mixed ratio saying how much of a

1372
00:37:31,926 --> 0:37:33,386
style needs to be applied on the

1373
00:37:33,386 --> 0:37:33,826
content.

1374
00:37:34,306 --> 0:37:35,506
So, I have three parameters now.

1375
00:37:36,826 --> 0:37:37,886
Well, this year we're going to

1376
00:37:37,886 --> 0:37:39,666
introduce API where we can use

1377
00:37:39,666 --> 0:37:41,716
multiple inputs through Vision

1378
00:37:41,896 --> 0:37:43,476
to Core ML, and that's including

1379
00:37:43,476 --> 0:37:44,426
multi-image inputs.

1380
00:37:44,426 --> 0:37:47,696
Also, on the output section,

1381
00:37:48,186 --> 0:37:49,346
this sample shows only one

1382
00:37:49,346 --> 0:37:49,686
output.

1383
00:37:49,686 --> 0:37:50,616
But, for example, if you had

1384
00:37:50,616 --> 0:37:52,116
more than one, especially if you

1385
00:37:52,116 --> 0:37:53,376
have more than one of the same

1386
00:37:53,376 --> 0:37:55,256
type, it's hard to distinguish

1387
00:37:55,256 --> 0:37:56,356
them when they come in forms of

1388
00:37:56,356 --> 0:37:57,946
observation later on.

1389
00:37:58,416 --> 0:37:59,616
So, what we do this year, we

1390
00:37:59,616 --> 0:38:00,646
introduce a new field in the

1391
00:37:59,616 --> 0:38:00,646
introduce a new field in the

1392
00:38:00,646 --> 0:38:02,616
observation that maps exactly to

1393
00:38:02,616 --> 0:38:04,376
the name that shows up here in

1394
00:38:04,376 --> 0:38:05,056
the output section.

1395
00:38:06,216 --> 0:38:07,696
Let's take a look at the inputs

1396
00:38:07,696 --> 0:38:08,316
and outputs.

1397
00:38:08,316 --> 0:38:09,316
We will use them in the next

1398
00:38:09,316 --> 0:38:09,576
slide.

1399
00:38:12,936 --> 0:38:14,136
This is the code snippet that

1400
00:38:14,176 --> 0:38:16,816
represents how to use Core ML

1401
00:38:16,956 --> 0:38:17,396
through Vision.

1402
00:38:18,676 --> 0:38:20,236
The highlighted sections show

1403
00:38:20,446 --> 0:38:21,256
what's new this year.

1404
00:38:21,716 --> 0:38:22,796
Let's keep them for now, and

1405
00:38:22,796 --> 0:38:23,896
we'll go over the code, and

1406
00:38:23,896 --> 0:38:25,316
we'll return to them later.

1407
00:38:26,156 --> 0:38:27,466
In order to run Core ML through

1408
00:38:27,466 --> 0:38:29,446
Vision, first you need to log

1409
00:38:29,446 --> 0:38:30,236
your Core ML model.

1410
00:38:31,266 --> 0:38:32,926
Then, you need to create Vision

1411
00:38:32,926 --> 0:38:34,956
CoreMLmodel wrapper around it.

1412
00:38:35,726 --> 0:38:37,476
Then, you need to create Vision

1413
00:38:37,476 --> 0:38:39,346
CoreMLRequest and pass in that

1414
00:38:39,346 --> 0:38:39,676
wrapper.

1415
00:38:41,266 --> 0:38:42,326
Then you create

1416
00:38:42,556 --> 0:38:44,136
ImageRequestHandler, you process

1417
00:38:44,136 --> 0:38:45,446
your request, and you look at

1418
00:38:45,446 --> 0:38:46,086
the results.

1419
00:38:47,386 --> 0:38:49,596
Now, with the new API that we

1420
00:38:49,596 --> 0:38:51,596
added this year, that only image

1421
00:38:51,596 --> 0:38:53,116
that you could use last year is

1422
00:38:53,116 --> 0:38:54,986
the default or the main image is

1423
00:38:54,986 --> 0:38:56,276
the image that is passing to

1424
00:38:56,476 --> 0:38:58,046
ImageRequestHandler, but that's

1425
00:38:58,046 --> 0:38:59,526
also the image whose name needs

1426
00:38:59,526 --> 0:39:01,126
to be assigned to input feature

1427
00:38:59,526 --> 0:39:01,126
to be assigned to input feature

1428
00:39:01,506 --> 0:39:03,456
name field of the CoreMLModel

1429
00:39:03,456 --> 0:39:03,796
wrapper.

1430
00:39:05,026 --> 0:39:06,886
All other parameters whether

1431
00:39:06,886 --> 0:39:08,646
images or not will have to be

1432
00:39:08,716 --> 0:39:10,406
passed through feature provider

1433
00:39:10,446 --> 0:39:11,686
property of the CoreMLModel

1434
00:39:11,686 --> 0:39:12,016
wrapper.

1435
00:39:12,376 --> 0:39:14,036
As you can see, image style and

1436
00:39:14,036 --> 0:39:15,436
mixed ratio are passed in that

1437
00:39:15,436 --> 0:39:15,666
way.

1438
00:39:16,956 --> 0:39:18,336
Finally, when you look at the

1439
00:39:18,336 --> 0:39:19,506
results, you can look at the

1440
00:39:19,786 --> 0:39:21,386
feature name property of the

1441
00:39:21,386 --> 0:39:22,646
observation that comes out, and

1442
00:39:22,646 --> 0:39:24,186
you can compare it in this case

1443
00:39:24,186 --> 0:39:25,246
against image result.

1444
00:39:25,506 --> 0:39:26,656
That's exactly the name that

1445
00:39:26,656 --> 0:39:27,706
appears in the output section of

1446
00:39:27,756 --> 0:39:29,066
Core ML, and that way you can

1447
00:39:29,096 --> 0:39:30,006
process your results

1448
00:39:30,146 --> 0:39:30,666
accordingly.

1449
00:39:32,636 --> 0:39:33,626
This slide actually concludes

1450
00:39:33,626 --> 0:39:34,666
our presentation for today.

1451
00:39:34,966 --> 0:39:36,076
For more information you can

1452
00:39:36,076 --> 0:39:37,136
refer to the links on the slide.

1453
00:39:37,466 --> 0:39:39,276
Thank you, and have a great rest

1454
00:39:39,276 --> 0:39:39,976
of your WWDC.

1455
00:39:40,016 --> 0:39:42,000
[ Applause ]
