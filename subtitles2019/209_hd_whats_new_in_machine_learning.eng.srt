1
00:00:01,516 --> 0:00:04,500
[ Music ]

2
00:00:08,516 --> 0:00:17,546
[ Applause ]

3
00:00:18,046 --> 0:00:19,896
&gt;&gt; Hello. Welcome everyone.

4
00:00:20,656 --> 0:00:22,786
My name is Gaurav and today

5
00:00:22,786 --> 0:00:24,306
we're going to talk about What's

6
00:00:24,396 --> 0:00:27,456
New in Machine Learning.

7
00:00:29,696 --> 0:00:31,316
Machine Learning is used by

8
00:00:31,406 --> 0:00:32,326
thousands of apps.

9
00:00:33,356 --> 0:00:35,556
The apps that you are making are

10
00:00:35,556 --> 0:00:36,436
amazing.

11
00:00:37,006 --> 0:00:38,896
They're touching every aspect of

12
00:00:38,946 --> 0:00:39,846
a user's life.

13
00:00:40,456 --> 0:00:45,086
In hospitals, doctors are using

14
00:00:45,086 --> 0:00:47,326
apps such as Butterfly iQ to do

15
00:00:47,326 --> 0:00:50,356
medical diagnostics in real

16
00:00:52,096 --> 0:00:52,206
time.

17
00:00:52,416 --> 0:00:54,866
In sports, coaches are using

18
00:00:54,866 --> 0:00:57,186
apps such as HomeCourt to train

19
00:00:57,186 --> 0:00:57,646
their players.

20
00:00:58,576 --> 0:01:02,896
In creativity, apps such as

21
00:00:58,576 --> 0:01:02,896
In creativity, apps such as

22
00:01:02,896 --> 0:01:04,936
Pixelmator Pro are helping other

23
00:01:04,936 --> 0:01:06,366
users to augment their

24
00:01:06,366 --> 0:01:07,686
creativity using ML.

25
00:01:08,746 --> 0:01:10,366
These are just a few examples,

26
00:01:10,576 --> 0:01:12,446
and we would like all of these

27
00:01:12,446 --> 0:01:14,746
apps you guys should also make

28
00:01:14,746 --> 0:01:17,396
these kind of apps.

29
00:01:17,656 --> 0:01:19,446
So now the question is what's

30
00:01:19,486 --> 0:01:19,686
new?

31
00:01:20,556 --> 0:01:24,736
Let me begin by saying a lot.

32
00:01:25,576 --> 0:01:28,896
We have so much material that we

33
00:01:28,896 --> 0:01:30,746
can hardly cover it one session

34
00:01:30,746 --> 0:01:31,556
or two sessions.

35
00:01:32,126 --> 0:01:35,746
We need ten sessions to cover

36
00:01:35,746 --> 0:01:36,856
the entire material.

37
00:01:38,316 --> 0:01:39,606
We also have a session

38
00:01:39,776 --> 0:01:41,616
intersecting the design in

39
00:01:41,616 --> 0:01:43,216
machine learning.

40
00:01:44,556 --> 0:01:47,216
We also have Daily Labs where

41
00:01:47,216 --> 0:01:49,496
you can meet and discuss your

42
00:01:49,496 --> 0:01:51,006
ideas with Apple Machine

43
00:01:51,006 --> 0:01:51,746
Learning engineers.

44
00:01:53,116 --> 0:01:55,156
All of us are here to remove any

45
00:01:55,156 --> 0:01:56,956
roadblocks that you might be

46
00:01:56,956 --> 0:01:58,966
facing while integrating Machine

47
00:01:59,026 --> 0:02:00,326
Learning in your app.

48
00:01:59,026 --> 0:02:00,326
Learning in your app.

49
00:02:02,876 --> 0:02:04,776
In all of these sessions, you

50
00:02:04,776 --> 0:02:06,366
will see that we follow simple

51
00:02:06,366 --> 0:02:07,016
principles.

52
00:02:08,356 --> 0:02:11,246
We want Machine Learning to be

53
00:02:11,246 --> 0:02:12,976
as easy to use as possible.

54
00:02:12,976 --> 0:02:14,336
We are laser focused on doing

55
00:02:15,006 --> 0:02:15,106
that.

56
00:02:15,736 --> 0:02:17,766
We want to make it flexible so

57
00:02:17,766 --> 0:02:20,466
you can do it by variety of

58
00:02:21,436 --> 0:02:22,926
tasks, and we want to make it

59
00:02:22,976 --> 0:02:25,506
powerful so you can run state of

60
00:02:25,506 --> 0:02:27,246
the art Machine Learning models

61
00:02:27,336 --> 0:02:28,136
on your devices.

62
00:02:28,876 --> 0:02:32,976
It is truly Machine Learning for

63
00:02:32,976 --> 0:02:35,036
everyone whether you are a

64
00:02:35,036 --> 0:02:36,606
researcher or someone new to

65
00:02:36,606 --> 0:02:37,286
Machine Learning.

66
00:02:38,776 --> 0:02:40,046
With Create ML app you can

67
00:02:40,046 --> 0:02:41,236
make state of the art,

68
00:02:41,236 --> 0:02:42,816
task-focused ML models.

69
00:02:44,526 --> 0:02:45,846
The next big pillar of our

70
00:02:45,846 --> 0:02:47,286
offering is Domain APIs.

71
00:02:49,256 --> 0:02:50,746
Domain APIs allow you to

72
00:02:50,746 --> 0:02:52,106
leverage Apple's built-in

73
00:02:52,106 --> 0:02:53,346
intelligence and models.

74
00:02:54,266 --> 0:02:55,506
So you don't have to worry about

75
00:02:55,506 --> 0:02:56,916
collecting the data and building

76
00:02:56,916 --> 0:02:57,426
the model.

77
00:02:57,646 --> 0:03:00,296
Simply call the API and be done.

78
00:02:57,646 --> 0:03:00,296
Simply call the API and be done.

79
00:03:02,696 --> 0:03:04,326
This year we are significantly

80
00:03:04,356 --> 0:03:05,806
expanding our Domain APIs.

81
00:03:06,256 --> 0:03:08,676
We have Domain APIs in Vision,

82
00:03:09,106 --> 0:03:13,196
Text, Speech and Sound.

83
00:03:13,406 --> 0:03:15,226
Now let's take a sneak peek of

84
00:03:15,276 --> 0:03:16,386
some of these APIs.

85
00:03:17,146 --> 0:03:19,746
Let's start with Vision.

86
00:03:20,976 --> 0:03:22,906
Vision allows you to reason

87
00:03:23,016 --> 0:03:24,716
about the content of the image.

88
00:03:26,206 --> 0:03:27,386
One of the new features this

89
00:03:27,386 --> 0:03:29,036
year is Image Saliency.

90
00:03:30,096 --> 0:03:31,576
Image Saliency can help you

91
00:03:31,576 --> 0:03:33,536
identify the most relevant

92
00:03:33,536 --> 0:03:35,256
region in your image.

93
00:03:36,106 --> 0:03:39,466
In this case, the region around

94
00:03:39,636 --> 0:03:40,146
the person.

95
00:03:41,936 --> 0:03:43,056
You can use it to generate

96
00:03:43,056 --> 0:03:44,996
thumbnails or to make image

97
00:03:44,996 --> 0:03:49,196
cropping, generate memories,

98
00:03:49,196 --> 0:03:49,976
guide camera et cetera.

99
00:03:50,046 --> 0:03:53,786
Another big feature we are

100
00:03:53,786 --> 0:03:55,406
introducing this year is Text

101
00:03:55,466 --> 0:03:56,126
Recognition.

102
00:03:57,666 --> 0:03:59,516
You can now take a picture of

103
00:03:59,516 --> 0:04:01,476
the document, perform

104
00:03:59,516 --> 0:04:01,476
the document, perform

105
00:04:01,476 --> 0:04:03,556
perspective correction, lighting

106
00:04:03,556 --> 0:04:05,566
correction and reorganize the

107
00:04:05,616 --> 0:04:06,976
text on the device.

108
00:04:07,516 --> 0:04:13,986
[ Applause ]

109
00:04:14,486 --> 0:04:18,106
This is huge, but that's not

110
00:04:18,106 --> 0:04:18,666
all.

111
00:04:18,666 --> 0:04:19,796
We have Image inbuilt

112
00:04:19,796 --> 0:04:22,016
classifier, human detector, pet

113
00:04:22,016 --> 0:04:24,036
detector, and we are going to

114
00:04:24,036 --> 0:04:26,166
cover them in detail in our two

115
00:04:26,166 --> 0:04:27,046
Vision sessions.

116
00:04:31,036 --> 0:04:33,106
Next domain is Natural Language.

117
00:04:33,656 --> 0:04:36,356
Just like you use Vision to

118
00:04:36,356 --> 0:04:38,376
reason about images you can use

119
00:04:38,376 --> 0:04:41,046
Natural Language to reason about

120
00:04:42,056 --> 0:04:43,076
the text.

121
00:04:43,256 --> 0:04:44,906
New this year is inbuilt

122
00:04:44,906 --> 0:04:46,166
Sentiment Analysis.

123
00:04:46,166 --> 0:04:48,626
So you can use to analyze the

124
00:04:48,686 --> 0:04:50,616
sentiment of the text in real

125
00:04:50,616 --> 0:04:52,636
time on the device in a privacy

126
00:04:52,636 --> 0:04:53,216
friendly way.

127
00:04:53,836 --> 0:04:55,296
So, for example, if somebody

128
00:04:55,296 --> 0:04:56,486
types something like this I was

129
00:04:56,566 --> 0:04:57,926
so excited about the season

130
00:04:57,926 --> 0:04:59,406
finale that's a positive

131
00:05:00,256 --> 0:05:02,656
sentiment, but it was a bit

132
00:05:02,656 --> 0:05:04,386
disappointing at the end it's a

133
00:05:04,386 --> 0:05:05,406
negative sentiment.

134
00:05:06,056 --> 0:05:08,656
So you can provide this kind of

135
00:05:08,656 --> 0:05:10,006
feedback in real time.

136
00:05:10,526 --> 0:05:14,516
For the first time we are also

137
00:05:14,516 --> 0:05:16,826
exposing inbuilt Word

138
00:05:16,826 --> 0:05:17,386
Embeddings.

139
00:05:18,606 --> 0:05:20,096
Word Embeddings can allow you to

140
00:05:20,166 --> 0:05:21,666
find semantically similar words.

141
00:05:21,666 --> 0:05:23,426
So, for example, the word

142
00:05:23,426 --> 0:05:25,246
thunderstorm is very close to

143
00:05:25,246 --> 0:05:27,496
cloudy but it is very far away

144
00:05:27,496 --> 0:05:28,406
from shoes and boots.

145
00:05:28,896 --> 0:05:30,486
One of the big use case of Word

146
00:05:30,486 --> 0:05:31,486
Embeddings is semantic search

147
00:05:31,486 --> 0:05:33,636
and we are going to give you an

148
00:05:34,246 --> 0:05:35,246
example soon.

149
00:05:35,456 --> 0:05:36,546
Natural Language will be

150
00:05:36,546 --> 0:05:38,446
discussed in detail in Advances

151
00:05:38,446 --> 0:05:39,666
in Natural Language Framework

152
00:05:39,716 --> 0:05:40,086
session.

153
00:05:40,636 --> 0:05:45,836
The third way user interacts

154
00:05:45,836 --> 0:05:47,476
with your app is through speech

155
00:05:47,826 --> 0:05:48,426
and sound.

156
00:05:49,566 --> 0:05:51,076
Now we have an on-device speech

157
00:05:51,106 --> 0:05:52,496
support so you can transcribe

158
00:05:52,546 --> 0:05:54,986
the text on the device so you no

159
00:05:54,986 --> 0:05:56,226
longer have to rely on the

160
00:05:56,226 --> 0:05:59,266
network connection, and we also

161
00:05:59,266 --> 0:06:01,086
have new Voice Analytics API

162
00:05:59,266 --> 0:06:01,086
have new Voice Analytics API

163
00:06:01,436 --> 0:06:03,066
that can tell you not only what

164
00:06:03,066 --> 0:06:05,926
is spoken but how it is spoken.

165
00:06:05,926 --> 0:06:07,066
So you can differentiate between

166
00:06:07,066 --> 0:06:08,486
a normal voice and a high

167
00:06:08,576 --> 0:06:09,986
jittery voice.

168
00:06:11,016 --> 0:06:12,916
We also have a brand-new Sound

169
00:06:12,916 --> 0:06:14,396
Analysis Framework, that we will

170
00:06:14,396 --> 0:06:15,936
discuss in Create ML session.

171
00:06:16,416 --> 0:06:21,496
There's a lot more in each of

172
00:06:21,496 --> 0:06:24,146
these domains and another thing

173
00:06:24,146 --> 0:06:25,606
you can do to combine these

174
00:06:25,606 --> 0:06:27,396
domains almost seamlessly.

175
00:06:28,216 --> 0:06:29,246
Let me show you an example.

176
00:06:30,026 --> 0:06:33,176
Let's just say you want to build

177
00:06:33,176 --> 0:06:34,616
a feature that does Semantic

178
00:06:34,616 --> 0:06:35,566
Search on Images.

179
00:06:35,676 --> 0:06:36,896
That's a very complex feature.

180
00:06:36,896 --> 0:06:38,746
So if a user searches for

181
00:06:38,746 --> 0:06:41,056
thunderstorm, you want to

182
00:06:41,056 --> 0:06:42,366
provide them the results not

183
00:06:42,366 --> 0:06:43,886
only for thunderstorm but also

184
00:06:44,026 --> 0:06:46,716
for sky and cloudy.

185
00:06:46,836 --> 0:06:49,756
Now, you can combine Vision with

186
00:06:49,756 --> 0:06:51,336
Natural Language to implement

187
00:06:51,336 --> 0:06:53,376
this feature in very few lines

188
00:06:53,376 --> 0:06:53,806
of code.

189
00:06:55,176 --> 0:06:56,206
This is how you will do it.

190
00:06:56,206 --> 0:06:58,146
You will run your Image

191
00:06:58,146 --> 0:07:02,056
Classifier on images or you can

192
00:06:58,146 --> 0:07:02,056
Classifier on images or you can

193
00:07:02,056 --> 0:07:03,606
have them use inbuilt Image

194
00:07:03,606 --> 0:07:05,286
Classifier to generate the tags.

195
00:07:06,016 --> 0:07:08,886
When the user types the word

196
00:07:08,936 --> 0:07:10,356
something like thunderstorm, you

197
00:07:10,356 --> 0:07:11,926
can use Word Embeddings to

198
00:07:11,926 --> 0:07:15,226
generate similar words and find

199
00:07:15,226 --> 0:07:16,576
the images that matches these

200
00:07:16,646 --> 0:07:16,896
tags.

201
00:07:17,626 --> 0:07:21,896
And that's not all.

202
00:07:21,896 --> 0:07:23,756
You can also combine the custom

203
00:07:23,756 --> 0:07:24,936
models that you made using

204
00:07:24,936 --> 0:07:26,746
Create ML with Domain APIs, and

205
00:07:27,036 --> 0:07:28,646
we are going to show an example

206
00:07:28,646 --> 0:07:30,686
of that in our Creating Great

207
00:07:30,686 --> 0:07:32,496
Apps Using Core ML and ARKit

208
00:07:32,606 --> 0:07:32,996
session.

209
00:07:33,546 --> 0:07:37,426
So to summarize, Domain APIs

210
00:07:37,426 --> 0:07:38,766
allow you to leverage Apple's

211
00:07:38,896 --> 0:07:40,576
built-in intelligence and model

212
00:07:40,856 --> 0:07:42,586
using API so you don't have to

213
00:07:42,586 --> 0:07:43,856
collect the data and make the

214
00:07:43,856 --> 0:07:44,266
models.

215
00:07:44,736 --> 0:07:45,776
And this year we have a

216
00:07:45,866 --> 0:07:48,046
significant expansion in our

217
00:07:48,046 --> 0:07:49,276
Vision Natural Language and

218
00:07:49,276 --> 0:07:49,976
Speech and Sound APIs.

219
00:07:56,336 --> 0:07:58,176
Now let's talk about Core ML 3,

220
00:07:58,526 --> 0:08:00,136
the third big pillar of our

221
00:07:58,526 --> 0:08:00,136
the third big pillar of our

222
00:08:00,846 --> 0:08:01,016
offering.

223
00:08:03,536 --> 0:08:06,106
Core ML is now supported across

224
00:08:06,196 --> 0:08:10,156
all our platforms.

225
00:08:10,216 --> 0:08:11,396
All the work is done on the

226
00:08:11,396 --> 0:08:13,046
device so user's privacy is

227
00:08:13,076 --> 0:08:13,936
maintained.

228
00:08:14,716 --> 0:08:16,686
Core ML is hardware accelerated

229
00:08:16,686 --> 0:08:18,466
so you can do, you can use it

230
00:08:18,466 --> 0:08:19,726
for realtime Machine Learning

231
00:08:20,646 --> 0:08:22,586
and you don't need a server and

232
00:08:22,586 --> 0:08:23,656
it always available.

233
00:08:24,356 --> 0:08:29,666
Core ML has always supported a

234
00:08:29,666 --> 0:08:31,056
wide variety of Machine Learning

235
00:08:31,056 --> 0:08:33,006
models ranging from classical

236
00:08:33,006 --> 0:08:34,566
generalized linear models, tree

237
00:08:34,566 --> 0:08:35,976
ensembles and support vector

238
00:08:35,976 --> 0:08:38,275
machines as well as neural

239
00:08:38,275 --> 0:08:40,395
networks such as Convolution

240
00:08:40,395 --> 0:08:41,946
Neural Network and Recurrent

241
00:08:41,946 --> 0:08:43,155
Neural Networks.

242
00:08:46,236 --> 0:08:48,596
New in Core ML is model

243
00:08:48,596 --> 0:08:50,146
flexibility and model

244
00:08:50,146 --> 0:08:51,046
personalization.

245
00:08:51,626 --> 0:08:53,196
So let's take a look at both of

246
00:08:53,926 --> 0:08:54,016
them.

247
00:08:55,956 --> 0:08:58,226
Core ML has expanded support for

248
00:08:58,226 --> 0:09:01,036
Data Neural Networks and we have

249
00:08:58,226 --> 0:09:01,036
Data Neural Networks and we have

250
00:09:01,036 --> 0:09:03,686
added a support for more than

251
00:09:03,686 --> 0:09:06,106
100+ Neural Network layers.

252
00:09:07,356 --> 0:09:09,046
This means that you can bring

253
00:09:09,046 --> 0:09:11,186
almost, you can bring the most

254
00:09:11,186 --> 0:09:13,456
cutting-edge Machine Learning

255
00:09:13,456 --> 0:09:15,486
models into your app such as

256
00:09:15,486 --> 0:09:16,926
ELMo, BERT, Wavenet.

257
00:09:17,816 --> 0:09:18,586
What does that mean?

258
00:09:20,136 --> 0:09:21,716
Let's just say you have an app

259
00:09:21,716 --> 0:09:22,906
and you want to integrate a

260
00:09:22,906 --> 0:09:24,056
state of the art Question and

261
00:09:24,056 --> 0:09:26,476
Answer system in your app so

262
00:09:26,476 --> 0:09:28,156
that when a user asks a question

263
00:09:28,156 --> 0:09:29,556
how many sessions will there be

264
00:09:29,556 --> 0:09:30,606
at WWDC this year?

265
00:09:31,196 --> 0:09:32,896
You can do it by using BERT

266
00:09:32,966 --> 0:09:33,336
model.

267
00:09:34,536 --> 0:09:35,796
So the model can analyze the

268
00:09:35,796 --> 0:09:38,116
statement and gives feedback the

269
00:09:38,146 --> 0:09:43,956
result over 100 is the answer.

270
00:09:43,956 --> 0:09:45,286
Besides Natural Language you can

271
00:09:45,286 --> 0:09:47,756
also run the latest advances in

272
00:09:47,756 --> 0:09:49,166
Vision such as Instance

273
00:09:49,166 --> 0:09:51,666
Segmentation as well as latest

274
00:09:51,666 --> 0:09:53,256
advances in Audio Generation.

275
00:09:56,876 --> 0:09:58,606
And to take full advantage of

276
00:09:58,606 --> 0:10:00,216
this expanded support of Core

277
00:09:58,606 --> 0:10:00,216
this expanded support of Core

278
00:10:00,266 --> 0:10:03,146
ML, we are updating our

279
00:10:03,146 --> 0:10:03,556
converters.

280
00:10:03,556 --> 0:10:04,986
So we will have a brand new

281
00:10:04,986 --> 0:10:06,276
TensorFlow Core ML converter and

282
00:10:06,276 --> 0:10:08,676
ONNX support ML converter will

283
00:10:08,676 --> 0:10:09,566
be coming soon.

284
00:10:10,176 --> 0:10:14,756
We are also updating our Model

285
00:10:14,756 --> 0:10:16,326
Gallery importing some of these

286
00:10:16,366 --> 0:10:18,726
research models on our Model

287
00:10:18,726 --> 0:10:20,596
Gallery so you can start using

288
00:10:20,596 --> 0:10:21,566
them immediately.

289
00:10:22,686 --> 0:10:24,466
So Core ML 3 with this new model

290
00:10:24,466 --> 0:10:26,016
representation and embedded

291
00:10:26,056 --> 0:10:27,666
converters with Model Gallery

292
00:10:28,196 --> 0:10:29,526
can help you bring the cutting

293
00:10:29,526 --> 0:10:31,316
edge ML research into your app.

294
00:10:32,216 --> 0:10:33,476
Now a big feature, Model

295
00:10:33,476 --> 0:10:34,456
Personalization.

296
00:10:35,146 --> 0:10:36,836
So let me explain what it is.

297
00:10:37,336 --> 0:10:39,086
So up to now you have seen, you

298
00:10:39,086 --> 0:10:41,106
have used Create ML to build the

299
00:10:41,106 --> 0:10:44,006
models and Core ML to deploy the

300
00:10:44,006 --> 0:10:47,616
models, but new in Core ML 3 is

301
00:10:47,846 --> 0:10:49,806
On-Device Model Personalization.

302
00:10:50,266 --> 0:10:52,726
You can fine tune the model on

303
00:10:52,726 --> 0:10:53,236
the device.

304
00:10:53,966 --> 0:10:58,196
And we use this kind of

305
00:10:58,196 --> 0:11:00,466
technology in Face ID and

306
00:10:58,196 --> 0:11:00,466
technology in Face ID and

307
00:11:00,466 --> 0:11:01,276
setting Watch Face.

308
00:11:02,026 --> 0:11:05,536
So this is how it happens.

309
00:11:05,756 --> 0:11:08,506
Today you have data, you make an

310
00:11:08,566 --> 0:11:11,526
ML model, you ship it to your

311
00:11:11,736 --> 0:11:13,776
app and all of your users

312
00:11:13,776 --> 0:11:15,136
download the same model.

313
00:11:15,706 --> 0:11:19,716
And this is great because now

314
00:11:20,196 --> 0:11:21,596
users don't have to upload their

315
00:11:21,596 --> 0:11:21,986
photos.

316
00:11:22,346 --> 0:11:24,166
You can directly take photos,

317
00:11:24,646 --> 0:11:25,956
run through the model on the

318
00:11:25,956 --> 0:11:28,466
device and get inference such as

319
00:11:31,516 --> 0:11:31,616
dog.

320
00:11:31,836 --> 0:11:34,126
But what if you are trying to

321
00:11:34,126 --> 0:11:35,496
deal with a concept that is

322
00:11:35,496 --> 0:11:36,526
unique to each user?

323
00:11:37,096 --> 0:11:38,826
Say the concept of My Dog.

324
00:11:39,696 --> 0:11:41,006
You might want users to find

325
00:11:41,006 --> 0:11:42,866
pictures of their own dog in the

326
00:11:42,916 --> 0:11:44,456
photo library and not all dog

327
00:11:44,456 --> 0:11:47,216
photos they've taken.

328
00:11:47,336 --> 0:11:48,696
And each user's dog looks

329
00:11:48,696 --> 0:11:49,156
different.

330
00:11:49,156 --> 0:11:50,426
For example, someone may have

331
00:11:50,486 --> 0:11:51,706
Golden Retriever, someone may

332
00:11:52,196 --> 0:11:54,386
have this Bulldog, ah, and this

333
00:11:54,386 --> 0:11:56,976
is my crazy dog.

334
00:11:58,976 --> 0:12:01,546
So, how do we do that?

335
00:11:58,976 --> 0:12:01,546
So, how do we do that?

336
00:12:02,746 --> 0:12:05,106
So what we want is for user 1 we

337
00:12:05,106 --> 0:12:06,936
need image classifier that takes

338
00:12:06,936 --> 0:12:08,806
this dog and says it's their

339
00:12:08,806 --> 0:12:09,076
dog.

340
00:12:10,286 --> 0:12:12,326
For user 2, that's the English

341
00:12:12,326 --> 0:12:14,906
Bulldog and this the third dog.

342
00:12:18,576 --> 0:12:21,286
So in order to do that, one

343
00:12:21,286 --> 0:12:23,196
approach would be to use Server

344
00:12:23,196 --> 0:12:23,886
Based Approach.

345
00:12:23,916 --> 0:12:26,126
So you may ask each of your

346
00:12:26,636 --> 0:12:30,366
users to upload the photos in

347
00:12:31,836 --> 0:12:32,476
the Cloud.

348
00:12:32,476 --> 0:12:34,496
Server generates model for each

349
00:12:34,496 --> 0:12:37,526
of the users and then sends it

350
00:12:39,056 --> 0:12:39,466
back.

351
00:12:39,466 --> 0:12:41,826
Unfortunately, this approach has

352
00:12:41,826 --> 0:12:42,606
privacy concerns.

353
00:12:42,606 --> 0:12:45,006
Your user may not be very happy

354
00:12:45,266 --> 0:12:47,236
uploading their photo to your

355
00:12:47,236 --> 0:12:48,496
server and you may also not be

356
00:12:48,496 --> 0:12:49,516
okay taking their photo.

357
00:12:50,606 --> 0:12:51,866
You have to setup the server so

358
00:12:51,916 --> 0:12:53,296
there is some cost involved.

359
00:12:53,846 --> 0:12:56,346
Let us say you have a million

360
00:12:56,346 --> 0:12:58,036
users, which we sincerely hope

361
00:12:58,036 --> 0:12:59,396
you do, you have to make

362
00:12:59,396 --> 0:13:00,776
millions such models and keep

363
00:12:59,396 --> 0:13:00,776
millions such models and keep

364
00:13:00,836 --> 0:13:02,966
track of them over time.

365
00:13:05,176 --> 0:13:07,356
With Core ML 3 you can do more

366
00:13:07,356 --> 0:13:09,186
personalization on the device.

367
00:13:09,396 --> 0:13:11,776
So if you have a training data

368
00:13:11,776 --> 0:13:13,886
on the device, you can just

369
00:13:13,886 --> 0:13:15,906
simply use it to fine tune the

370
00:13:15,906 --> 0:13:17,146
model on the device.

371
00:13:17,946 --> 0:13:23,366
So previously you take LMH and

372
00:13:23,366 --> 0:13:27,156
you do the inference and now you

373
00:13:27,156 --> 0:13:28,936
can take the label data feedback

374
00:13:28,936 --> 0:13:30,396
from the user, provide some kind

375
00:13:30,396 --> 0:13:32,386
of training data and fine tune

376
00:13:32,386 --> 0:13:34,276
the model on the device.

377
00:13:35,516 --> 0:13:42,226
[ Applause ]

378
00:13:42,726 --> 0:13:44,256
You have a personalized model

379
00:13:44,406 --> 0:13:46,006
for each user.

380
00:13:48,176 --> 0:13:50,826
We respect user's privacy and

381
00:13:50,826 --> 0:13:52,696
you don't need to put server.

382
00:13:53,386 --> 0:13:56,386
So Core ML 3 supports On-Device

383
00:13:56,386 --> 0:13:58,116
Personalization for Neural

384
00:13:58,936 --> 0:13:59,326
Networks.

385
00:13:59,786 --> 0:14:01,146
We're also supporting Nearest

386
00:13:59,786 --> 0:14:01,146
We're also supporting Nearest

387
00:14:01,276 --> 0:14:03,346
Neighbor and you can do it in

388
00:14:03,346 --> 0:14:05,246
the background at night.

389
00:14:08,536 --> 0:14:10,276
So to summarize and end our

390
00:14:10,356 --> 0:14:13,046
session we saw Create ML a brand

391
00:14:13,046 --> 0:14:16,006
new app to build ML models, we

392
00:14:16,006 --> 0:14:17,556
have a significant expansion in

393
00:14:17,556 --> 0:14:21,126
our Domain APIs and Core ML is

394
00:14:21,196 --> 0:14:22,586
much more flexible and now

395
00:14:22,586 --> 0:14:23,706
supports On-Device

396
00:14:23,706 --> 0:14:24,666
Personalization.

397
00:14:25,206 --> 0:14:29,016
You can find more information on

398
00:14:29,016 --> 0:14:30,796
our Developer Website Session

399
00:14:30,796 --> 0:14:33,966
209 and we hope you are as

400
00:14:33,996 --> 0:14:35,636
excited about these technologies

401
00:14:35,636 --> 0:14:36,176
as we are.

402
00:14:36,566 --> 0:14:37,876
I look forward to seeing you in

403
00:14:37,876 --> 0:14:38,226
the labs.

404
00:14:38,366 --> 0:14:38,706
Thank you.

405
00:14:39,516 --> 0:14:43,500
[ Applause ]
