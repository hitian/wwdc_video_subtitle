1
00:00:00,506 --> 0:00:04,500
[ Music ]

2
00:00:09,516 --> 0:00:12,996
[ Applause ]

3
00:00:13,496 --> 0:00:14,336
&gt;&gt; Hi, everyone.

4
00:00:16,676 --> 0:00:18,306
I'm so excited that you're able

5
00:00:18,306 --> 0:00:19,136
to join us today.

6
00:00:19,546 --> 0:00:21,226
I'm Kayur and I'm here with some

7
00:00:21,286 --> 0:00:23,756
amazing designers, Rubii and

8
00:00:23,756 --> 0:00:24,166
Cas.

9
00:00:24,676 --> 0:00:25,996
And we're so excited to talk to

10
00:00:25,996 --> 0:00:27,716
you today about design and

11
00:00:27,716 --> 0:00:28,366
machine learning.

12
00:00:29,666 --> 0:00:31,686
You see, at Apple, we've been

13
00:00:31,686 --> 0:00:33,326
thinking about design and

14
00:00:33,326 --> 0:00:34,826
machine learning for some time

15
00:00:34,826 --> 0:00:35,036
now.

16
00:00:35,326 --> 0:00:36,886
And when we think about and we

17
00:00:36,886 --> 0:00:38,896
talk about machine learning, we

18
00:00:38,896 --> 0:00:40,226
talk about all the amazing

19
00:00:40,226 --> 0:00:42,056
technological advances in speech

20
00:00:42,056 --> 0:00:43,756
recognition and computer vision.

21
00:00:44,336 --> 0:00:45,896
We talk about deep neural nets.

22
00:00:45,896 --> 0:00:48,306
We talk about custom processing

23
00:00:48,356 --> 0:00:50,016
for on-device machine learning.

24
00:00:50,616 --> 0:00:52,386
We talk about all these things

25
00:00:53,036 --> 0:00:54,566
because we care about products

26
00:00:55,626 --> 0:00:57,566
and we know that many of the

27
00:00:57,606 --> 0:01:00,066
products that we build today and

28
00:00:57,606 --> 0:01:00,066
products that we build today and

29
00:01:00,066 --> 0:01:01,946
many of the products and

30
00:01:01,946 --> 0:01:03,306
experiences that we want to

31
00:01:03,306 --> 0:01:05,296
build in the future would not be

32
00:01:05,296 --> 0:01:07,076
possible without machine

33
00:01:07,076 --> 0:01:07,386
learning.

34
00:01:08,556 --> 0:01:09,816
Let's take a look at some of the

35
00:01:11,056 --> 0:01:11,366
products.

36
00:01:11,696 --> 0:01:12,226
AirPods.

37
00:01:12,676 --> 0:01:14,946
AirPods allow you to summon Siri

38
00:01:15,266 --> 0:01:16,736
to control your device with your

39
00:01:16,736 --> 0:01:17,056
voice.

40
00:01:17,766 --> 0:01:19,256
You can easily change your music

41
00:01:19,256 --> 0:01:21,106
or answer a call all when your

42
00:01:21,106 --> 0:01:22,796
hands are busy and when you

43
00:01:22,796 --> 0:01:24,206
can't really look at the screen.

44
00:01:24,996 --> 0:01:26,466
This AirPods experience would

45
00:01:26,466 --> 0:01:27,986
not be possible without machine

46
00:01:27,986 --> 0:01:29,466
learning and it would not be

47
00:01:29,466 --> 0:01:30,396
possible without Siri.

48
00:01:31,686 --> 0:01:33,106
Machine learning allows us to

49
00:01:33,106 --> 0:01:35,096
create Face ID which provides

50
00:01:35,096 --> 0:01:36,966
fast secure authentication for

51
00:01:36,966 --> 0:01:38,026
your iOS devices.

52
00:01:38,536 --> 0:01:41,256
And we use machine learning in

53
00:01:41,256 --> 0:01:43,226
ways that you may not expect,

54
00:01:43,766 --> 0:01:47,376
subtle ways that improve your

55
00:01:47,376 --> 0:01:48,846
experience with your devices.

56
00:01:49,716 --> 0:01:50,946
We use machine learning to

57
00:01:50,946 --> 0:01:52,836
invisibly improve your typing

58
00:01:52,836 --> 0:01:53,386
experience.

59
00:01:54,196 --> 0:01:56,056
We increase or decrease the

60
00:01:56,056 --> 0:01:57,626
target tap area for keyboard

61
00:01:57,656 --> 0:01:59,256
buttons based on the word you're

62
00:01:59,256 --> 0:02:00,076
most likely to type.

63
00:01:59,256 --> 0:02:00,076
most likely to type.

64
00:02:01,306 --> 0:02:02,586
Machine learning applications

65
00:02:02,586 --> 0:02:03,376
are diverse.

66
00:02:04,196 --> 0:02:05,576
They're not just about voice

67
00:02:05,576 --> 0:02:07,416
assistance or recommending

68
00:02:07,416 --> 0:02:09,246
content or self-driving cars,

69
00:02:10,316 --> 0:02:11,626
machine learning applications

70
00:02:11,676 --> 0:02:13,536
are as diverse as any

71
00:02:13,536 --> 0:02:14,576
application of software

72
00:02:14,576 --> 0:02:15,206
engineering.

73
00:02:16,036 --> 0:02:18,016
They can make existing products

74
00:02:18,166 --> 0:02:20,316
better or enable new products

75
00:02:20,316 --> 0:02:21,046
that enrich the human

76
00:02:21,046 --> 0:02:21,596
experience.

77
00:02:22,586 --> 0:02:24,786
And inside Apple, we've been

78
00:02:24,786 --> 0:02:26,566
talking about and thinking about

79
00:02:27,066 --> 0:02:28,776
what makes a great machine

80
00:02:28,776 --> 0:02:30,026
learning product for some time

81
00:02:30,026 --> 0:02:30,226
now.

82
00:02:31,216 --> 0:02:32,176
And to go into this in more

83
00:02:32,176 --> 0:02:34,376
detail, let's dive into a single

84
00:02:34,376 --> 0:02:35,626
experience, Photos.

85
00:02:36,656 --> 0:02:38,076
Photos uses machine learning in

86
00:02:38,076 --> 0:02:39,026
a variety of ways.

87
00:02:39,616 --> 0:02:40,766
Machine learning helps people

88
00:02:41,006 --> 0:02:43,086
create albums, edit photos and

89
00:02:43,086 --> 0:02:44,396
search for specific memories.

90
00:02:45,166 --> 0:02:48,796
Speaking of memories, recently I

91
00:02:48,796 --> 0:02:49,796
was looking for this memory.

92
00:02:50,506 --> 0:02:51,256
This is Angie.

93
00:02:51,626 --> 0:02:53,216
She's a friend of the family and

94
00:02:53,216 --> 0:02:54,226
this picture was taken from a

95
00:02:54,226 --> 0:02:55,316
hike a few years ago.

96
00:02:56,126 --> 0:02:58,116
I felt nostalgic and wanted to

97
00:02:58,116 --> 0:02:59,126
share the picture with some

98
00:02:59,126 --> 0:03:00,386
friends who were already there.

99
00:02:59,126 --> 0:03:00,386
friends who were already there.

100
00:03:00,806 --> 0:03:03,136
Until very recently, finding a

101
00:03:03,136 --> 0:03:04,566
specific picture was painful.

102
00:03:05,396 --> 0:03:06,526
I'd have to sift through

103
00:03:06,526 --> 0:03:08,506
thousands of pictures, try and

104
00:03:08,506 --> 0:03:10,056
remember where I was or when I

105
00:03:10,056 --> 0:03:10,706
took the picture.

106
00:03:11,256 --> 0:03:12,036
I would've given up.

107
00:03:12,776 --> 0:03:14,036
Finding a picture used to be a

108
00:03:14,036 --> 0:03:16,336
barrier to me engaging with my

109
00:03:16,336 --> 0:03:17,696
memories and sharing them with

110
00:03:17,726 --> 0:03:18,326
my family.

111
00:03:19,326 --> 0:03:22,046
In iOS X, Apple introduced a new

112
00:03:22,046 --> 0:03:23,186
search experience that could

113
00:03:23,186 --> 0:03:25,006
like inside a picture and

114
00:03:25,006 --> 0:03:26,356
detect, among other things,

115
00:03:26,356 --> 0:03:26,706
dogs.

116
00:03:27,326 --> 0:03:28,686
And over the years, we've been

117
00:03:28,686 --> 0:03:29,946
making this experience better.

118
00:03:31,366 --> 0:03:32,456
Here's what the experience looks

119
00:03:32,456 --> 0:03:32,766
like now.

120
00:03:33,586 --> 0:03:35,776
I tap on the Search box.

121
00:03:35,776 --> 0:03:37,366
I type in dog.

122
00:03:37,366 --> 0:03:39,086
And I get a list of pictures

123
00:03:39,086 --> 0:03:39,496
with dogs.

124
00:03:39,496 --> 0:03:41,006
I can then select the picture I

125
00:03:41,006 --> 0:03:42,296
was looking for and share it.

126
00:03:42,556 --> 0:03:43,236
Easy peasy.

127
00:03:44,406 --> 0:03:45,976
Searching for pictures based on

128
00:03:45,976 --> 0:03:48,016
what's in a picture changes how

129
00:03:48,016 --> 0:03:49,176
we engage with our memories.

130
00:03:49,926 --> 0:03:50,746
It's something we take for

131
00:03:50,746 --> 0:03:51,166
granted.

132
00:03:51,856 --> 0:03:52,966
In fact, it's such a great

133
00:03:52,966 --> 0:03:54,556
experience it's hard to remember

134
00:03:54,836 --> 0:03:55,976
a time before we had this

135
00:03:55,976 --> 0:03:56,606
capability.

136
00:03:57,286 --> 0:03:58,096
It's well designed.

137
00:03:59,046 --> 0:04:00,736
And there's a lot of interface

138
00:03:59,046 --> 0:04:00,736
And there's a lot of interface

139
00:04:00,736 --> 0:04:02,656
work that goes into making this

140
00:04:02,656 --> 0:04:03,236
experience.

141
00:04:03,916 --> 0:04:05,446
Photos suggest categories you

142
00:04:05,446 --> 0:04:06,776
can search for, it

143
00:04:06,776 --> 0:04:08,546
auto-completes searches and it

144
00:04:08,546 --> 0:04:09,706
connects search results to

145
00:04:09,706 --> 0:04:12,276
Moments but designing this

146
00:04:12,276 --> 0:04:13,916
experience is more than just

147
00:04:13,916 --> 0:04:15,836
designing a grid view in the

148
00:04:15,836 --> 0:04:16,456
Search field.

149
00:04:17,086 --> 0:04:18,106
If we just looked at the

150
00:04:18,106 --> 0:04:19,916
interface, we'd miss the most

151
00:04:19,916 --> 0:04:21,016
important parts of the search

152
00:04:21,016 --> 0:04:23,166
experience, the results being

153
00:04:23,166 --> 0:04:25,146
outputted into categories that

154
00:04:25,146 --> 0:04:26,006
people can search for.

155
00:04:27,086 --> 0:04:29,316
These aspects of the experience

156
00:04:29,316 --> 0:04:30,276
should be designed.

157
00:04:30,846 --> 0:04:32,216
We get to decide which

158
00:04:32,216 --> 0:04:34,076
categories are included and we

159
00:04:34,076 --> 0:04:35,756
decide the level of quality we

160
00:04:35,756 --> 0:04:38,576
want for each category.

161
00:04:38,646 --> 0:04:40,256
We've realized that to build a

162
00:04:40,256 --> 0:04:41,116
good machine learning

163
00:04:41,116 --> 0:04:42,886
experience, we have to design

164
00:04:42,886 --> 0:04:44,176
more than just the interface.

165
00:04:44,766 --> 0:04:45,276
What do we mean?

166
00:04:46,606 --> 0:04:47,996
Well, when most people think

167
00:04:47,996 --> 0:04:49,576
about design, they think about

168
00:04:49,576 --> 0:04:51,206
the interface, how the product

169
00:04:51,206 --> 0:04:53,236
looks and feels, the flow of the

170
00:04:53,236 --> 0:04:53,846
experience.

171
00:04:54,526 --> 0:04:57,446
With machine learning, we have

172
00:04:57,516 --> 0:04:59,576
to design how the product works.

173
00:05:00,596 --> 0:05:01,796
Now we know that many of you in

174
00:05:01,796 --> 0:05:02,916
the design and developer

175
00:05:02,916 --> 0:05:04,626
community have some machine

176
00:05:04,626 --> 0:05:05,456
learning experience.

177
00:05:05,906 --> 0:05:06,826
You could create a search

178
00:05:06,826 --> 0:05:08,696
experience from scratch but to

179
00:05:08,696 --> 0:05:10,066
get everyone on the same page,

180
00:05:10,436 --> 0:05:11,646
let's look at how machine

181
00:05:11,646 --> 0:05:12,946
learning for photo search works.

182
00:05:14,596 --> 0:05:16,536
Deep within the logic of Photos

183
00:05:16,916 --> 0:05:18,436
is a function that takes a

184
00:05:18,436 --> 0:05:20,346
picture and detects if that

185
00:05:20,346 --> 0:05:21,626
picture contains any of the

186
00:05:21,626 --> 0:05:22,976
categories it can recognize.

187
00:05:23,836 --> 0:05:25,206
In our case, it recognizes that

188
00:05:25,256 --> 0:05:26,626
this picture of Angie has a dog

189
00:05:26,626 --> 0:05:27,506
in it.

190
00:05:28,076 --> 0:05:30,006
In traditional programming, we

191
00:05:30,006 --> 0:05:30,986
would have to create the

192
00:05:30,986 --> 0:05:33,596
function by writing code to tell

193
00:05:33,596 --> 0:05:34,906
the computer what to do.

194
00:05:35,986 --> 0:05:37,536
Our code would have to work for

195
00:05:37,746 --> 0:05:39,286
a variety of different breeds,

196
00:05:39,766 --> 0:05:42,566
different scenarios, different

197
00:05:42,806 --> 0:05:44,626
photo resolutions because their

198
00:05:44,626 --> 0:05:46,256
customers are diverse and so are

199
00:05:46,256 --> 0:05:46,836
their dogs.

200
00:05:47,776 --> 0:05:48,956
And we wouldn't want a search

201
00:05:48,956 --> 0:05:50,196
for dogs to be littered with

202
00:05:50,196 --> 0:05:51,336
pictures of other animals.

203
00:05:51,956 --> 0:05:52,796
Our code would have to

204
00:05:52,796 --> 0:05:54,696
differentiate between dogs and

205
00:05:54,696 --> 0:05:56,576
similar animals, like this hyena

206
00:05:56,966 --> 0:05:58,416
which looks like a dog but you

207
00:05:58,416 --> 0:06:00,446
should not have as a pet.

208
00:05:58,416 --> 0:06:00,446
should not have as a pet.

209
00:06:01,426 --> 0:06:03,656
Writing code that generalizes

210
00:06:03,656 --> 0:06:05,196
the countless variations of dogs

211
00:06:05,196 --> 0:06:06,776
in photos would be impossible.

212
00:06:07,176 --> 0:06:08,516
And dogs are just one of

213
00:06:08,516 --> 0:06:09,776
thousands of categories that

214
00:06:09,776 --> 0:06:10,766
Photos can recognize.

215
00:06:11,846 --> 0:06:13,476
There're experiences we want to

216
00:06:13,476 --> 0:06:15,096
create where we can't just tell

217
00:06:15,096 --> 0:06:16,076
the computer what to do.

218
00:06:17,456 --> 0:06:18,306
And for many of these

219
00:06:18,306 --> 0:06:20,256
experiences, we can use machine

220
00:06:20,256 --> 0:06:22,076
learning to teach a computer

221
00:06:22,076 --> 0:06:22,606
what to do.

222
00:06:23,946 --> 0:06:25,416
We teach by providing examples.

223
00:06:25,416 --> 0:06:26,526
If we wanted to distinguish

224
00:06:26,526 --> 0:06:28,086
pictures of dogs from pictures

225
00:06:28,086 --> 0:06:29,936
that do not contain dogs, we

226
00:06:29,936 --> 0:06:31,296
have to provide pictures with

227
00:06:31,296 --> 0:06:32,706
dogs and pictures without dogs.

228
00:06:33,566 --> 0:06:35,786
Machine learning learns a

229
00:06:35,956 --> 0:06:37,856
function that we can later use

230
00:06:37,856 --> 0:06:39,186
in our app.

231
00:06:39,856 --> 0:06:41,366
This function can take this

232
00:06:41,366 --> 0:06:42,966
picture of Angie and understand

233
00:06:42,966 --> 0:06:43,916
that it's a picture of a dog.

234
00:06:44,796 --> 0:06:46,776
We call this function a model

235
00:06:47,436 --> 0:06:50,296
and a model is what makes the

236
00:06:50,296 --> 0:06:53,026
Photos Search experience work.

237
00:06:54,316 --> 0:06:56,436
This model can generalize two

238
00:06:56,636 --> 0:06:57,576
pictures that it's never seen

239
00:06:57,576 --> 0:06:57,886
before.

240
00:06:58,326 --> 0:06:59,686
It can recognize different dog

241
00:06:59,736 --> 0:07:01,766
species and differentiate dogs

242
00:06:59,736 --> 0:07:01,766
species and differentiate dogs

243
00:07:01,766 --> 0:07:03,406
from other animals and objects.

244
00:07:04,616 --> 0:07:06,226
Models are central to the

245
00:07:06,226 --> 0:07:07,246
machine learning experience.

246
00:07:07,896 --> 0:07:09,876
Every product or experience that

247
00:07:09,876 --> 0:07:11,026
depends on machine learning

248
00:07:11,026 --> 0:07:11,896
depends on a model.

249
00:07:12,796 --> 0:07:14,736
Siri has a model that converts

250
00:07:14,736 --> 0:07:15,436
your voice to text.

251
00:07:16,176 --> 0:07:17,986
And the keyboard has a model

252
00:07:17,986 --> 0:07:19,686
that infers the key you meant to

253
00:07:19,686 --> 0:07:20,726
type based on the letters you've

254
00:07:20,726 --> 0:07:21,776
already typed and your typing

255
00:07:21,776 --> 0:07:21,976
history.

256
00:07:27,176 --> 0:07:29,506
To design a great machine

257
00:07:29,506 --> 0:07:31,776
learning experience, we have to

258
00:07:31,776 --> 0:07:35,226
design both how it works and how

259
00:07:35,226 --> 0:07:35,966
it looks and feels.

260
00:07:36,586 --> 0:07:38,606
We have to design both the model

261
00:07:38,606 --> 0:07:39,486
and the interface.

262
00:07:40,216 --> 0:07:41,696
In this talk, we'll cover both.

263
00:07:43,166 --> 0:07:44,006
Creating a model can be

264
00:07:44,006 --> 0:07:44,746
complicated.

265
00:07:44,746 --> 0:07:46,456
You have to make a lot of

266
00:07:46,456 --> 0:07:48,256
decisions about algorithms and

267
00:07:48,256 --> 0:07:49,406
parameters and frameworks.

268
00:07:49,906 --> 0:07:51,536
And all of these decisions

269
00:07:51,666 --> 0:07:53,736
affect what the model does and,

270
00:07:53,736 --> 0:07:54,696
therefore, the experience.

271
00:07:55,696 --> 0:07:57,506
Machine learning decisions are

272
00:07:57,506 --> 0:08:00,646
all design decisions but not all

273
00:07:57,506 --> 0:08:00,646
all design decisions but not all

274
00:08:00,646 --> 0:08:02,336
of them are effective places for

275
00:08:02,336 --> 0:08:03,026
design input.

276
00:08:03,816 --> 0:08:05,836
We choose a few places where we

277
00:08:05,836 --> 0:08:07,236
think design can have the most

278
00:08:07,856 --> 0:08:08,046
impact.

279
00:08:09,116 --> 0:08:11,236
We talk about the need to design

280
00:08:11,436 --> 0:08:12,616
what we use to teach the

281
00:08:12,616 --> 0:08:13,076
computer.

282
00:08:13,426 --> 0:08:14,056
This is the data.

283
00:08:14,966 --> 0:08:17,176
And we also talk about how we

284
00:08:17,176 --> 0:08:18,936
design what we evaluate and

285
00:08:19,756 --> 0:08:22,736
these are the metrics.

286
00:08:22,796 --> 0:08:24,756
Along with the model, we do need

287
00:08:24,756 --> 0:08:25,776
to design the interface.

288
00:08:26,726 --> 0:08:27,886
We have to design what the model

289
00:08:27,886 --> 0:08:29,566
outputs and how those model's

290
00:08:29,566 --> 0:08:31,066
outputs are presented to people.

291
00:08:31,916 --> 0:08:33,336
We also have to design how

292
00:08:33,336 --> 0:08:34,616
people interact with the model

293
00:08:34,826 --> 0:08:36,905
and, if appropriate, provide

294
00:08:36,905 --> 0:08:38,385
input to improve the model.

295
00:08:38,946 --> 0:08:40,466
Rubii and Cas will talk about

296
00:08:40,466 --> 0:08:41,000
that later.

297
00:08:42,905 --> 0:08:44,476
So let's start.

298
00:08:45,076 --> 0:08:47,926
Let's talk about the data.

299
00:08:50,476 --> 0:08:52,366
To recognize dogs in pictures,

300
00:08:52,706 --> 0:08:54,896
we used machine learning to

301
00:08:54,896 --> 0:08:56,606
create a model using examples.

302
00:08:57,926 --> 0:08:59,576
Those examples are called data.

303
00:09:00,176 --> 0:09:02,596
And to build a good search

304
00:09:02,596 --> 0:09:04,146
experience, you need a lot of

305
00:09:04,146 --> 0:09:04,686
diverse data.

306
00:09:05,356 --> 0:09:06,886
You need photos of the

307
00:09:06,886 --> 0:09:08,386
categories that you want people

308
00:09:08,386 --> 0:09:09,436
to be able to search for.

309
00:09:10,176 --> 0:09:11,516
You need to provide plenty of

310
00:09:11,516 --> 0:09:13,736
pictures of dogs and of other

311
00:09:13,736 --> 0:09:15,636
animals to ensure the search

312
00:09:15,636 --> 0:09:17,676
experience doesn't show random

313
00:09:17,676 --> 0:09:18,626
animals when you're searching

314
00:09:18,626 --> 0:09:19,086
for dogs.

315
00:09:19,686 --> 0:09:21,396
And a good search experience

316
00:09:21,486 --> 0:09:23,036
needs to support thousands of

317
00:09:23,036 --> 0:09:24,556
categories that cover the

318
00:09:24,606 --> 0:09:25,976
objects and events people want

319
00:09:25,976 --> 0:09:26,486
to search for.

320
00:09:27,246 --> 0:09:28,186
And for each of those

321
00:09:28,186 --> 0:09:29,526
categories, you'll need data.

322
00:09:29,646 --> 0:09:31,226
If you want to support a new

323
00:09:31,226 --> 0:09:32,566
category, you'll need data.

324
00:09:33,326 --> 0:09:34,166
If you want to improve an

325
00:09:34,166 --> 0:09:35,916
existing category, you will need

326
00:09:35,916 --> 0:09:36,336
data.

327
00:09:36,956 --> 0:09:39,036
Choosing data is key to

328
00:09:39,036 --> 0:09:40,186
designing the experience.

329
00:09:41,676 --> 0:09:43,696
Data determines the behavior of

330
00:09:43,696 --> 0:09:44,286
a model.

331
00:09:44,646 --> 0:09:46,246
It's easily the most important

332
00:09:46,246 --> 0:09:47,246
decision you'll make when

333
00:09:47,246 --> 0:09:48,056
creating your model.

334
00:09:48,726 --> 0:09:50,086
And if you don't have data that

335
00:09:50,596 --> 0:09:52,206
captures an important scenario,

336
00:09:52,656 --> 0:09:54,546
it's unlikely that your model's

337
00:09:54,546 --> 0:09:55,386
going to work well in that

338
00:09:55,386 --> 0:09:55,956
scenario.

339
00:09:56,496 --> 0:09:59,336
And since data determines the

340
00:09:59,336 --> 0:10:01,396
behavior of the model, and since

341
00:09:59,336 --> 0:10:01,396
behavior of the model, and since

342
00:10:01,396 --> 0:10:02,716
the behavior of the model

343
00:10:02,936 --> 0:10:05,066
determines the experience, data

344
00:10:05,066 --> 0:10:07,546
needs to be designed to reflect

345
00:10:07,656 --> 0:10:09,276
good values in the best interest

346
00:10:09,276 --> 0:10:10,106
of your customers.

347
00:10:11,336 --> 0:10:12,846
To understand this better, let's

348
00:10:12,846 --> 0:10:13,656
look at an example.

349
00:10:14,806 --> 0:10:15,786
Let's look at Portrait mode.

350
00:10:17,326 --> 0:10:19,276
Portrait mode uses machine

351
00:10:19,276 --> 0:10:21,636
learning to detect faces and

352
00:10:21,636 --> 0:10:22,896
segment your body from the

353
00:10:22,896 --> 0:10:23,406
background.

354
00:10:24,776 --> 0:10:26,846
Historically, face recognition

355
00:10:26,846 --> 0:10:28,516
hasn't worked well for people of

356
00:10:28,516 --> 0:10:28,886
color.

357
00:10:29,546 --> 0:10:31,486
At Apple, we want to make sure

358
00:10:31,486 --> 0:10:33,406
our experiences are inclusive.

359
00:10:34,066 --> 0:10:35,556
So our design and engineering

360
00:10:35,556 --> 0:10:37,666
teams gather data from across

361
00:10:37,736 --> 0:10:39,406
different races, different

362
00:10:39,406 --> 0:10:40,656
cultures, and different

363
00:10:40,656 --> 0:10:41,196
scenarios.

364
00:10:41,916 --> 0:10:43,666
We built a dataset that matched

365
00:10:43,666 --> 0:10:45,776
the experience we wanted to

366
00:10:45,776 --> 0:10:46,646
create.

367
00:10:47,316 --> 0:10:49,366
We collected data intentionally.

368
00:10:50,186 --> 0:10:51,106
When collecting data, it's

369
00:10:51,106 --> 0:10:52,266
important to ask questions.

370
00:10:52,836 --> 0:10:54,306
Who is collecting the data?

371
00:10:54,786 --> 0:10:56,056
What data are they collecting?

372
00:10:56,316 --> 0:10:57,336
And how are they doing it?

373
00:10:58,076 --> 0:10:59,886
Data bias can seep into your

374
00:10:59,886 --> 0:11:01,416
dataset in a variety of ways.

375
00:10:59,886 --> 0:11:01,416
dataset in a variety of ways.

376
00:11:01,996 --> 0:11:03,976
And data bias extends beyond

377
00:11:03,976 --> 0:11:04,336
fairness.

378
00:11:05,316 --> 0:11:07,056
You should strive to be fair and

379
00:11:07,056 --> 0:11:07,516
inclusive.

380
00:11:07,836 --> 0:11:08,966
Those are good values.

381
00:11:09,326 --> 0:11:11,576
But you should also see how data

382
00:11:11,576 --> 0:11:13,276
collection might skew away from

383
00:11:13,276 --> 0:11:14,266
other design intents.

384
00:11:15,046 --> 0:11:16,596
If you're creating a fun or

385
00:11:16,596 --> 0:11:18,396
enriching experience, you should

386
00:11:18,396 --> 0:11:20,406
sample data for fun, enriching

387
00:11:20,406 --> 0:11:21,076
experiences.

388
00:11:21,846 --> 0:11:23,036
If your product is going to be

389
00:11:23,036 --> 0:11:25,166
used outdoors, you should sample

390
00:11:25,166 --> 0:11:26,006
from scenarios that are

391
00:11:26,006 --> 0:11:26,656
outdoors.

392
00:11:27,806 --> 0:11:29,446
It's tempting to disconnect from

393
00:11:29,446 --> 0:11:31,146
this decision and make decisions

394
00:11:31,146 --> 0:11:33,566
about data collection that just

395
00:11:33,566 --> 0:11:35,146
sample the data from your

396
00:11:35,146 --> 0:11:37,126
existing customers and get a

397
00:11:37,126 --> 0:11:38,816
uniform sample of the world from

398
00:11:38,816 --> 0:11:41,776
other mechanisms but you can't

399
00:11:41,776 --> 0:11:43,256
just reflect the world.

400
00:11:43,706 --> 0:11:45,116
Reflecting the world can

401
00:11:45,116 --> 0:11:46,456
reinforce systemic biases.

402
00:11:47,336 --> 0:11:48,506
You shouldn't optimize for the

403
00:11:48,506 --> 0:11:50,076
customers you have.

404
00:11:50,526 --> 0:11:51,526
You should optimize for the

405
00:11:51,526 --> 0:11:52,736
customers you want.

406
00:11:53,696 --> 0:11:55,176
You shouldn't reflect the world

407
00:11:55,176 --> 0:11:55,646
as it is.

408
00:11:55,696 --> 0:11:57,756
You should reflect a better

409
00:11:57,756 --> 0:11:59,336
world, that world that you want

410
00:11:59,336 --> 0:11:59,636
it to be.

411
00:12:00,696 --> 0:12:02,806
Collecting data is a way to

412
00:12:02,806 --> 0:12:05,526
design experiences and great

413
00:12:05,526 --> 0:12:07,306
experiences often change the

414
00:12:07,306 --> 0:12:07,706
world.

415
00:12:08,196 --> 0:12:09,536
They don't just reflect what

416
00:12:09,536 --> 0:12:10,426
already exists.

417
00:12:11,886 --> 0:12:13,166
So how do we put this into

418
00:12:13,166 --> 0:12:14,000
practice?

419
00:12:15,676 --> 0:12:17,796
We'll start by collecting data

420
00:12:17,796 --> 0:12:18,506
intentionally.

421
00:12:19,226 --> 0:12:20,876
Understand the experience you're

422
00:12:20,876 --> 0:12:22,526
trying to create and think about

423
00:12:22,526 --> 0:12:23,646
what you'll need to make that

424
00:12:23,646 --> 0:12:24,496
experience work.

425
00:12:25,326 --> 0:12:26,426
If specific scenarios are

426
00:12:26,426 --> 0:12:28,096
important, spend more time and

427
00:12:28,096 --> 0:12:29,296
ensure you collect data for

428
00:12:29,296 --> 0:12:30,046
those scenarios.

429
00:12:31,126 --> 0:12:32,896
Spending time up front

430
00:12:33,926 --> 0:12:35,976
collecting the right data can

431
00:12:35,976 --> 0:12:37,836
save time and effort and money

432
00:12:37,836 --> 0:12:38,166
later.

433
00:12:38,726 --> 0:12:41,566
Make sure you test for biases.

434
00:12:42,266 --> 0:12:43,516
Analyze your dataset.

435
00:12:44,036 --> 0:12:45,656
Catalogue your assumptions about

436
00:12:45,856 --> 0:12:47,106
who will use your product and

437
00:12:47,106 --> 0:12:48,086
how it will be used.

438
00:12:48,796 --> 0:12:50,126
Think of ways your data might be

439
00:12:50,126 --> 0:12:51,806
biased towards or against

440
00:12:51,806 --> 0:12:53,266
certain populations or certain

441
00:12:53,266 --> 0:12:54,016
experiences.

442
00:12:54,486 --> 0:12:58,446
Update data as products change.

443
00:12:59,436 --> 0:13:01,476
Product specifications change

444
00:12:59,436 --> 0:13:01,476
Product specifications change

445
00:13:01,476 --> 0:13:02,446
the more you learn about your

446
00:13:02,446 --> 0:13:04,566
customers or the market or what

447
00:13:04,566 --> 0:13:05,896
you want to actually create.

448
00:13:06,946 --> 0:13:08,106
You need to change your data to

449
00:13:08,106 --> 0:13:08,616
match.

450
00:13:09,236 --> 0:13:10,326
You may not need to collect more

451
00:13:10,326 --> 0:13:10,596
data.

452
00:13:11,206 --> 0:13:12,926
You may get rid of concepts that

453
00:13:12,926 --> 0:13:15,066
are not important but you must

454
00:13:15,066 --> 0:13:17,476
routinely align your data with

455
00:13:17,476 --> 0:13:18,506
your product goals.

456
00:13:19,006 --> 0:13:22,926
And beware of standard datasets.

457
00:13:23,286 --> 0:13:25,006
Academic or industry benchmark

458
00:13:25,006 --> 0:13:27,386
datasets might be a great way to

459
00:13:27,386 --> 0:13:29,266
start to get some knowledge

460
00:13:29,266 --> 0:13:30,266
about how machine learning

461
00:13:30,266 --> 0:13:30,596
works.

462
00:13:30,926 --> 0:13:32,196
It might be a great way to ramp

463
00:13:32,196 --> 0:13:33,516
up your product development

464
00:13:33,516 --> 0:13:34,066
process.

465
00:13:34,706 --> 0:13:35,986
But they're not designed to

466
00:13:35,986 --> 0:13:37,736
represent real experiences,

467
00:13:38,276 --> 0:13:39,666
especially your experience.

468
00:13:40,396 --> 0:13:41,596
Before you start using an

469
00:13:41,596 --> 0:13:43,326
off-the-shelf dataset, think

470
00:13:43,326 --> 0:13:44,506
about what the data covers and

471
00:13:44,506 --> 0:13:45,116
doesn't cover.

472
00:13:45,556 --> 0:13:47,276
Spend time augmenting the data

473
00:13:47,276 --> 0:13:48,666
to match your needs.

474
00:13:49,936 --> 0:13:52,926
Spending time thinking about

475
00:13:53,516 --> 0:13:55,916
cataloguing and critically

476
00:13:56,196 --> 0:13:58,376
collecting data can help you

477
00:13:58,376 --> 0:13:59,976
align what you want your product

478
00:13:59,976 --> 0:14:02,156
to be and what machine learning

479
00:13:59,976 --> 0:14:02,156
to be and what machine learning

480
00:14:02,156 --> 0:14:02,726
can provide.

481
00:14:03,226 --> 0:14:06,906
So, we just talked about data,

482
00:14:06,966 --> 0:14:08,376
which is what you use to teach a

483
00:14:08,376 --> 0:14:08,756
model.

484
00:14:10,166 --> 0:14:11,576
Next, let's talk about how you

485
00:14:11,576 --> 0:14:12,396
evaluate your model.

486
00:14:13,066 --> 0:14:14,906
This is done through metrics.

487
00:14:15,456 --> 0:14:18,376
You evaluate your model by

488
00:14:18,376 --> 0:14:18,946
testing it.

489
00:14:19,276 --> 0:14:20,566
In the Photo Search example,

490
00:14:21,116 --> 0:14:22,226
you'd give it pictures of

491
00:14:22,226 --> 0:14:24,206
animals and your model would

492
00:14:24,206 --> 0:14:25,476
predict what's in the pictures.

493
00:14:25,776 --> 0:14:27,076
And once it's done, you can

494
00:14:27,076 --> 0:14:28,536
compare how many times it was

495
00:14:28,886 --> 0:14:31,006
correct and how many times it

496
00:14:31,006 --> 0:14:31,786
was incorrect.

497
00:14:32,406 --> 0:14:34,676
Here, we see that the model gave

498
00:14:34,676 --> 0:14:36,656
the correct prediction 75% of

499
00:14:36,656 --> 0:14:37,016
the time.

500
00:14:37,686 --> 0:14:39,526
This is a metric and a metric

501
00:14:39,676 --> 0:14:41,846
like this will define how

502
00:14:41,846 --> 0:14:43,686
successful your model is and,

503
00:14:43,686 --> 0:14:45,216
therefore, whether it's ready to

504
00:14:45,216 --> 0:14:46,706
use or whether you have to go

505
00:14:46,706 --> 0:14:47,786
back to the drawing board and

506
00:14:47,786 --> 0:14:48,676
improve it.

507
00:14:49,146 --> 0:14:51,366
Models can be evaluated in many

508
00:14:51,366 --> 0:14:51,776
ways.

509
00:14:52,156 --> 0:14:53,346
You could look at how fast the

510
00:14:53,346 --> 0:14:54,846
model works or how many

511
00:14:54,846 --> 0:14:55,936
categories it can support.

512
00:14:56,316 --> 0:14:57,576
You can balance all of these

513
00:14:57,576 --> 0:14:59,286
together and find the right set

514
00:14:59,286 --> 0:15:00,846
of metrics for your problem.

515
00:14:59,286 --> 0:15:00,846
of metrics for your problem.

516
00:15:02,106 --> 0:15:04,096
Designing metrics define how the

517
00:15:04,096 --> 0:15:05,526
model will work and, therefore,

518
00:15:05,526 --> 0:15:06,956
how the experience will work but

519
00:15:07,136 --> 0:15:10,966
ultimately you get to decide.

520
00:15:11,076 --> 0:15:12,886
Metrics encode what you think a

521
00:15:12,886 --> 0:15:13,836
good experience is.

522
00:15:14,296 --> 0:15:15,776
They define what you care about

523
00:15:15,776 --> 0:15:16,816
and what you can ignore.

524
00:15:17,906 --> 0:15:19,476
If part of the experience is not

525
00:15:19,476 --> 0:15:21,246
measured, it may be sacrificed

526
00:15:21,306 --> 0:15:22,436
for something that is.

527
00:15:23,586 --> 0:15:25,326
And as a consequence, your

528
00:15:25,326 --> 0:15:28,416
metrics reflect your values.

529
00:15:28,886 --> 0:15:30,436
Let's look at an example.

530
00:15:31,806 --> 0:15:32,826
Let's look at Face ID.

531
00:15:34,256 --> 0:15:36,296
Face ID uses machine learning to

532
00:15:36,296 --> 0:15:38,376
detect your face to unlock your

533
00:15:38,376 --> 0:15:38,726
device.

534
00:15:40,236 --> 0:15:42,136
And behind Face ID are a number

535
00:15:42,136 --> 0:15:43,126
of design intents.

536
00:15:43,806 --> 0:15:45,026
The most important is Face ID

537
00:15:45,026 --> 0:15:46,426
needs to be secure.

538
00:15:46,786 --> 0:15:48,546
Our customers trust us with some

539
00:15:48,546 --> 0:15:49,676
of their most private data.

540
00:15:50,116 --> 0:15:51,856
There are many metrics that we

541
00:15:51,856 --> 0:15:53,946
use to track different aspects

542
00:15:53,946 --> 0:15:54,576
of security.

543
00:15:55,256 --> 0:15:56,776
One key metric was the chance

544
00:15:56,806 --> 0:15:58,666
that a random person could find

545
00:15:58,666 --> 0:16:00,206
your phone, pick it up and

546
00:15:58,666 --> 0:16:00,206
your phone, pick it up and

547
00:16:00,206 --> 0:16:00,746
unlock it.

548
00:16:01,626 --> 0:16:02,816
We tracked this metric and

549
00:16:02,816 --> 0:16:05,146
worked hard to reduce this

550
00:16:05,826 --> 0:16:05,976
chance.

551
00:16:06,136 --> 0:16:07,856
At launch, we knew that there

552
00:16:07,856 --> 0:16:09,316
was a one-in-a-million chance

553
00:16:09,316 --> 0:16:10,916
that a random person could

554
00:16:10,916 --> 0:16:12,526
unlock your phone using Face ID.

555
00:16:12,976 --> 0:16:14,976
And since Face ID was a new

556
00:16:14,976 --> 0:16:16,526
experience, we needed to

557
00:16:16,526 --> 0:16:17,886
communicate this to our

558
00:16:17,886 --> 0:16:18,356
customers.

559
00:16:19,116 --> 0:16:20,586
That way they understood that we

560
00:16:20,586 --> 0:16:21,646
cared about their data and they

561
00:16:21,646 --> 0:16:23,106
could actually trust Face ID.

562
00:16:23,916 --> 0:16:25,786
But one in a million doesn't

563
00:16:25,786 --> 0:16:26,746
tell the whole story.

564
00:16:27,766 --> 0:16:30,066
Each failure is a person, a

565
00:16:30,066 --> 0:16:32,106
scenario, an experience that we

566
00:16:32,106 --> 0:16:35,286
have to think about and address.

567
00:16:35,466 --> 0:16:36,256
Say you're me.

568
00:16:36,606 --> 0:16:38,786
I'm your average run-of-the-mill

569
00:16:38,916 --> 0:16:43,296
Apple employee and recently I

570
00:16:43,296 --> 0:16:46,216
had a run-in with the new

571
00:16:46,216 --> 0:16:47,056
MacBook Pro.

572
00:16:47,366 --> 0:16:48,916
Due to a small mishap, I

573
00:16:49,066 --> 0:16:50,746
accidentally opened up a portal

574
00:16:50,746 --> 0:16:51,736
to another timeline.

575
00:16:51,736 --> 0:16:55,776
Now I have to routinely fight

576
00:16:55,776 --> 0:16:57,276
off my mirror universe nemesis.

577
00:16:57,766 --> 0:16:59,566
I obviously don't want him

578
00:16:59,566 --> 0:17:01,716
accessing secret Apple data, so

579
00:16:59,566 --> 0:17:01,716
accessing secret Apple data, so

580
00:17:01,716 --> 0:17:03,766
I have to rely on a pass code.

581
00:17:04,306 --> 0:17:06,955
And people like me with similar

582
00:17:06,955 --> 0:17:08,546
looking family members or evil

583
00:17:08,546 --> 0:17:10,306
twins need to take the same

584
00:17:10,306 --> 0:17:10,846
precautions.

585
00:17:12,266 --> 0:17:14,646
As Apple, we also talked about

586
00:17:14,756 --> 0:17:15,675
this limitation.

587
00:17:16,146 --> 0:17:18,596
We dedicated time in a keynote

588
00:17:18,955 --> 0:17:20,026
because it's important that our

589
00:17:20,026 --> 0:17:23,046
customers understand and take

590
00:17:23,046 --> 0:17:24,556
steps to secure their data.

591
00:17:25,445 --> 0:17:26,656
It's important for you to

592
00:17:26,656 --> 0:17:27,886
understand and communicate

593
00:17:27,886 --> 0:17:30,386
limitations to unpack aggregate

594
00:17:30,466 --> 0:17:31,076
statistics.

595
00:17:31,566 --> 0:17:34,256
Not all mistakes are equal.

596
00:17:35,126 --> 0:17:37,266
Mistakes in machine learning are

597
00:17:37,266 --> 0:17:37,866
inevitable.

598
00:17:38,296 --> 0:17:40,126
Few models work 100% of the

599
00:17:40,126 --> 0:17:40,536
time.

600
00:17:41,326 --> 0:17:42,396
When you start, you may not

601
00:17:42,396 --> 0:17:44,296
understand all the limitations.

602
00:17:44,696 --> 0:17:45,756
That's OK.

603
00:17:46,116 --> 0:17:47,286
It's an iterative process.

604
00:17:48,036 --> 0:17:50,136
But over time, you need to build

605
00:17:50,136 --> 0:17:51,876
a better understanding of what

606
00:17:51,926 --> 0:17:53,426
is and is not possible.

607
00:17:54,446 --> 0:17:56,306
Mistakes don't mean you can't

608
00:17:56,306 --> 0:17:57,676
create an experience that people

609
00:17:57,676 --> 0:17:57,966
love.

610
00:17:58,586 --> 0:18:00,066
If you understand mistakes, you

611
00:17:58,586 --> 0:18:00,066
If you understand mistakes, you

612
00:18:00,066 --> 0:18:02,406
can account for them either by

613
00:18:02,526 --> 0:18:04,436
creating new models, improving

614
00:18:04,436 --> 0:18:06,286
the product or clearly

615
00:18:06,286 --> 0:18:07,386
communicating technical

616
00:18:07,386 --> 0:18:07,976
limitations.

617
00:18:10,366 --> 0:18:12,196
And remember that metrics are

618
00:18:12,196 --> 0:18:14,696
proxies for what we actually

619
00:18:14,696 --> 0:18:15,196
care about.

620
00:18:16,106 --> 0:18:17,656
Metrics provide numbers, a sense

621
00:18:17,656 --> 0:18:20,156
of scientific accuracy, and it's

622
00:18:20,306 --> 0:18:22,386
easy to get caught up in numbers

623
00:18:22,386 --> 0:18:24,326
and improving them but we

624
00:18:24,326 --> 0:18:26,096
ultimately care about abstract

625
00:18:26,096 --> 0:18:26,606
concepts.

626
00:18:27,446 --> 0:18:28,646
These things are hard to measure

627
00:18:28,926 --> 0:18:30,926
like good experiences, happy

628
00:18:30,926 --> 0:18:32,396
customers and strong brands.

629
00:18:33,256 --> 0:18:34,836
Let's look at an example.

630
00:18:35,976 --> 0:18:37,386
To understand metrics as

631
00:18:37,426 --> 0:18:39,486
proxies, let's talk about the

632
00:18:39,486 --> 0:18:40,876
App Store.

633
00:18:41,056 --> 0:18:42,556
The App Store uses machine

634
00:18:42,556 --> 0:18:44,346
learning to recommend new apps

635
00:18:44,866 --> 0:18:46,176
based on what you've downloaded.

636
00:18:46,666 --> 0:18:48,476
And at first approximation, the

637
00:18:48,596 --> 0:18:50,186
amount of time a person spends

638
00:18:50,186 --> 0:18:51,526
in the app might be a good

639
00:18:51,526 --> 0:18:51,896
measure.

640
00:18:52,226 --> 0:18:52,426
Right?

641
00:18:52,426 --> 0:18:54,426
Afterall, the more time you

642
00:18:54,426 --> 0:18:55,526
spend doing something, the more

643
00:18:55,526 --> 0:18:56,036
you like it.

644
00:18:56,036 --> 0:18:59,246
And if the App Store is purely

645
00:18:59,246 --> 0:19:00,756
driven by metrics and models,

646
00:18:59,246 --> 0:19:00,756
driven by metrics and models,

647
00:19:01,416 --> 0:19:02,566
people would see apps that are

648
00:19:02,566 --> 0:19:04,016
very similar to the apps they

649
00:19:04,016 --> 0:19:04,686
currently use.

650
00:19:05,416 --> 0:19:06,426
I like playing games.

651
00:19:06,886 --> 0:19:08,786
So my top recommended apps are

652
00:19:08,786 --> 0:19:09,386
all games.

653
00:19:10,066 --> 0:19:12,176
And for a while, I might like

654
00:19:12,206 --> 0:19:14,006
those recommendations but over

655
00:19:14,006 --> 0:19:15,686
time, I'd feel boxed in because

656
00:19:15,686 --> 0:19:16,946
I care about more than just

657
00:19:16,946 --> 0:19:17,306
games.

658
00:19:18,316 --> 0:19:20,886
People have diverse interests

659
00:19:20,886 --> 0:19:21,406
and tastes.

660
00:19:22,146 --> 0:19:23,576
The amount of time I spend in an

661
00:19:23,576 --> 0:19:25,236
app may not indicate how much I

662
00:19:25,236 --> 0:19:26,366
value it.

663
00:19:27,076 --> 0:19:29,156
The App Store makes an editorial

664
00:19:29,156 --> 0:19:30,516
content with recommendations.

665
00:19:31,076 --> 0:19:32,416
This allows people to explore a

666
00:19:32,416 --> 0:19:34,016
diverse set of applications and

667
00:19:34,016 --> 0:19:35,836
experiences that they may not

668
00:19:35,836 --> 0:19:37,886
normally see if they just got

669
00:19:37,886 --> 0:19:39,116
their top recommendations.

670
00:19:40,126 --> 0:19:42,346
Editorial content is a way to

671
00:19:42,346 --> 0:19:43,866
address limitations of metrics

672
00:19:43,866 --> 0:19:44,916
and recommendations.

673
00:19:45,856 --> 0:19:47,386
But as you deploy your products

674
00:19:47,386 --> 0:19:48,696
and learn more about customer

675
00:19:48,696 --> 0:19:50,626
needs, what you care about

676
00:19:50,706 --> 0:19:51,246
evolves.

677
00:19:51,886 --> 0:19:54,666
You can evolve your metrics to

678
00:19:54,666 --> 0:19:55,156
match.

679
00:19:55,936 --> 0:19:57,376
For example, you can bake

680
00:19:57,416 --> 0:19:59,186
diversity into your metrics.

681
00:20:00,046 --> 0:20:01,246
You can measure when people

682
00:20:01,246 --> 0:20:03,406
engage with diverse content and

683
00:20:03,406 --> 0:20:05,286
actively create models that

684
00:20:05,286 --> 0:20:06,886
balance the quality and

685
00:20:06,886 --> 0:20:08,336
diversity of apps that people

686
00:20:08,336 --> 0:20:08,866
engage in.

687
00:20:10,056 --> 0:20:11,156
You should ensure that your

688
00:20:11,156 --> 0:20:12,506
metrics still track what you

689
00:20:12,506 --> 0:20:15,106
care about: a good experience,

690
00:20:15,636 --> 0:20:16,896
happy and fulfilled customers

691
00:20:16,896 --> 0:20:17,736
and strong brands.

692
00:20:18,606 --> 0:20:19,906
But to do that, you need to

693
00:20:19,906 --> 0:20:21,346
question metrics to ensure

694
00:20:21,346 --> 0:20:21,856
they're relevant.

695
00:20:21,856 --> 0:20:25,836
In your day to day, you can put

696
00:20:25,836 --> 0:20:26,546
this into practice.

697
00:20:27,196 --> 0:20:29,756
You can try to understand

698
00:20:29,986 --> 0:20:31,716
mistakes because not all

699
00:20:31,716 --> 0:20:32,616
mistakes are equal.

700
00:20:33,256 --> 0:20:34,436
Group failure cases into

701
00:20:34,436 --> 0:20:35,746
categories and scenarios.

702
00:20:36,436 --> 0:20:37,566
This will help you understand

703
00:20:37,566 --> 0:20:39,116
how important each scenario is.

704
00:20:39,776 --> 0:20:41,586
Decide if it's best handled by a

705
00:20:41,586 --> 0:20:43,636
non-ML approach like improving

706
00:20:43,636 --> 0:20:45,866
the design or if you actually

707
00:20:45,866 --> 0:20:46,796
need to build a better model.

708
00:20:47,366 --> 0:20:49,826
And design for failure

709
00:20:49,826 --> 0:20:50,396
scenarios.

710
00:20:51,146 --> 0:20:52,706
When storyboarding or sketching

711
00:20:52,706 --> 0:20:54,416
out an experience, don't just

712
00:20:54,476 --> 0:20:55,656
sketch out what happens if

713
00:20:55,656 --> 0:20:56,446
things go well.

714
00:20:57,246 --> 0:20:58,356
Sketch out what happens if

715
00:20:58,356 --> 0:20:59,286
something goes wrong.

716
00:20:59,916 --> 0:21:01,396
This will help you emphasize

717
00:20:59,916 --> 0:21:01,396
This will help you emphasize

718
00:21:01,396 --> 0:21:02,306
what your customers might

719
00:21:02,306 --> 0:21:02,886
experience.

720
00:21:03,466 --> 0:21:06,886
Spend time evaluating the

721
00:21:06,886 --> 0:21:07,556
experience.

722
00:21:08,536 --> 0:21:09,466
The metric will give you an

723
00:21:09,466 --> 0:21:11,056
objective value for how well

724
00:21:11,056 --> 0:21:11,926
your model is working.

725
00:21:12,496 --> 0:21:14,166
Remember, the model is not the

726
00:21:14,166 --> 0:21:14,816
experience.

727
00:21:15,516 --> 0:21:17,206
The actual experience is the

728
00:21:17,206 --> 0:21:18,236
experience.

729
00:21:18,596 --> 0:21:19,946
So try to measure that

730
00:21:19,946 --> 0:21:20,556
experience.

731
00:21:21,036 --> 0:21:22,096
Run user studies.

732
00:21:22,436 --> 0:21:23,256
Build demos.

733
00:21:23,916 --> 0:21:25,096
Talk to your customers.

734
00:21:25,216 --> 0:21:26,096
Read forums.

735
00:21:27,026 --> 0:21:28,796
If the experience is bad and the

736
00:21:28,796 --> 0:21:29,906
metric says it's good, your

737
00:21:30,346 --> 0:21:32,046
metric is probably wrong.

738
00:21:32,606 --> 0:21:34,526
If the experience seems the same

739
00:21:34,526 --> 0:21:35,546
the metric says it's getting

740
00:21:35,546 --> 0:21:37,726
better, your metric is likely

741
00:21:37,726 --> 0:21:38,216
wrong.

742
00:21:38,776 --> 0:21:42,336
And over time, evolve your

743
00:21:42,336 --> 0:21:42,806
metrics.

744
00:21:43,436 --> 0:21:44,636
Question and assess the

745
00:21:44,636 --> 0:21:45,786
effectiveness of your metrics

746
00:21:46,096 --> 0:21:46,606
constantly.

747
00:21:47,176 --> 0:21:48,546
The more you rely on something,

748
00:21:48,606 --> 0:21:51,946
the more you should question it.

749
00:21:53,456 --> 0:21:55,836
Ultimately, your metrics reflect

750
00:21:55,836 --> 0:21:56,496
your values.

751
00:21:57,016 --> 0:21:59,716
And if you want to align your

752
00:21:59,716 --> 0:22:00,656
metrics with your values, you

753
00:21:59,716 --> 0:22:00,656
metrics with your values, you

754
00:22:00,656 --> 0:22:01,516
have to think carefully,

755
00:22:01,626 --> 0:22:03,356
critically and continuously

756
00:22:03,646 --> 0:22:04,686
about what is and is not

757
00:22:04,686 --> 0:22:05,026
measured.

758
00:22:05,536 --> 0:22:08,366
We just talked about metrics

759
00:22:08,876 --> 0:22:10,066
which is how you understand if

760
00:22:10,066 --> 0:22:11,646
the model is good, how you

761
00:22:11,646 --> 0:22:14,446
evaluate a model.

762
00:22:14,606 --> 0:22:16,616
Next, let's talk about the

763
00:22:16,616 --> 0:22:17,226
interface.

764
00:22:17,836 --> 0:22:20,106
Designing metrics in data is key

765
00:22:20,106 --> 0:22:21,006
to designing how your model

766
00:22:21,006 --> 0:22:21,356
behaves.

767
00:22:22,226 --> 0:22:23,766
But we also have to figure out

768
00:22:23,766 --> 0:22:25,696
how do we surface the model in

769
00:22:25,696 --> 0:22:26,236
the interface.

770
00:22:26,436 --> 0:22:27,876
And to do that, I'll hand it

771
00:22:27,876 --> 0:22:30,096
over to Rubii and Cas to talk

772
00:22:30,146 --> 0:22:31,696
more about outputs/inputs.

773
00:22:31,946 --> 0:22:32,696
Thank you so much.

774
00:22:33,516 --> 0:22:37,500
[ Applause ]

775
00:22:46,126 --> 0:22:47,836
&gt;&gt; As Kayur has shown, there's a

776
00:22:47,836 --> 0:22:49,856
lot that goes into creating the

777
00:22:49,856 --> 0:22:51,286
underlying machine learning

778
00:22:51,286 --> 0:22:52,506
system that powers an

779
00:22:52,506 --> 0:22:53,136
experience.

780
00:22:54,036 --> 0:22:55,356
But we also need to design

781
00:22:55,356 --> 0:22:57,336
interfaces that allow people to

782
00:22:57,336 --> 0:22:58,636
interact with the experience in

783
00:22:58,636 --> 0:22:59,756
an intuitive way.

784
00:23:00,416 --> 0:23:02,926
The interface can translate

785
00:23:02,926 --> 0:23:04,586
results from a model to outputs

786
00:23:04,806 --> 0:23:05,676
that people can actually

787
00:23:05,676 --> 0:23:06,286
interact with.

788
00:23:07,286 --> 0:23:08,436
The feedback we collect from

789
00:23:08,436 --> 0:23:10,466
these interactions are inputs we

790
00:23:10,466 --> 0:23:11,916
can use to future improve the

791
00:23:11,916 --> 0:23:12,506
experience.

792
00:23:12,996 --> 0:23:16,996
We've created a set of patterns

793
00:23:17,116 --> 0:23:18,126
in the human interface

794
00:23:18,126 --> 0:23:19,786
guidelines to help you design

795
00:23:19,786 --> 0:23:21,086
output and inputs.

796
00:23:22,286 --> 0:23:23,336
Cas and I will go in-depth

797
00:23:23,336 --> 0:23:24,506
through some of them to help you

798
00:23:24,506 --> 0:23:26,316
understand how they apply to the

799
00:23:26,316 --> 0:23:27,236
experience you're building.

800
00:23:27,236 --> 0:23:31,416
First let's talk about outputs.

801
00:23:31,486 --> 0:23:34,326
Outputs can be more than just

802
00:23:34,326 --> 0:23:34,856
predictions.

803
00:23:35,106 --> 0:23:36,706
They're a design medium that can

804
00:23:36,786 --> 0:23:38,306
augment an experience into

805
00:23:38,306 --> 0:23:39,506
something that feels more

806
00:23:39,506 --> 0:23:41,326
contextual and seamlessly

807
00:23:41,326 --> 0:23:41,776
helpful.

808
00:23:42,856 --> 0:23:44,406
There are four types of outputs

809
00:23:45,226 --> 0:23:47,436
we'll discuss.

810
00:23:47,556 --> 0:23:48,986
Multiple options allow you to

811
00:23:49,026 --> 0:23:51,196
present a diverse set of outputs

812
00:23:51,746 --> 0:23:53,686
to people.

813
00:23:53,806 --> 0:23:55,626
Attributions are explanations

814
00:23:55,816 --> 0:23:57,356
that help people understand more

815
00:23:57,356 --> 0:23:58,596
about how your app makes

816
00:23:58,656 --> 0:23:59,146
decisions.

817
00:24:01,196 --> 0:24:02,876
Confidence is a measurement of

818
00:24:02,876 --> 0:24:06,556
certainty for an output.

819
00:24:06,726 --> 0:24:08,426
And limitations occurs when

820
00:24:08,426 --> 0:24:09,536
there's a mismatch between

821
00:24:09,536 --> 0:24:10,736
people's mental model of a

822
00:24:10,736 --> 0:24:12,606
feature and what the feature can

823
00:24:12,656 --> 0:24:13,426
actually do.

824
00:24:14,006 --> 0:24:16,836
Let's start with multiple

825
00:24:16,836 --> 0:24:17,256
options.

826
00:24:17,716 --> 0:24:22,586
As I had mentioned, multiple

827
00:24:22,586 --> 0:24:24,126
options allow people to choose

828
00:24:24,286 --> 0:24:25,646
from among the results a feature

829
00:24:25,646 --> 0:24:26,106
generates.

830
00:24:27,186 --> 0:24:28,296
What does this actually mean?

831
00:24:28,826 --> 0:24:31,686
Well it's often tempting to only

832
00:24:31,686 --> 0:24:33,306
reveal the best option that was

833
00:24:33,306 --> 0:24:34,546
created by your model.

834
00:24:34,686 --> 0:24:35,936
That's often not the best

835
00:24:35,936 --> 0:24:36,546
experience.

836
00:24:37,356 --> 0:24:38,586
Let me show you an example from

837
00:24:38,586 --> 0:24:39,126
the real world.

838
00:24:40,166 --> 0:24:41,786
Last weekend I wanted to take a

839
00:24:41,786 --> 0:24:43,516
short trip and I asked my

840
00:24:43,516 --> 0:24:45,306
colleagues what's the best way

841
00:24:45,366 --> 0:24:47,056
to get between San Francisco and

842
00:24:47,056 --> 0:24:47,426
Napa.

843
00:24:49,166 --> 0:24:50,516
Depending on the person and the

844
00:24:50,516 --> 0:24:52,186
time of day that I asked, the

845
00:24:52,186 --> 0:24:53,286
answers varied.

846
00:24:53,666 --> 0:24:55,136
Sometimes they would say take

847
00:24:55,136 --> 0:24:55,896
the I-80.

848
00:24:56,166 --> 0:24:57,516
Sometimes they would say take

849
00:24:57,516 --> 0:24:57,976
the 101.

850
00:24:58,826 --> 0:25:01,076
This is because route prediction

851
00:24:58,826 --> 0:25:01,076
This is because route prediction

852
00:25:01,076 --> 0:25:03,186
is a complex task with a lot of

853
00:25:03,186 --> 0:25:04,366
different variables that might

854
00:25:04,366 --> 0:25:05,556
constantly be changing.

855
00:25:06,366 --> 0:25:07,636
There might be traffic or

856
00:25:07,636 --> 0:25:09,056
construction or even car

857
00:25:09,926 --> 0:25:10,196
accidents.

858
00:25:11,936 --> 0:25:13,716
Given all of that information,

859
00:25:13,926 --> 0:25:15,696
in Maps we try to predict the

860
00:25:15,696 --> 0:25:17,276
best route possible but

861
00:25:17,276 --> 0:25:19,236
sometimes a single option is not

862
00:25:19,236 --> 0:25:19,586
enough.

863
00:25:21,096 --> 0:25:22,716
We can't know everything that a

864
00:25:22,716 --> 0:25:24,176
particular person may care

865
00:25:24,176 --> 0:25:24,456
about.

866
00:25:25,336 --> 0:25:26,996
They may prefer scenic routes or

867
00:25:26,996 --> 0:25:28,856
routes without tolls or routes

868
00:25:28,856 --> 0:25:30,566
not along highways.

869
00:25:32,856 --> 0:25:34,676
Providing a set of meaningfully

870
00:25:34,676 --> 0:25:36,246
different routes can help people

871
00:25:36,246 --> 0:25:37,566
navigate the gap between their

872
00:25:37,636 --> 0:25:39,766
preferences and what a model can

873
00:25:39,766 --> 0:25:40,726
realistically predict.

874
00:25:41,526 --> 0:25:43,176
Here, Maps gives us three

875
00:25:43,176 --> 0:25:45,116
distinctly different routes: One

876
00:25:45,116 --> 0:25:46,486
that goes to the North Bay and

877
00:25:46,486 --> 0:25:47,916
two that go to the East Bay.

878
00:25:48,926 --> 0:25:50,056
This can give people a sense of

879
00:25:50,106 --> 0:25:51,766
control over how to get to their

880
00:25:51,766 --> 0:25:52,976
final destination.

881
00:25:56,336 --> 0:25:57,696
Whenever possible, you should

882
00:25:57,696 --> 0:25:59,556
prefer diverse options that

883
00:25:59,556 --> 0:26:00,806
encapsulate a meaningful

884
00:25:59,556 --> 0:26:00,806
encapsulate a meaningful

885
00:26:00,806 --> 0:26:04,256
selection of choices.

886
00:26:06,516 --> 0:26:08,246
People might also be using Maps

887
00:26:08,246 --> 0:26:09,736
while on the go and some of the

888
00:26:09,736 --> 0:26:11,056
options might appear very

889
00:26:11,056 --> 0:26:12,746
similar which makes it difficult

890
00:26:12,746 --> 0:26:14,086
for people to quickly choose the

891
00:26:14,086 --> 0:26:14,226
route.

892
00:26:15,556 --> 0:26:17,276
We can also use attributions to

893
00:26:17,276 --> 0:26:18,336
help people differentiate

894
00:26:18,336 --> 0:26:19,916
between the options and make a

895
00:26:19,916 --> 0:26:20,786
choice faster.

896
00:26:22,816 --> 0:26:24,756
Here, we show you whether a

897
00:26:24,756 --> 0:26:27,606
route requires a toll, which

898
00:26:27,606 --> 0:26:30,286
highway it goes through and we

899
00:26:30,416 --> 0:26:31,936
even highlight each path on the

900
00:26:31,936 --> 0:26:34,196
map making it easier to find

901
00:26:34,196 --> 0:26:36,376
that perfect scenic waterfront

902
00:26:37,216 --> 0:26:37,346
drive.

903
00:26:39,336 --> 0:26:41,136
These small summaries are a

904
00:26:41,136 --> 0:26:42,666
great tool to help people

905
00:26:42,666 --> 0:26:44,236
differentiate between a broad

906
00:26:44,296 --> 0:26:45,776
selection of options at a quick

907
00:26:45,776 --> 0:26:47,306
glance and easily make a

908
00:26:47,306 --> 0:26:50,766
selection while on the go.

909
00:26:51,026 --> 0:26:53,076
Now I know Maps is an obvious

910
00:26:53,076 --> 0:26:54,716
example for presenting options

911
00:26:55,246 --> 0:26:56,656
because it's reacting to very

912
00:26:56,656 --> 0:26:57,426
explicit input.

913
00:26:57,426 --> 0:26:59,576
I know where I am, where I want

914
00:26:59,576 --> 0:27:01,316
to go and I'm expecting a list

915
00:26:59,576 --> 0:27:01,316
to go and I'm expecting a list

916
00:27:01,316 --> 0:27:02,596
of options that can get me

917
00:27:03,066 --> 0:27:03,166
there.

918
00:27:03,736 --> 0:27:05,796
But multiple options can also be

919
00:27:05,796 --> 0:27:07,106
useful for features that are

920
00:27:07,106 --> 0:27:08,456
proactively surfacing

921
00:27:08,456 --> 0:27:10,336
suggestions or it might be more

922
00:27:10,336 --> 0:27:12,246
ambiguous what my current intent

923
00:27:12,246 --> 0:27:13,886
and context is.

924
00:27:16,296 --> 0:27:17,376
Let's talk about the watch.

925
00:27:17,376 --> 0:27:19,476
When I start the day every

926
00:27:19,476 --> 0:27:21,686
morning, I might want to check

927
00:27:22,076 --> 0:27:23,756
the weather, see my first

928
00:27:23,756 --> 0:27:27,686
appointment and see reminders of

929
00:27:27,686 --> 0:27:29,426
what I need to do that day.

930
00:27:30,036 --> 0:27:31,536
Siri watch face allows me to

931
00:27:31,536 --> 0:27:33,736
customize up to 19 different

932
00:27:33,736 --> 0:27:35,366
data sources that I might want

933
00:27:35,366 --> 0:27:36,326
to get information from

934
00:27:36,866 --> 0:27:38,736
including weather, calendar, and

935
00:27:38,736 --> 0:27:39,186
reminders.

936
00:27:40,016 --> 0:27:40,936
You can even choose to see

937
00:27:40,936 --> 0:27:42,086
information from third-party

938
00:27:42,086 --> 0:27:42,346
apps.

939
00:27:43,166 --> 0:27:45,046
Now information from 19

940
00:27:45,046 --> 0:27:46,746
different apps is a lot to look

941
00:27:46,746 --> 0:27:48,066
at on such a small screen.

942
00:27:49,096 --> 0:27:50,446
That's why Siri chooses to

943
00:27:50,516 --> 0:27:52,056
surface a smaller selection of

944
00:27:52,056 --> 0:27:55,036
options based on time, location,

945
00:27:55,306 --> 0:27:56,356
and what I've previously

946
00:27:56,356 --> 0:27:57,036
interacted with.

947
00:27:57,546 --> 0:28:01,456
Since my watch knows it's

948
00:27:57,546 --> 0:28:01,456
Since my watch knows it's

949
00:28:01,456 --> 0:28:03,626
morning time, I'm at home, and

950
00:28:03,626 --> 0:28:05,586
in the past I've always selected

951
00:28:05,586 --> 0:28:06,986
the option to turn on the lights

952
00:28:06,986 --> 0:28:07,976
at the certain time.

953
00:28:08,466 --> 0:28:09,836
It'll surface this as a top

954
00:28:09,876 --> 0:28:11,256
suggestion every morning.

955
00:28:11,776 --> 0:28:14,406
Siri can understand my usage

956
00:28:14,406 --> 0:28:16,056
history to help me automate this

957
00:28:16,096 --> 0:28:18,526
mundane task without having to

958
00:28:18,526 --> 0:28:20,046
scroll through this long list of

959
00:28:20,046 --> 0:28:22,146
options when I first wake up.

960
00:28:22,756 --> 0:28:24,086
This makes getting up in the

961
00:28:24,086 --> 0:28:25,626
morning so much more effortless

962
00:28:25,626 --> 0:28:26,706
for someone who's not a morning

963
00:28:26,706 --> 0:28:28,286
person, like me.

964
00:28:31,196 --> 0:28:33,176
This is important because every

965
00:28:33,176 --> 0:28:35,196
time someone selects an option,

966
00:28:35,556 --> 0:28:36,466
they're actually giving you

967
00:28:36,466 --> 0:28:38,396
valuable implicit feedback that

968
00:28:38,396 --> 0:28:39,796
you can use to make sure the

969
00:28:39,796 --> 0:28:41,706
most relevant option is always

970
00:28:41,706 --> 0:28:44,416
ranked at the top.

971
00:28:44,416 --> 0:28:45,166
If you learn from their

972
00:28:45,166 --> 0:28:46,666
selection, you'll be able to

973
00:28:46,666 --> 0:28:48,576
surface better suggestions over

974
00:28:48,576 --> 0:28:48,876
time.

975
00:28:54,786 --> 0:28:55,926
Now remember when we talked

976
00:28:55,926 --> 0:28:59,266
about Maps and how attributions

977
00:28:59,266 --> 0:29:00,676
can be used to differentiate

978
00:28:59,266 --> 0:29:00,676
can be used to differentiate

979
00:29:01,566 --> 0:29:01,686
routes?

980
00:29:03,276 --> 0:29:04,746
Attributions can actually be

981
00:29:04,746 --> 0:29:09,376
used for so much more than that.

982
00:29:09,656 --> 0:29:11,406
Attributions are explanations

983
00:29:11,576 --> 0:29:13,126
that help people understand more

984
00:29:13,126 --> 0:29:14,316
about how your app makes

985
00:29:14,316 --> 0:29:14,816
decisions.

986
00:29:15,346 --> 0:29:19,036
Within the App Store, we use

987
00:29:19,036 --> 0:29:20,546
attributions to explain how

988
00:29:20,546 --> 0:29:22,086
recommendations are created.

989
00:29:23,066 --> 0:29:24,646
This helps people understand why

990
00:29:24,646 --> 0:29:25,606
they might be seeing certain

991
00:29:25,606 --> 0:29:27,596
suggestions and how their data

992
00:29:27,596 --> 0:29:29,046
is being used.

993
00:29:30,716 --> 0:29:32,366
But let's say the App Store is

994
00:29:32,366 --> 0:29:33,776
showing me some recommendations

995
00:29:33,776 --> 0:29:35,006
because I downloaded a cooking

996
00:29:35,916 --> 0:29:36,046
app.

997
00:29:36,046 --> 0:29:37,486
Now I might have downloaded an

998
00:29:37,546 --> 0:29:39,316
app to cook but that doesn't

999
00:29:39,316 --> 0:29:40,856
necessarily mean I'm into

1000
00:29:40,856 --> 0:29:41,186
cooking.

1001
00:29:41,676 --> 0:29:44,756
Neither does it mean that I love

1002
00:29:44,886 --> 0:29:46,366
that particular app.

1003
00:29:48,056 --> 0:29:49,526
Therefore, when using

1004
00:29:49,526 --> 0:29:50,726
attributions to explain

1005
00:29:50,726 --> 0:29:52,296
suggestions, you should relate

1006
00:29:52,296 --> 0:29:53,866
to objective facts rather than

1007
00:29:53,866 --> 0:29:54,776
subjective taste.

1008
00:29:55,646 --> 0:29:57,316
Here, we simply state that these

1009
00:29:57,316 --> 0:29:58,546
suggestions are because you've

1010
00:29:58,546 --> 0:29:59,626
downloaded the "New York Times"

1011
00:29:59,626 --> 0:29:59,976
Cooking app.

1012
00:30:04,486 --> 0:30:06,076
We can never have a complete

1013
00:30:06,076 --> 0:30:07,836
picture of someone's taste and

1014
00:30:07,836 --> 0:30:09,526
preferences because they might

1015
00:30:09,526 --> 0:30:11,056
evolve on a daily basis.

1016
00:30:11,896 --> 0:30:13,116
People might also share an

1017
00:30:13,116 --> 0:30:14,416
account with their friends and

1018
00:30:14,416 --> 0:30:16,766
family; therefore, profiling

1019
00:30:16,766 --> 0:30:18,646
them makes them feel confused

1020
00:30:18,896 --> 0:30:20,596
and boxed in.

1021
00:30:20,816 --> 0:30:22,076
You should avoid profiling

1022
00:30:22,076 --> 0:30:23,966
people by not using language

1023
00:30:23,966 --> 0:30:26,026
that implies understanding or

1024
00:30:26,026 --> 0:30:27,636
judgment of their emotions and

1025
00:30:27,636 --> 0:30:28,226
preferences.

1026
00:30:32,416 --> 0:30:34,226
We can also use attributions to

1027
00:30:34,226 --> 0:30:35,516
help people understand the

1028
00:30:35,516 --> 0:30:37,206
trustworthiness of a result.

1029
00:30:37,206 --> 0:30:40,026
I'm an astronomy enthusiast and

1030
00:30:40,026 --> 0:30:41,436
I wanted to see which planet was

1031
00:30:41,436 --> 0:30:42,666
visible in the sky tonight.

1032
00:30:43,296 --> 0:30:45,426
So I asked Siri, "When can you

1033
00:30:45,426 --> 0:30:46,036
see Jupiter?"

1034
00:30:48,596 --> 0:30:50,216
Siri tells me when the event

1035
00:30:50,216 --> 0:30:52,926
will happen but also gives me

1036
00:30:52,926 --> 0:30:54,846
more information from Wolfram

1037
00:30:55,356 --> 0:30:55,506
Alpha.

1038
00:30:56,156 --> 0:30:57,626
This helps me understand that

1039
00:30:57,626 --> 0:30:59,146
it's a reliable prediction

1040
00:30:59,466 --> 0:31:00,696
because Wolfram Alpha is a

1041
00:30:59,466 --> 0:31:00,696
because Wolfram Alpha is a

1042
00:31:00,696 --> 0:31:02,206
well-known knowledge engine.

1043
00:31:03,026 --> 0:31:04,876
This is really important when it

1044
00:31:04,876 --> 0:31:06,366
comes to displaying predictions

1045
00:31:06,726 --> 0:31:08,036
where the trustworthiness of the

1046
00:31:08,036 --> 0:31:10,266
source really matters like

1047
00:31:10,396 --> 0:31:12,106
scientific information or

1048
00:31:12,106 --> 0:31:12,826
election results.

1049
00:31:13,326 --> 0:31:18,426
Cite data sources so people can

1050
00:31:18,426 --> 0:31:20,316
determine how reliable a certain

1051
00:31:20,316 --> 0:31:20,976
prediction might be.

1052
00:31:26,606 --> 0:31:28,866
Now attributions can be used for

1053
00:31:28,866 --> 0:31:30,096
more than just explaining

1054
00:31:30,096 --> 0:31:31,916
personalization or predictions.

1055
00:31:32,426 --> 0:31:33,316
We can also use it to

1056
00:31:33,316 --> 0:31:34,516
communicate confidence.

1057
00:31:34,516 --> 0:31:38,986
Let's look at the App Store

1058
00:31:38,986 --> 0:31:40,946
again and see how we really try

1059
00:31:40,946 --> 0:31:42,676
to avoid technical jargon here.

1060
00:31:44,616 --> 0:31:46,296
For example, we could've said

1061
00:31:46,866 --> 0:31:48,906
these apps are an 85% match for

1062
00:31:48,906 --> 0:31:51,086
you but it's really difficult to

1063
00:31:51,086 --> 0:31:53,006
understand what 85% mean.

1064
00:31:53,816 --> 0:31:54,936
That number might also mean

1065
00:31:54,936 --> 0:31:56,016
different things to different

1066
00:31:56,016 --> 0:31:56,366
people.

1067
00:31:56,896 --> 0:32:00,686
That's why we try to use a more

1068
00:31:56,896 --> 0:32:00,686
That's why we try to use a more

1069
00:32:00,746 --> 0:32:02,826
understandable explanation such

1070
00:32:02,826 --> 0:32:04,966
as recommendations based on apps

1071
00:32:05,266 --> 0:32:06,106
you've downloaded.

1072
00:32:06,626 --> 0:32:11,056
That 85% I mentioned is a

1073
00:32:11,056 --> 0:32:12,816
measurement of confidence or how

1074
00:32:12,916 --> 0:32:14,556
certain the model is that you'll

1075
00:32:14,556 --> 0:32:15,706
like the recommendations.

1076
00:32:16,826 --> 0:32:18,566
Confidence is what you get from

1077
00:32:18,566 --> 0:32:20,336
a machine learning model but

1078
00:32:20,336 --> 0:32:21,866
it's important to translate it

1079
00:32:21,946 --> 0:32:22,886
into something more

1080
00:32:22,886 --> 0:32:27,606
understandable for people.

1081
00:32:27,736 --> 0:32:29,626
Sometimes it's OK to directly

1082
00:32:29,626 --> 0:32:30,576
display confidence.

1083
00:32:31,556 --> 0:32:32,596
The Weather app gives you

1084
00:32:32,596 --> 0:32:33,756
information that it's going to

1085
00:32:33,756 --> 0:32:35,686
rain but it also gives you a

1086
00:32:35,686 --> 0:32:37,466
percentage for how likely it's

1087
00:32:37,466 --> 0:32:39,166
going to rain.

1088
00:32:39,386 --> 0:32:41,516
Now you might think what kind of

1089
00:32:41,516 --> 0:32:42,926
a decision should I make based

1090
00:32:42,926 --> 0:32:44,186
on this 30% chance.

1091
00:32:45,106 --> 0:32:46,156
Is it worth bringing my

1092
00:32:46,156 --> 0:32:46,596
umbrella?

1093
00:32:47,726 --> 0:32:48,836
It's unclear.

1094
00:32:49,136 --> 0:32:50,136
But we've been conditioned to

1095
00:32:50,136 --> 0:32:51,686
understand this percentage over

1096
00:32:52,646 --> 0:32:52,756
time.

1097
00:32:55,266 --> 0:32:56,386
And while numbers are

1098
00:32:56,386 --> 0:32:57,996
appropriate for statistical

1099
00:32:57,996 --> 0:32:59,426
predictions such as weather,

1100
00:33:00,116 --> 0:33:01,646
sports results or election

1101
00:33:01,646 --> 0:33:03,616
predictions, it might not work

1102
00:33:03,616 --> 0:33:04,596
in other situations.

1103
00:33:05,166 --> 0:33:08,276
This is Hopper.

1104
00:33:09,056 --> 0:33:10,666
It's an app that helps you book

1105
00:33:10,716 --> 0:33:11,346
cheaper flights.

1106
00:33:12,716 --> 0:33:14,246
It uses confidence to tell

1107
00:33:14,246 --> 0:33:15,466
people whether the fare will go

1108
00:33:15,466 --> 0:33:16,636
up or down.

1109
00:33:17,986 --> 0:33:19,416
And we don't expect people to

1110
00:33:19,416 --> 0:33:20,866
grapple with percentages here

1111
00:33:20,996 --> 0:33:22,236
because in this context,

1112
00:33:22,416 --> 0:33:23,876
percentages are difficult to

1113
00:33:23,876 --> 0:33:24,366
interpret.

1114
00:33:24,836 --> 0:33:28,146
Seeing a 65% chance the price

1115
00:33:28,146 --> 0:33:29,586
will go down is not really

1116
00:33:29,586 --> 0:33:30,216
actionable.

1117
00:33:30,886 --> 0:33:34,626
How is 65% different than 70%?

1118
00:33:35,716 --> 0:33:37,166
The way Hopper presents

1119
00:33:37,166 --> 0:33:38,676
confidence is by giving you

1120
00:33:38,676 --> 0:33:40,746
actual suggestions for actions

1121
00:33:40,746 --> 0:33:42,276
you should take, whether you

1122
00:33:42,276 --> 0:33:44,676
should wait or buy now.

1123
00:33:47,026 --> 0:33:48,666
It also provides additional

1124
00:33:48,666 --> 0:33:50,576
context such as how much money

1125
00:33:50,576 --> 0:33:51,906
you're going to save and the

1126
00:33:51,906 --> 0:33:53,386
optimal time range to book a

1127
00:33:53,386 --> 0:33:53,706
flight.

1128
00:33:55,206 --> 0:33:56,716
These additional explanations

1129
00:33:56,716 --> 0:33:58,456
help people make a more informed

1130
00:33:58,546 --> 0:33:59,736
decision.

1131
00:34:02,706 --> 0:34:03,866
When confidence is used

1132
00:34:03,866 --> 0:34:05,416
appropriately, it can make an

1133
00:34:05,416 --> 0:34:07,906
experience feel so much more

1134
00:34:07,906 --> 0:34:08,346
human.

1135
00:34:08,346 --> 0:34:11,406
Most of the time you should

1136
00:34:11,626 --> 0:34:13,126
prefer to translate confidence

1137
00:34:13,166 --> 0:34:15,295
into easy-to-understand terms to

1138
00:34:15,295 --> 0:34:16,446
help people make a decision.

1139
00:34:20,795 --> 0:34:22,376
So by this point, you probably

1140
00:34:22,376 --> 0:34:23,956
understand that confidence can

1141
00:34:23,956 --> 0:34:26,096
be a great tool to help people

1142
00:34:26,096 --> 0:34:27,696
estimate how risky a certain

1143
00:34:27,696 --> 0:34:30,516
decision might be but there are

1144
00:34:30,516 --> 0:34:31,766
cases where this doesn't work.

1145
00:34:33,106 --> 0:34:36,065
For example, if I ask you how

1146
00:34:36,065 --> 0:34:37,406
long it would take for me to get

1147
00:34:37,406 --> 0:34:39,056
between here and San Francisco

1148
00:34:40,166 --> 0:34:42,966
and you replied, "I'm 72%

1149
00:34:42,966 --> 0:34:44,585
confident you'll get there at

1150
00:34:44,585 --> 0:34:45,096
1:30."

1151
00:34:46,116 --> 0:34:47,426
That would be pretty confusing.

1152
00:34:47,426 --> 0:34:49,476
How can I be sure what time I'll

1153
00:34:49,476 --> 0:34:53,876
actually get home?

1154
00:34:54,085 --> 0:34:55,446
A better way is to provide a

1155
00:34:55,446 --> 0:34:56,676
range in the prediction.

1156
00:34:57,546 --> 0:34:59,216
With ride sharing, it's often

1157
00:34:59,216 --> 0:35:00,716
difficult to pinpoint an exact

1158
00:34:59,216 --> 0:35:00,716
difficult to pinpoint an exact

1159
00:35:00,716 --> 0:35:02,746
arrival time since there might

1160
00:35:02,746 --> 0:35:04,246
be traffic or the driver might

1161
00:35:04,246 --> 0:35:05,196
pick up more riders.

1162
00:35:06,876 --> 0:35:08,796
Lyft help people estimate when

1163
00:35:08,796 --> 0:35:09,706
they will arrive at their

1164
00:35:09,706 --> 0:35:11,436
destination before they book a

1165
00:35:11,436 --> 0:35:13,716
ride by providing a range about

1166
00:35:13,716 --> 0:35:15,086
the time the driver will pick

1167
00:35:15,086 --> 0:35:18,066
you up and the time you will

1168
00:35:18,066 --> 0:35:19,086
arrive at the airport.

1169
00:35:20,366 --> 0:35:21,686
These ranges help me have a

1170
00:35:21,686 --> 0:35:23,106
realistic expectation of when

1171
00:35:23,106 --> 0:35:23,946
I'll actually get home.

1172
00:35:28,626 --> 0:35:30,126
Range can be a great tool to

1173
00:35:30,326 --> 0:35:32,026
help people estimate how risky a

1174
00:35:32,026 --> 0:35:33,796
certain decision might be.

1175
00:35:36,936 --> 0:35:38,866
Now these are all good examples

1176
00:35:38,866 --> 0:35:40,986
of how to display confidence but

1177
00:35:40,986 --> 0:35:42,366
what do you do when there is low

1178
00:35:42,366 --> 0:35:44,166
confidence and the model doesn't

1179
00:35:44,206 --> 0:35:45,476
have enough information to

1180
00:35:45,596 --> 0:35:46,926
actually take an action?

1181
00:35:47,476 --> 0:35:49,946
In a lot of cases, we can

1182
00:35:49,946 --> 0:35:51,826
actually ask people for help.

1183
00:35:53,616 --> 0:35:56,286
For example, the Photos app can

1184
00:35:56,366 --> 0:35:58,036
automatically recognize people

1185
00:35:58,566 --> 0:35:59,716
in order to make searching for

1186
00:35:59,716 --> 0:36:00,736
their pictures easier.

1187
00:35:59,716 --> 0:36:00,736
their pictures easier.

1188
00:36:02,166 --> 0:36:03,696
When Photos has low confidence

1189
00:36:03,696 --> 0:36:05,666
who is in a picture, it will ask

1190
00:36:05,756 --> 0:36:07,296
people to confirm additional

1191
00:36:07,296 --> 0:36:09,466
photos of that person before

1192
00:36:09,466 --> 0:36:10,806
automatically labeling them.

1193
00:36:12,196 --> 0:36:13,736
Asking for confirmation is

1194
00:36:13,736 --> 0:36:14,986
important because it'd be

1195
00:36:14,986 --> 0:36:16,526
annoying if I saw pictures that

1196
00:36:16,526 --> 0:36:18,306
were marked as me that weren't

1197
00:36:18,446 --> 0:36:19,416
photos of me at all.

1198
00:36:19,916 --> 0:36:23,616
With every photo I mark as me,

1199
00:36:24,426 --> 0:36:25,586
Photos become better at

1200
00:36:25,586 --> 0:36:27,506
recognizing who I am.

1201
00:36:28,736 --> 0:36:30,176
This is an example of a

1202
00:36:30,176 --> 0:36:31,956
limitation that the facial

1203
00:36:31,956 --> 0:36:33,366
recognition in Photos has,

1204
00:36:37,296 --> 0:36:39,326
which also brings me to our last

1205
00:36:39,326 --> 0:36:39,966
output pattern.

1206
00:36:40,546 --> 0:36:43,936
So what actually is a

1207
00:36:43,936 --> 0:36:44,526
limitation?

1208
00:36:45,556 --> 0:36:46,896
A limitation is that when

1209
00:36:46,896 --> 0:36:48,076
there's a mismatch between

1210
00:36:48,076 --> 0:36:49,516
people's mental model of a

1211
00:36:49,516 --> 0:36:51,306
feature and what the feature can

1212
00:36:51,306 --> 0:36:52,346
actually do.

1213
00:36:52,946 --> 0:36:55,636
I might expect Photos to always

1214
00:36:55,636 --> 0:36:57,456
know who I am even though it

1215
00:36:57,456 --> 0:36:59,206
can't do that yet because a

1216
00:36:59,206 --> 0:37:00,976
photo might be blurry or taken

1217
00:36:59,206 --> 0:37:00,976
photo might be blurry or taken

1218
00:37:00,976 --> 0:37:04,896
in low light or not in focus.

1219
00:37:05,046 --> 0:37:06,606
Every feature has limitations

1220
00:37:07,126 --> 0:37:09,056
whether by design, capability,

1221
00:37:09,216 --> 0:37:09,976
or circumstance.

1222
00:37:10,556 --> 0:37:12,766
You have to strengthen people's

1223
00:37:12,766 --> 0:37:13,856
trust in your app by

1224
00:37:13,856 --> 0:37:15,546
acknowledging its limitations

1225
00:37:15,746 --> 0:37:17,676
and teaching people how to work

1226
00:37:18,326 --> 0:37:21,166
with them.

1227
00:37:21,376 --> 0:37:23,566
Now Memoji is one of my favorite

1228
00:37:23,566 --> 0:37:25,736
features but people might not

1229
00:37:25,736 --> 0:37:27,526
understand that it won't work in

1230
00:37:27,526 --> 0:37:30,436
certain scenarios, whether your

1231
00:37:30,606 --> 0:37:34,916
face is not in view, something

1232
00:37:34,916 --> 0:37:39,026
is covering the camera, or

1233
00:37:39,026 --> 0:37:40,636
you're in a dark room.

1234
00:37:41,216 --> 0:37:42,886
Every time this limitation

1235
00:37:42,886 --> 0:37:45,036
occurs, we instantly show these

1236
00:37:45,036 --> 0:37:46,956
inline coaching tips to help

1237
00:37:46,956 --> 0:37:48,736
people move past the limitation

1238
00:37:48,996 --> 0:37:50,236
and successfully use the

1239
00:37:50,286 --> 0:37:50,666
feature.

1240
00:37:51,186 --> 0:37:54,216
Explaining the limitations when

1241
00:37:54,216 --> 0:37:55,826
they happen help people learn to

1242
00:37:55,826 --> 0:37:57,446
avoid these situations in the

1243
00:37:57,446 --> 0:37:57,896
future.

1244
00:37:58,546 --> 0:38:03,066
People might lose trust in your

1245
00:37:58,546 --> 0:38:03,066
People might lose trust in your

1246
00:38:03,066 --> 0:38:04,966
feature if limitations are not

1247
00:38:04,966 --> 0:38:06,026
appropriately addressed.

1248
00:38:07,426 --> 0:38:08,726
You should take care to manage

1249
00:38:08,726 --> 0:38:10,376
people's expectations of what a

1250
00:38:10,376 --> 0:38:13,146
feature can actually do and

1251
00:38:13,146 --> 0:38:14,766
guide people to move past

1252
00:38:14,986 --> 0:38:15,666
limitations.

1253
00:38:19,726 --> 0:38:21,036
Another way to help people move

1254
00:38:21,096 --> 0:38:22,866
past limitations is to suggest

1255
00:38:22,966 --> 0:38:25,146
alternative ways to accomplish

1256
00:38:25,786 --> 0:38:26,666
their goals.

1257
00:38:26,856 --> 0:38:28,146
In order to do this right, you

1258
00:38:28,286 --> 0:38:29,936
need to understand the goal well

1259
00:38:29,936 --> 0:38:31,996
enough to suggest alternatives

1260
00:38:31,996 --> 0:38:32,946
that actually make sense.

1261
00:38:34,596 --> 0:38:36,636
For example, if people ask Siri

1262
00:38:36,636 --> 0:38:38,656
to set a timer on a Mac, it

1263
00:38:38,656 --> 0:38:40,426
can't perform the action because

1264
00:38:40,456 --> 0:38:43,286
timers aren't available at

1265
00:38:45,176 --> 0:38:45,346
MacOS.

1266
00:38:45,486 --> 0:38:47,156
Instead of simply replying, "I

1267
00:38:47,156 --> 0:38:48,736
can't do it," which is kind of

1268
00:38:48,736 --> 0:38:50,746
frustrating, Siri suggests

1269
00:38:50,746 --> 0:38:52,076
setting a reminder instead.

1270
00:38:53,336 --> 0:38:54,506
This suggestion actually makes

1271
00:38:54,506 --> 0:38:56,456
sense because Siri understands

1272
00:38:56,726 --> 0:38:58,136
that the goal is to receive an

1273
00:38:58,136 --> 0:39:00,436
alert at a certain time and

1274
00:38:58,136 --> 0:39:00,436
alert at a certain time and

1275
00:39:00,436 --> 0:39:02,376
setting a reminder accomplishes

1276
00:39:02,456 --> 0:39:04,136
the same goal as setting a

1277
00:39:04,816 --> 0:39:04,956
timer.

1278
00:39:07,256 --> 0:39:08,996
When possible, suggest

1279
00:39:08,996 --> 0:39:10,266
alternatives that can help

1280
00:39:10,266 --> 0:39:11,586
people accomplish their goals.

1281
00:39:11,586 --> 0:39:16,736
I hope these patterns have given

1282
00:39:16,736 --> 0:39:18,256
you some ideas on different

1283
00:39:18,256 --> 0:39:19,966
outputs that you can get from a

1284
00:39:19,966 --> 0:39:20,946
machine learning model.

1285
00:39:21,486 --> 0:39:24,066
But you have to remember that

1286
00:39:24,066 --> 0:39:25,466
outputs are a design medium.

1287
00:39:26,566 --> 0:39:27,926
By understanding the types of

1288
00:39:27,926 --> 0:39:29,676
outputs that are available, you

1289
00:39:29,676 --> 0:39:31,676
can choose outputs that align

1290
00:39:31,676 --> 0:39:32,846
with the experience you want to

1291
00:39:32,846 --> 0:39:34,706
build and not just rely on

1292
00:39:34,706 --> 0:39:36,266
standard outputs from the model.

1293
00:39:36,896 --> 0:39:39,306
We should respect people's

1294
00:39:39,306 --> 0:39:41,476
agency and time by choosing

1295
00:39:41,596 --> 0:39:42,906
outputs that are easy to

1296
00:39:42,906 --> 0:39:44,866
understand and effortlessly

1297
00:39:44,866 --> 0:39:45,286
helpful.

1298
00:39:48,586 --> 0:39:50,406
As you've seen, there are many

1299
00:39:50,406 --> 0:39:51,756
different ways to translate

1300
00:39:51,756 --> 0:39:53,206
outputs from the model to the

1301
00:39:53,206 --> 0:39:53,676
interface.

1302
00:39:54,366 --> 0:39:57,326
But of course, outputs aren't

1303
00:39:57,326 --> 0:39:57,756
static.

1304
00:39:58,526 --> 0:40:00,386
They're dynamic and constantly

1305
00:39:58,526 --> 0:40:00,386
They're dynamic and constantly

1306
00:40:00,386 --> 0:40:01,646
update based on inputs.

1307
00:40:02,936 --> 0:40:04,136
People interact with the

1308
00:40:04,136 --> 0:40:05,956
experience through inputs and

1309
00:40:05,956 --> 0:40:07,446
now Cas will tell you all about

1310
00:40:07,446 --> 0:40:08,656
how to design for inputs.

1311
00:40:08,656 --> 0:40:08,896
Thank you.

1312
00:40:09,516 --> 0:40:15,500
[ Applause ]

1313
00:40:19,546 --> 0:40:21,006
&gt;&gt; So Rubii showed a range of

1314
00:40:21,006 --> 0:40:22,556
outputs that people can interact

1315
00:40:22,556 --> 0:40:24,726
with on the interface and these

1316
00:40:24,726 --> 0:40:26,476
interactions serve as inputs

1317
00:40:26,616 --> 0:40:27,796
that we can use to collect

1318
00:40:27,886 --> 0:40:29,576
feedback for information from

1319
00:40:29,576 --> 0:40:31,446
people so we can improve our

1320
00:40:31,446 --> 0:40:32,056
experience.

1321
00:40:33,256 --> 0:40:34,346
These are the four inputs we

1322
00:40:34,346 --> 0:40:34,946
will discuss.

1323
00:40:36,656 --> 0:40:38,136
Calibration helps you get

1324
00:40:38,136 --> 0:40:39,576
essential information for

1325
00:40:39,576 --> 0:40:40,786
someone to engage in your

1326
00:40:40,786 --> 0:40:41,426
experience.

1327
00:40:42,566 --> 0:40:43,706
Calibration allows you to

1328
00:40:43,706 --> 0:40:45,886
collect important information

1329
00:40:46,116 --> 0:40:47,466
from interactions someone has

1330
00:40:47,466 --> 0:40:48,506
with your experience.

1331
00:40:49,396 --> 0:40:51,066
Explicit feedback also allows

1332
00:40:51,066 --> 0:40:52,586
you to collect information but

1333
00:40:52,586 --> 0:40:53,976
this time by asking specific

1334
00:40:53,976 --> 0:40:55,336
questions about the results

1335
00:40:55,336 --> 0:40:55,816
you're showing.

1336
00:40:56,836 --> 0:40:58,246
And corrections allow people to

1337
00:40:58,326 --> 0:40:59,996
fix a mistake a model has made

1338
00:41:00,416 --> 0:41:02,006
by using familiar interfaces.

1339
00:41:02,826 --> 0:41:04,306
So let's start with calibration.

1340
00:41:05,696 --> 0:41:07,256
As I just said, calibrations

1341
00:41:07,256 --> 0:41:08,436
allow people to provide

1342
00:41:08,436 --> 0:41:09,906
essential information for them

1343
00:41:09,906 --> 0:41:11,396
to engage in your experience.

1344
00:41:12,126 --> 0:41:13,326
We can use calibration for

1345
00:41:13,326 --> 0:41:14,836
example to collect biometric

1346
00:41:14,836 --> 0:41:16,586
data or data about your

1347
00:41:16,586 --> 0:41:17,146
surroundings.

1348
00:41:17,966 --> 0:41:18,866
Let me give an example.

1349
00:41:19,836 --> 0:41:20,936
Let's look at HomeCourt.

1350
00:41:21,556 --> 0:41:22,896
HomeCourt is an app that helps

1351
00:41:22,896 --> 0:41:24,236
you become a better basketball

1352
00:41:24,296 --> 0:41:24,576
player.

1353
00:41:25,316 --> 0:41:26,536
It uses machine learning to

1354
00:41:26,536 --> 0:41:28,016
analyze images from the camera

1355
00:41:28,326 --> 0:41:29,696
and detect for example how

1356
00:41:29,696 --> 0:41:31,526
accurately you take shots at the

1357
00:41:31,526 --> 0:41:31,816
hoop.

1358
00:41:32,716 --> 0:41:34,246
In order to do so, the camera

1359
00:41:34,246 --> 0:41:35,706
needs to be calibrated so it can

1360
00:41:35,706 --> 0:41:37,636
detect you, the hoop and the

1361
00:41:37,636 --> 0:41:37,876
court.

1362
00:41:38,436 --> 0:41:40,446
And HomeCourt has done an

1363
00:41:40,446 --> 0:41:42,046
amazing job at doing this fast

1364
00:41:42,046 --> 0:41:42,736
and intuitively.

1365
00:41:43,516 --> 0:41:44,286
You simply point the

1366
00:41:44,336 --> 0:41:45,936
front-facing camera to the hoop

1367
00:41:46,046 --> 0:41:47,656
and it immediately detects it by

1368
00:41:47,656 --> 0:41:49,006
putting a wide square around it.

1369
00:41:49,696 --> 0:41:50,916
It then asks you to make one

1370
00:41:50,916 --> 0:41:52,136
shot and from then on, you're

1371
00:41:52,136 --> 0:41:53,996
good to go and it will start

1372
00:41:53,996 --> 0:41:55,006
counting your shots.

1373
00:41:55,546 --> 0:41:58,206
What's remarkable here is what

1374
00:41:58,206 --> 0:41:59,516
HomeCourt is not asking you to

1375
00:41:59,516 --> 0:41:59,706
do.

1376
00:42:00,396 --> 0:42:01,296
For example, you don't have to

1377
00:42:01,296 --> 0:42:02,646
draw lines around the hoop or

1378
00:42:02,646 --> 0:42:03,046
the court.

1379
00:42:04,026 --> 0:42:05,536
It doesn't ask for confirmation

1380
00:42:05,536 --> 0:42:06,706
on whether it got the hoop right

1381
00:42:06,706 --> 0:42:07,086
or wrong.

1382
00:42:07,876 --> 0:42:08,926
And you don't need to take shots

1383
00:42:08,926 --> 0:42:10,506
from multiple angles.

1384
00:42:12,076 --> 0:42:13,816
Calibration also happens in some

1385
00:42:13,816 --> 0:42:15,256
of Apple's products, for example

1386
00:42:15,576 --> 0:42:16,566
when you set up Face ID.

1387
00:42:17,636 --> 0:42:19,406
Face ID uses calibration to only

1388
00:42:19,406 --> 0:42:21,036
collect essential information.

1389
00:42:21,986 --> 0:42:23,406
It asks you to scan your face

1390
00:42:23,406 --> 0:42:24,806
twice and from then on, it is

1391
00:42:24,806 --> 0:42:26,246
set up and it will always keep

1392
00:42:26,246 --> 0:42:27,646
working regardless of whether

1393
00:42:27,646 --> 0:42:28,866
you might start to wear glasses

1394
00:42:29,146 --> 0:42:30,316
or change your hairstyle.

1395
00:42:30,906 --> 0:42:33,196
So when you're using

1396
00:42:33,196 --> 0:42:35,106
calibration, try to be quick and

1397
00:42:35,106 --> 0:42:36,746
effortless and only ask for

1398
00:42:36,746 --> 0:42:39,036
essential information and, when

1399
00:42:39,036 --> 0:42:40,916
possible, try to avoid the need

1400
00:42:41,186 --> 0:42:42,556
for multiple calibrations.

1401
00:42:43,136 --> 0:42:46,456
Once you're setting up Face ID,

1402
00:42:46,596 --> 0:42:47,796
we help you along the way.

1403
00:42:48,226 --> 0:42:49,446
We start with an introduction

1404
00:42:49,446 --> 0:42:50,916
that clearly states why your

1405
00:42:50,916 --> 0:42:52,466
phone needs to scan your face.

1406
00:42:53,106 --> 0:42:54,256
And we do this by explaining

1407
00:42:54,256 --> 0:42:55,856
both how it works and what it

1408
00:42:55,856 --> 0:42:56,966
will allow you to do.

1409
00:42:57,456 --> 0:42:59,746
Throughout the process, we

1410
00:42:59,746 --> 0:43:00,736
always give you a sense of

1411
00:42:59,746 --> 0:43:00,736
always give you a sense of

1412
00:43:00,816 --> 0:43:01,296
progress.

1413
00:43:01,736 --> 0:43:03,036
Here, we fill out the lines

1414
00:43:03,036 --> 0:43:03,766
around your face.

1415
00:43:04,286 --> 0:43:07,906
And if this progress might

1416
00:43:07,906 --> 0:43:09,456
stall, we make sure you never

1417
00:43:09,456 --> 0:43:10,696
get stuck and we provide

1418
00:43:10,696 --> 0:43:11,826
guidance to help you move

1419
00:43:11,946 --> 0:43:12,466
forward.

1420
00:43:12,976 --> 0:43:14,626
Here, we point arrows that point

1421
00:43:14,626 --> 0:43:15,466
in which way to look.

1422
00:43:15,466 --> 0:43:19,126
And at the end, we give the

1423
00:43:19,126 --> 0:43:20,006
notion of success.

1424
00:43:20,396 --> 0:43:21,706
We tell you the work is done and

1425
00:43:21,706 --> 0:43:23,636
the feature can be used now.

1426
00:43:24,336 --> 0:43:26,096
So always provide help by

1427
00:43:26,096 --> 0:43:27,496
introducing, guiding and

1428
00:43:27,496 --> 0:43:29,136
confirming the calibration.

1429
00:43:29,696 --> 0:43:32,696
Of course, Face ID collects a

1430
00:43:32,696 --> 0:43:34,136
lot of sensitive information.

1431
00:43:34,376 --> 0:43:35,796
And so to respect your privacy,

1432
00:43:36,076 --> 0:43:37,546
we give you a way to edit or

1433
00:43:37,546 --> 0:43:39,046
remove this information in

1434
00:43:39,046 --> 0:43:39,456
Settings.

1435
00:43:41,066 --> 0:43:42,776
So when possible, allow people

1436
00:43:42,776 --> 0:43:44,106
to update their information.

1437
00:43:44,686 --> 0:43:48,636
Now as I said before, Face ID

1438
00:43:48,636 --> 0:43:50,406
asks for just one calibration

1439
00:43:50,636 --> 0:43:51,896
and from then on, it will keep

1440
00:43:51,896 --> 0:43:53,226
working regardless of whether

1441
00:43:53,226 --> 0:43:54,556
you might start to wear glasses,

1442
00:43:54,906 --> 0:43:56,376
change your hairstyle, maybe

1443
00:43:56,376 --> 0:43:58,156
wear a hat or a scarf sometimes

1444
00:43:58,566 --> 0:44:00,046
or even as your face changes as

1445
00:43:58,566 --> 0:44:00,046
or even as your face changes as

1446
00:44:00,046 --> 0:44:00,736
you become older.

1447
00:44:01,656 --> 0:44:02,886
This was a big challenge when we

1448
00:44:02,886 --> 0:44:03,736
designed Face ID.

1449
00:44:04,296 --> 0:44:05,756
It would've been a lot easier to

1450
00:44:05,756 --> 0:44:07,446
occasionally ask to recalibrate

1451
00:44:07,446 --> 0:44:09,396
your face so your iPhone or iPad

1452
00:44:09,486 --> 0:44:10,506
could keep recognizing you.

1453
00:44:11,336 --> 0:44:12,326
But who would like to rescan

1454
00:44:12,326 --> 0:44:13,656
their face each time they change

1455
00:44:13,656 --> 0:44:14,236
their hairstyle?

1456
00:44:15,356 --> 0:44:16,456
So instead of asking for

1457
00:44:16,456 --> 0:44:18,436
multiple calibrations, Face ID

1458
00:44:18,436 --> 0:44:20,196
collects implicit feedback to

1459
00:44:20,196 --> 0:44:21,566
update the information about

1460
00:44:21,566 --> 0:44:23,906
your face each time you use Face

1461
00:44:23,906 --> 0:44:24,046
ID.

1462
00:44:24,046 --> 0:44:26,046
And that brings us to our next

1463
00:44:26,046 --> 0:44:28,536
pattern which is implicit

1464
00:44:29,536 --> 0:44:29,746
feedback.

1465
00:44:30,636 --> 0:44:32,566
Implicit feedback is information

1466
00:44:32,566 --> 0:44:33,346
that arises from the

1467
00:44:33,346 --> 0:44:34,886
interactions people have with

1468
00:44:34,886 --> 0:44:36,906
your app and that information

1469
00:44:36,906 --> 0:44:38,326
can be used to improve the model

1470
00:44:38,636 --> 0:44:39,346
and your feature.

1471
00:44:40,446 --> 0:44:42,006
A very common example of using

1472
00:44:42,006 --> 0:44:43,196
implicit feedback is

1473
00:44:43,266 --> 0:44:44,126
personalization.

1474
00:44:44,886 --> 0:44:46,616
For example, Siri personalizes

1475
00:44:46,736 --> 0:44:48,796
the search experiences on iOS

1476
00:44:49,166 --> 0:44:50,336
based on how you use your

1477
00:44:50,336 --> 0:44:50,836
device.

1478
00:44:51,366 --> 0:44:53,716
When you trigger the Search bar

1479
00:44:53,716 --> 0:44:55,046
on your Home screen, Siri

1480
00:44:55,046 --> 0:44:56,536
presents a range of apps you

1481
00:44:56,536 --> 0:44:57,436
might want to use.

1482
00:44:58,366 --> 0:45:00,206
Which apps appear here depends

1483
00:44:58,366 --> 0:45:00,206
Which apps appear here depends

1484
00:45:00,206 --> 0:45:01,756
on the implicit feedback you

1485
00:45:01,756 --> 0:45:02,476
give to Siri.

1486
00:45:03,186 --> 0:45:04,096
These might be apps you

1487
00:45:04,096 --> 0:45:05,686
frequently used, maybe ones

1488
00:45:05,686 --> 0:45:07,466
you've just used or ones you use

1489
00:45:07,466 --> 0:45:08,846
typically at this time of day.

1490
00:45:09,826 --> 0:45:11,196
For example in the car, I might

1491
00:45:11,196 --> 0:45:13,086
get Maps so I can get directions

1492
00:45:13,496 --> 0:45:14,896
or music and podcasts so I have

1493
00:45:14,896 --> 0:45:15,816
something to listen to.

1494
00:45:16,786 --> 0:45:18,306
At work, I might get apps that

1495
00:45:18,306 --> 0:45:19,706
help me at a meeting like Notes

1496
00:45:19,706 --> 0:45:21,756
or Reminders and at home, I

1497
00:45:21,756 --> 0:45:22,926
might get Messages to text

1498
00:45:22,926 --> 0:45:25,116
friends or family or suggestions

1499
00:45:25,116 --> 0:45:26,956
to check my activity or news.

1500
00:45:27,446 --> 0:45:30,206
Based on how I use these apps,

1501
00:45:30,356 --> 0:45:32,306
Siri tries to identify my intent

1502
00:45:32,826 --> 0:45:34,286
so it can surface apps to help

1503
00:45:34,286 --> 0:45:35,686
me achieve that intent.

1504
00:45:36,626 --> 0:45:37,626
So when you're using implicit

1505
00:45:37,626 --> 0:45:39,396
feedback, strive to identify

1506
00:45:39,396 --> 0:45:41,116
people's intent by looking at

1507
00:45:41,196 --> 0:45:42,466
how they interact with your

1508
00:45:42,466 --> 0:45:42,856
feature.

1509
00:45:43,386 --> 0:45:47,466
Over time, Siri will get better

1510
00:45:47,466 --> 0:45:49,296
at understanding my intent and,

1511
00:45:49,296 --> 0:45:50,666
apart from just showing apps as

1512
00:45:50,666 --> 0:45:52,106
a suggestion, it might also

1513
00:45:52,106 --> 0:45:53,846
start suggesting shortcuts of

1514
00:45:53,896 --> 0:45:55,356
actions I frequently perform.

1515
00:45:56,436 --> 0:45:57,976
For example in the car, I might

1516
00:45:57,976 --> 0:45:59,176
get directions to places I

1517
00:45:59,246 --> 0:46:01,236
frequently visit or the location

1518
00:45:59,246 --> 0:46:01,236
frequently visit or the location

1519
00:46:01,236 --> 0:46:02,086
of my next meeting.

1520
00:46:03,356 --> 0:46:04,856
At work, I might get shortcuts

1521
00:46:04,886 --> 0:46:06,536
to my meeting notes or my work

1522
00:46:06,536 --> 0:46:07,146
reminders.

1523
00:46:07,986 --> 0:46:09,236
And at home, I might get

1524
00:46:09,236 --> 0:46:10,456
shortcuts to turn on the lights

1525
00:46:10,796 --> 0:46:12,256
or message or FaceTime my

1526
00:46:12,256 --> 0:46:13,616
closest friends and family.

1527
00:46:15,076 --> 0:46:16,776
These shortcuts appear after a

1528
00:46:16,776 --> 0:46:17,686
few days or weeks.

1529
00:46:18,206 --> 0:46:19,666
The more specific they are, the

1530
00:46:19,666 --> 0:46:21,256
more certain Siri needs to be

1531
00:46:21,346 --> 0:46:22,176
about my intent.

1532
00:46:23,196 --> 0:46:24,506
As I said, these shortcuts are

1533
00:46:24,506 --> 0:46:26,696
based on implicit feedback, so

1534
00:46:26,696 --> 0:46:27,976
it might take a while for Siri

1535
00:46:27,976 --> 0:46:29,366
to be certain enough about a

1536
00:46:29,366 --> 0:46:31,136
suggestion and that's OK.

1537
00:46:32,206 --> 0:46:33,496
It's better to be patient and

1538
00:46:33,496 --> 0:46:34,896
become certain of a suggestion

1539
00:46:35,406 --> 0:46:36,636
than to be quick and show an

1540
00:46:36,666 --> 0:46:37,616
unhelpful suggestion.

1541
00:46:38,206 --> 0:46:40,566
So bear in mind that implicit

1542
00:46:40,606 --> 0:46:42,946
feedback can be slow but it also

1543
00:46:42,946 --> 0:46:44,666
can be more accurate over time.

1544
00:46:45,266 --> 0:46:48,966
Now these suggestions can appear

1545
00:46:48,966 --> 0:46:50,596
on the Lock screen and it might

1546
00:46:50,646 --> 0:46:51,676
be that they contain some

1547
00:46:51,676 --> 0:46:52,726
sensitive information.

1548
00:46:53,246 --> 0:46:54,956
For example, it might show an

1549
00:46:54,956 --> 0:46:55,996
event you want to keep as a

1550
00:46:55,996 --> 0:46:58,356
surprise or notes you might want

1551
00:46:58,496 --> 0:46:59,426
to keep secret.

1552
00:47:00,096 --> 0:47:02,196
To respect people's privacy, we

1553
00:47:02,196 --> 0:47:03,316
added settings for you to

1554
00:47:03,316 --> 0:47:05,286
control which app and which app

1555
00:47:05,286 --> 0:47:07,306
shortcuts can appear in Search.

1556
00:47:07,866 --> 0:47:11,326
So respect people's privacy and

1557
00:47:11,326 --> 0:47:12,706
give them full control over

1558
00:47:12,706 --> 0:47:13,406
their information.

1559
00:47:13,866 --> 0:47:15,756
Treat it privately and securely.

1560
00:47:16,346 --> 0:47:19,246
So implicit feedback is a great

1561
00:47:19,246 --> 0:47:21,116
tool for personalization but you

1562
00:47:21,116 --> 0:47:22,486
can also use it for some maybe

1563
00:47:22,486 --> 0:47:24,476
less obvious purposes, for

1564
00:47:24,476 --> 0:47:25,586
example simply making a

1565
00:47:25,586 --> 0:47:27,586
direction faster or more

1566
00:47:27,586 --> 0:47:27,976
accurate.

1567
00:47:29,126 --> 0:47:30,386
One good example has been with

1568
00:47:30,386 --> 0:47:31,986
us since the beginning of iOS

1569
00:47:31,986 --> 0:47:32,716
and that's the keyboard.

1570
00:47:33,326 --> 0:47:36,456
Each key on the keyboard has a

1571
00:47:36,456 --> 0:47:38,466
touch area and the keyboard uses

1572
00:47:38,466 --> 0:47:39,996
machine learning to optimize

1573
00:47:39,996 --> 0:47:41,576
these touch areas based on what

1574
00:47:41,576 --> 0:47:41,936
you type.

1575
00:47:43,526 --> 0:47:44,936
Each touch area might become

1576
00:47:44,936 --> 0:47:46,516
bigger or smaller depending on

1577
00:47:46,516 --> 0:47:47,996
the word you're typing or the

1578
00:47:47,996 --> 0:47:49,616
way your fingers are positioned

1579
00:47:49,616 --> 0:47:51,976
on the keyboard.

1580
00:47:52,086 --> 0:47:52,986
Now notice that we're not

1581
00:47:53,076 --> 0:47:54,586
visibly making these buttons any

1582
00:47:54,586 --> 0:47:55,466
bigger or smaller.

1583
00:47:55,536 --> 0:47:57,356
The keyboard always appears in

1584
00:47:57,356 --> 0:47:57,966
the same way.

1585
00:47:58,636 --> 0:48:00,046
But over time, the keyboard

1586
00:47:58,636 --> 0:48:00,046
But over time, the keyboard

1587
00:48:00,046 --> 0:48:01,106
might start to feel more

1588
00:48:01,106 --> 0:48:02,806
accurate and more personalized

1589
00:48:02,966 --> 0:48:03,956
to you.

1590
00:48:04,996 --> 0:48:06,926
So use implicit feedback to make

1591
00:48:06,926 --> 0:48:08,376
your interactions more accurate

1592
00:48:08,706 --> 0:48:09,306
and delightful.

1593
00:48:12,856 --> 0:48:15,196
One last example are the Siri

1594
00:48:15,196 --> 0:48:16,556
suggestions in Safari.

1595
00:48:17,256 --> 0:48:18,916
Safari uses machine learning to

1596
00:48:18,916 --> 0:48:20,196
collect links from messages,

1597
00:48:20,416 --> 0:48:22,176
mail, your reading lists, your

1598
00:48:22,176 --> 0:48:23,446
iCloud tabs, and other places.

1599
00:48:24,216 --> 0:48:25,376
And hopefully these suggestions

1600
00:48:25,376 --> 0:48:26,716
are good and they help you

1601
00:48:26,716 --> 0:48:28,106
discover interesting content

1602
00:48:28,106 --> 0:48:29,866
from your friends and family or

1603
00:48:29,866 --> 0:48:31,156
ones you've saved yourself.

1604
00:48:31,806 --> 0:48:33,196
Occasionally, though, you might

1605
00:48:33,196 --> 0:48:34,496
get suggestions you'd rather not

1606
00:48:34,496 --> 0:48:36,536
see, maybe an article you're not

1607
00:48:36,536 --> 0:48:38,016
interested in or a source you

1608
00:48:38,016 --> 0:48:38,846
don't really trust.

1609
00:48:39,816 --> 0:48:41,216
If all of these suggestions turn

1610
00:48:41,216 --> 0:48:42,786
out to be unhelpful, you might

1611
00:48:42,786 --> 0:48:44,646
start losing trust in the

1612
00:48:44,646 --> 0:48:46,906
suggestions or even worse, start

1613
00:48:46,906 --> 0:48:48,406
to lose trust in Safari.

1614
00:48:48,906 --> 0:48:50,206
And that is obviously something

1615
00:48:50,206 --> 0:48:51,146
we would like to avoid.

1616
00:48:52,306 --> 0:48:53,766
Rubii talked about attribution

1617
00:48:53,766 --> 0:48:54,936
that can really help here in

1618
00:48:54,936 --> 0:48:56,226
explaining why we're showing

1619
00:48:56,226 --> 0:48:58,206
these suggestions but we also

1620
00:48:58,206 --> 0:48:59,176
want to make sure you can

1621
00:48:59,176 --> 0:49:01,086
control these suggestions so you

1622
00:48:59,176 --> 0:49:01,086
control these suggestions so you

1623
00:49:01,086 --> 0:49:02,016
can get rid of the ones you

1624
00:49:02,016 --> 0:49:03,426
don't want to see and make sure

1625
00:49:03,426 --> 0:49:04,696
they don't appear again.

1626
00:49:05,646 --> 0:49:06,796
And that brings us to our next

1627
00:49:06,796 --> 0:49:08,716
pattern which is explicit

1628
00:49:09,276 --> 0:49:09,466
feedback.

1629
00:49:11,176 --> 0:49:12,976
Explicit feedback allows your

1630
00:49:12,976 --> 0:49:14,646
app to collect information by

1631
00:49:14,646 --> 0:49:16,596
asking specific questions about

1632
00:49:16,596 --> 0:49:17,206
your results.

1633
00:49:17,756 --> 0:49:20,096
In the previous example, it

1634
00:49:20,096 --> 0:49:21,236
would be great if I could get

1635
00:49:21,236 --> 0:49:22,486
feedback on these suggestions

1636
00:49:22,536 --> 0:49:24,276
that I don't want to see and

1637
00:49:24,276 --> 0:49:26,036
that way the model can learn and

1638
00:49:26,036 --> 0:49:27,166
can avoid showing similar

1639
00:49:27,166 --> 0:49:28,696
suggestions in the future.

1640
00:49:30,056 --> 0:49:31,166
So how do I design these

1641
00:49:31,206 --> 0:49:31,976
feedback actions?

1642
00:49:32,556 --> 0:49:35,276
Here are two actions that

1643
00:49:35,356 --> 0:49:36,726
frequently appear when we're

1644
00:49:36,726 --> 0:49:38,186
using explicit feedback: the

1645
00:49:38,186 --> 0:49:39,216
heart and the heart with a

1646
00:49:39,216 --> 0:49:39,736
strikethrough.

1647
00:49:40,486 --> 0:49:41,746
Now seeing these as buttons

1648
00:49:41,746 --> 0:49:42,656
leaves a lot of room for

1649
00:49:42,656 --> 0:49:43,426
interpretation.

1650
00:49:44,126 --> 0:49:45,536
It's unclear what will happen

1651
00:49:45,536 --> 0:49:47,386
when I press one of these even

1652
00:49:47,386 --> 0:49:48,856
if I add labels like Love or

1653
00:49:48,856 --> 0:49:49,336
Dislike.

1654
00:49:50,566 --> 0:49:52,946
Let's say I want to get feedback

1655
00:49:52,946 --> 0:49:54,046
on the third article here.

1656
00:49:54,616 --> 0:49:55,866
I can bring up the Action menu

1657
00:49:55,866 --> 0:49:57,256
and I can see the action Love.

1658
00:49:58,866 --> 0:49:59,806
That leaves a lot of room for

1659
00:49:59,806 --> 0:50:00,516
interpretation.

1660
00:49:59,806 --> 0:50:00,516
interpretation.

1661
00:50:00,956 --> 0:50:02,136
Does this mean I have to love

1662
00:50:02,186 --> 0:50:03,306
every article that I like?

1663
00:50:04,706 --> 0:50:06,056
Positive explicit feedback

1664
00:50:06,166 --> 0:50:07,526
implies additional work.

1665
00:50:08,236 --> 0:50:09,416
People might think they have to

1666
00:50:09,416 --> 0:50:10,996
give positive explicit feedback

1667
00:50:11,266 --> 0:50:13,196
to every article they like just

1668
00:50:13,196 --> 0:50:14,736
to get more good suggestions.

1669
00:50:15,626 --> 0:50:17,166
So we actually recommend to

1670
00:50:17,166 --> 0:50:18,536
prioritize negative feedback

1671
00:50:18,806 --> 0:50:19,546
over positive.

1672
00:50:20,346 --> 0:50:21,946
Positive feedback can better be

1673
00:50:21,946 --> 0:50:23,136
inferred through implicit

1674
00:50:23,136 --> 0:50:25,526
signals, for example me reading

1675
00:50:25,526 --> 0:50:27,426
the article, bookmarking it or

1676
00:50:27,426 --> 0:50:28,506
sharing it.

1677
00:50:29,716 --> 0:50:31,606
So instead of showing both Love

1678
00:50:31,606 --> 0:50:33,036
and Dislike, we can actually

1679
00:50:33,036 --> 0:50:34,606
only show Dislike.

1680
00:50:35,516 --> 0:50:36,746
But still, that leaves a lot of

1681
00:50:36,746 --> 0:50:37,666
room for interpretation.

1682
00:50:38,426 --> 0:50:39,816
If I dislike an article, am I

1683
00:50:39,816 --> 0:50:41,286
disliking the article, the

1684
00:50:41,286 --> 0:50:43,396
author, the source, the person

1685
00:50:43,396 --> 0:50:45,016
who sent it to me, or maybe even

1686
00:50:45,016 --> 0:50:46,216
the app in which I received it?

1687
00:50:46,986 --> 0:50:47,826
It's still difficult to

1688
00:50:47,826 --> 0:50:50,426
understand what will happen when

1689
00:50:50,426 --> 0:50:52,366
I tap Dislike.

1690
00:50:52,496 --> 0:50:53,906
Words like "suggest less" or

1691
00:50:53,906 --> 0:50:55,476
"hide the suggestion" make it

1692
00:50:55,506 --> 0:50:57,036
much easier to understand what

1693
00:50:57,036 --> 0:50:58,216
will happen when I tap such a

1694
00:50:58,216 --> 0:50:59,196
button.

1695
00:51:00,196 --> 0:51:01,216
To give people even more

1696
00:51:01,216 --> 0:51:02,676
control, we can allow them to

1697
00:51:02,676 --> 0:51:04,396
select exactly what they like to

1698
00:51:04,396 --> 0:51:06,356
see less, for example less of

1699
00:51:06,356 --> 0:51:08,086
the source, the person who sent

1700
00:51:08,086 --> 0:51:09,726
it to me or the app in which I

1701
00:51:09,726 --> 0:51:10,846
received it.

1702
00:51:12,336 --> 0:51:14,126
So when using explicit feedback,

1703
00:51:14,526 --> 0:51:16,146
clearly describe the option and

1704
00:51:16,146 --> 0:51:17,006
its consequences.

1705
00:51:17,666 --> 0:51:19,246
Use words that describe what

1706
00:51:19,246 --> 0:51:19,716
will happen.

1707
00:51:19,936 --> 0:51:21,456
And when possible, provide

1708
00:51:21,576 --> 0:51:22,716
different options to better

1709
00:51:22,716 --> 0:51:23,926
understand the user's

1710
00:51:23,926 --> 0:51:24,426
preference.

1711
00:51:25,076 --> 0:51:28,446
And of course, when I select one

1712
00:51:28,446 --> 0:51:29,956
of these options, the interface

1713
00:51:29,956 --> 0:51:31,126
should immediately reflect my

1714
00:51:31,126 --> 0:51:33,066
choice and hide the suggestions

1715
00:51:33,236 --> 0:51:34,536
that match my preference.

1716
00:51:37,076 --> 0:51:38,796
So always act immediately and

1717
00:51:38,796 --> 0:51:40,496
persistently when the user gives

1718
00:51:40,746 --> 0:51:42,066
explicit feedback.

1719
00:51:43,936 --> 0:51:45,226
So you've seen that explicit

1720
00:51:45,226 --> 0:51:46,636
feedback allows us to correct

1721
00:51:46,636 --> 0:51:48,196
suggestions that might be wrong,

1722
00:51:48,196 --> 0:51:49,646
unwanted or inappropriate.

1723
00:51:50,616 --> 0:51:51,546
But for some machine learning

1724
00:51:51,546 --> 0:51:53,106
features, using explicit

1725
00:51:53,106 --> 0:51:55,216
feedback might not be the right

1726
00:51:55,216 --> 0:51:56,766
choice or might not even be

1727
00:51:56,766 --> 0:51:57,296
possible.

1728
00:51:58,386 --> 0:52:00,346
Here's Angie again, the dog from

1729
00:51:58,386 --> 0:52:00,346
Here's Angie again, the dog from

1730
00:52:00,346 --> 0:52:00,726
our friends.

1731
00:52:01,006 --> 0:52:02,366
She's often the main topic in

1732
00:52:02,366 --> 0:52:03,276
our group conversation.

1733
00:52:04,716 --> 0:52:05,996
Now the keyboard uses machine

1734
00:52:05,996 --> 0:52:07,876
learning to suggest corrections

1735
00:52:07,876 --> 0:52:08,746
of what I'm typing.

1736
00:52:09,526 --> 0:52:10,816
And when I initially type the

1737
00:52:10,816 --> 0:52:12,366
word "Angie," it wanted to

1738
00:52:12,366 --> 0:52:14,036
correct it to the word "angle."

1739
00:52:14,856 --> 0:52:16,266
Now that's obviously wrong but

1740
00:52:16,266 --> 0:52:17,796
using explicit feedback here

1741
00:52:18,046 --> 0:52:19,196
seems a little bit off.

1742
00:52:19,736 --> 0:52:20,696
Let's say I can bring up a

1743
00:52:20,696 --> 0:52:21,696
contextual menu here.

1744
00:52:22,936 --> 0:52:24,346
This doesn't feel very intuitive

1745
00:52:24,686 --> 0:52:25,896
and it doesn't really allow me

1746
00:52:25,896 --> 0:52:27,386
to say what I actually meant.

1747
00:52:28,746 --> 0:52:30,716
Instead, I can simply select the

1748
00:52:30,716 --> 0:52:33,436
words and correct the suggestion

1749
00:52:33,436 --> 0:52:34,516
the keyboard has made by

1750
00:52:34,516 --> 0:52:36,626
retyping it and this time the

1751
00:52:36,626 --> 0:52:38,066
keyboard will not autocorrect

1752
00:52:38,066 --> 0:52:38,166
it.

1753
00:52:38,826 --> 0:52:39,976
The keyboard learned from my

1754
00:52:39,976 --> 0:52:41,706
correction and the next time I

1755
00:52:41,706 --> 0:52:43,636
type in Angie, it knows I'm

1756
00:52:43,636 --> 0:52:45,386
referring to a name and not the

1757
00:52:45,386 --> 0:52:46,176
word "angle."

1758
00:52:47,186 --> 0:52:48,696
This is the most obvious example

1759
00:52:48,906 --> 0:52:50,436
of our last pattern which is

1760
00:52:50,556 --> 0:52:51,106
corrections.

1761
00:52:53,256 --> 0:52:55,596
Corrections allow people to fix

1762
00:52:55,596 --> 0:52:57,316
a mistake a model has made by

1763
00:52:57,316 --> 0:52:58,336
using known tasks.

1764
00:52:58,796 --> 0:52:59,986
And with known tasks, I'll

1765
00:52:59,986 --> 0:53:00,766
explain what that means.

1766
00:52:59,986 --> 0:53:00,766
explain what that means.

1767
00:53:01,806 --> 0:53:03,106
In the previous example, you've

1768
00:53:03,106 --> 0:53:04,346
seen that we corrected this

1769
00:53:04,676 --> 0:53:08,966
using standard text controls.

1770
00:53:09,486 --> 0:53:10,816
There was no new interface that

1771
00:53:10,816 --> 0:53:11,476
we showed here.

1772
00:53:11,946 --> 0:53:13,436
Everybody knows how to do this

1773
00:53:13,506 --> 0:53:14,686
since this is how the keyboard's

1774
00:53:14,906 --> 0:53:15,536
always worked.

1775
00:53:16,736 --> 0:53:17,986
Corrections are, therefore, an

1776
00:53:17,986 --> 0:53:19,536
amazing pattern to optimize your

1777
00:53:19,536 --> 0:53:21,436
results without feeling like

1778
00:53:21,436 --> 0:53:22,076
extra work.

1779
00:53:22,626 --> 0:53:24,916
Let me give another example.

1780
00:53:25,976 --> 0:53:27,526
Photos uses machine learning to

1781
00:53:27,526 --> 0:53:28,556
optimize your pictures.

1782
00:53:29,216 --> 0:53:30,496
It can help find to the right

1783
00:53:30,496 --> 0:53:32,096
rotation or cropping your

1784
00:53:32,096 --> 0:53:32,976
pictures could take.

1785
00:53:34,276 --> 0:53:35,536
The way Photos suggest these

1786
00:53:35,536 --> 0:53:38,406
croppings or rotations is quite

1787
00:53:38,406 --> 0:53:38,746
subtle.

1788
00:53:39,576 --> 0:53:40,806
When you go into Edit mode and

1789
00:53:40,806 --> 0:53:42,056
select Rotation or Cropping

1790
00:53:42,056 --> 0:53:44,456
tool, photos will crop or rotate

1791
00:53:44,456 --> 0:53:45,336
the picture for you.

1792
00:53:46,096 --> 0:53:47,386
Now it doesn't apply the

1793
00:53:47,476 --> 0:53:49,346
rotation or cropping, it simply

1794
00:53:49,346 --> 0:53:50,866
suggests it as a starting point.

1795
00:53:51,936 --> 0:53:53,036
If you like what Photos has

1796
00:53:53,096 --> 0:53:54,596
done, you can simply tap Done

1797
00:53:54,976 --> 0:53:56,966
and the rotation or cropping is

1798
00:53:56,966 --> 0:53:57,346
applied.

1799
00:53:57,346 --> 0:53:59,276
But if you'd like to change what

1800
00:53:59,346 --> 0:54:01,066
Photos has suggested, you can

1801
00:53:59,346 --> 0:54:01,066
Photos has suggested, you can

1802
00:54:01,066 --> 0:54:02,566
simply use the slider at the

1803
00:54:02,606 --> 0:54:04,256
bottom to rotate or drag the

1804
00:54:04,256 --> 0:54:05,346
corners to crop.

1805
00:54:05,346 --> 0:54:07,386
So these are great examples of

1806
00:54:07,416 --> 0:54:07,886
corrections.

1807
00:54:08,426 --> 0:54:10,046
We show you familiar controls

1808
00:54:10,516 --> 0:54:11,196
and we learn from the

1809
00:54:11,196 --> 0:54:12,356
corrections you make.

1810
00:54:13,046 --> 0:54:15,936
So that concludes the last

1811
00:54:15,936 --> 0:54:16,236
pattern.

1812
00:54:17,276 --> 0:54:18,236
Allow corrections through

1813
00:54:18,236 --> 0:54:18,906
familiar ways.

1814
00:54:19,416 --> 0:54:20,866
This makes it easy and fast for

1815
00:54:20,866 --> 0:54:21,696
somebody to correct.

1816
00:54:22,666 --> 0:54:24,136
Provide immediate value when

1817
00:54:24,136 --> 0:54:25,256
somebody makes a correction.

1818
00:54:25,466 --> 0:54:26,406
As we saw with the text

1819
00:54:26,406 --> 0:54:28,396
correction, the keyboard won't

1820
00:54:28,396 --> 0:54:29,616
autocorrect things that we

1821
00:54:29,616 --> 0:54:29,926
typed.

1822
00:54:30,856 --> 0:54:32,436
And use corrections as implicit

1823
00:54:32,436 --> 0:54:32,886
feedback.

1824
00:54:33,276 --> 0:54:34,446
Corrections work great as

1825
00:54:34,446 --> 0:54:36,076
implicit feedback to improve

1826
00:54:36,396 --> 0:54:37,766
your machine learning results.

1827
00:54:38,346 --> 0:54:41,566
So those are all the interface

1828
00:54:41,566 --> 0:54:42,676
patterns we wanted to show you

1829
00:54:42,676 --> 0:54:42,996
today.

1830
00:54:43,766 --> 0:54:45,026
As you can see, many of these

1831
00:54:45,026 --> 0:54:46,536
patterns leverage existing

1832
00:54:46,536 --> 0:54:48,216
interface elements we've had for

1833
00:54:48,216 --> 0:54:48,816
a long time.

1834
00:54:49,586 --> 0:54:51,436
Machine learning, therefore, is

1835
00:54:51,436 --> 0:54:52,916
an elevation of what we've been

1836
00:54:52,916 --> 0:54:53,756
doing all along.

1837
00:54:54,436 --> 0:54:55,236
These patterns are not

1838
00:54:55,236 --> 0:54:56,886
necessarily new but they are

1839
00:54:56,966 --> 0:54:57,976
applied in a new context.

1840
00:54:58,096 --> 0:55:00,536
And whenever we choose our

1841
00:54:58,096 --> 0:55:00,536
And whenever we choose our

1842
00:55:00,536 --> 0:55:02,046
patterns, we have to make sure

1843
00:55:02,046 --> 0:55:03,446
we respect people's work.

1844
00:55:04,456 --> 0:55:06,266
Adding additional attribution or

1845
00:55:06,266 --> 0:55:08,246
additional feedback actions asks

1846
00:55:08,246 --> 0:55:09,496
even more information for

1847
00:55:09,496 --> 0:55:11,246
someone to parse and that

1848
00:55:11,246 --> 0:55:12,406
invades their attention and

1849
00:55:12,406 --> 0:55:13,996
might distract them from their

1850
00:55:13,996 --> 0:55:14,786
task at hand.

1851
00:55:15,536 --> 0:55:16,816
So choose your patterns with

1852
00:55:16,866 --> 0:55:18,466
people in mind first.

1853
00:55:18,946 --> 0:55:22,416
So designing a machine learning

1854
00:55:22,416 --> 0:55:23,786
experience is not just

1855
00:55:23,786 --> 0:55:24,946
outputting your results in an

1856
00:55:24,946 --> 0:55:25,446
interface.

1857
00:55:26,376 --> 0:55:27,626
The way we select our data,

1858
00:55:27,806 --> 0:55:29,526
metrics, inputs and outputs

1859
00:55:29,836 --> 0:55:31,546
plays a very important role in

1860
00:55:31,546 --> 0:55:32,746
making these experiences

1861
00:55:32,816 --> 0:55:34,856
understandable, intuitive and

1862
00:55:34,856 --> 0:55:35,256
delightful.

1863
00:55:35,906 --> 0:55:39,716
Now to end our talk, I want to

1864
00:55:39,716 --> 0:55:40,916
go back to something that Kayur

1865
00:55:40,916 --> 0:55:41,786
said in the beginning.

1866
00:55:42,796 --> 0:55:44,156
Apple uses machine learning to

1867
00:55:44,156 --> 0:55:45,906
create products and experiences

1868
00:55:46,186 --> 0:55:47,416
we could never create before,

1869
00:55:47,876 --> 0:55:49,136
products and experiences we

1870
00:55:49,186 --> 0:55:49,956
really care about.

1871
00:55:51,156 --> 0:55:52,236
You've seen examples of how

1872
00:55:52,236 --> 0:55:53,826
machine learning helps us manage

1873
00:55:53,826 --> 0:55:55,986
our attention, on how it allows

1874
00:55:55,986 --> 0:55:56,676
us to give you better

1875
00:55:56,676 --> 0:55:58,656
recommendations, on how it can

1876
00:55:58,656 --> 0:56:00,276
present contextual information

1877
00:55:58,656 --> 0:56:00,276
present contextual information

1878
00:56:00,276 --> 0:56:01,896
at the right time, and how we

1879
00:56:01,896 --> 0:56:03,946
can automate mundane tasks so

1880
00:56:04,156 --> 0:56:05,646
you can focus on what matters.

1881
00:56:06,446 --> 0:56:07,716
But machine learning can help us

1882
00:56:07,716 --> 0:56:09,086
achieve much more than that.

1883
00:56:10,356 --> 0:56:11,846
Apple has always used technology

1884
00:56:11,846 --> 0:56:13,886
to empower everyone and machine

1885
00:56:13,886 --> 0:56:15,486
learning only elevates that

1886
00:56:15,486 --> 0:56:16,116
capability.

1887
00:56:16,716 --> 0:56:19,416
It can empower us to be healthy

1888
00:56:19,736 --> 0:56:22,186
by measuring our activity and

1889
00:56:23,016 --> 0:56:25,746
our steps.

1890
00:56:25,936 --> 0:56:27,336
Machine learning allows us to be

1891
00:56:27,336 --> 0:56:29,206
more inclusive by leveraging our

1892
00:56:29,476 --> 0:56:31,426
communication, for example by

1893
00:56:31,426 --> 0:56:32,776
adding voice control to the

1894
00:56:32,776 --> 0:56:34,456
camera so a blind person can see

1895
00:56:34,456 --> 0:56:35,886
what they're holding or it can

1896
00:56:35,886 --> 0:56:37,336
know when their loved ones are

1897
00:56:38,116 --> 0:56:40,216
in frame.

1898
00:56:40,396 --> 0:56:41,566
Machine learning helps us to be

1899
00:56:41,566 --> 0:56:43,416
safe by detecting problems with

1900
00:56:43,416 --> 0:56:46,016
our heart by measuring our heart

1901
00:56:46,016 --> 0:56:48,126
beats or an ECG.

1902
00:56:48,346 --> 0:56:49,976
And lastly, it can allow us to

1903
00:56:49,976 --> 0:56:51,656
be creative by seeing the world

1904
00:56:51,656 --> 0:56:53,606
in a whole new way, for example

1905
00:56:53,606 --> 0:56:54,856
by letting a known tool like a

1906
00:56:54,856 --> 0:56:56,946
pencil do magic things on

1907
00:56:56,946 --> 0:56:57,286
screen.

1908
00:56:58,896 --> 0:57:00,926
So you can see when we align the

1909
00:56:58,896 --> 0:57:00,926
So you can see when we align the

1910
00:57:00,926 --> 0:57:02,836
we create with the values we

1911
00:57:02,836 --> 0:57:04,246
uphold, we can create

1912
00:57:04,246 --> 0:57:05,786
experiences that really elevate

1913
00:57:05,846 --> 0:57:06,766
the best of humanity.

1914
00:57:07,316 --> 0:57:09,186
That's everything we had in

1915
00:57:09,186 --> 0:57:10,516
store for you and we hope we've

1916
00:57:10,566 --> 0:57:11,986
given you plenty of inspiration,

1917
00:57:11,986 --> 0:57:13,576
insight into designing great

1918
00:57:13,576 --> 0:57:14,716
machine learning experiences.

1919
00:57:15,806 --> 0:57:17,086
All the patterns we talked about

1920
00:57:17,086 --> 0:57:18,646
and more are available on the

1921
00:57:18,646 --> 0:57:20,456
Human Interface Guidelines as a

1922
00:57:21,066 --> 0:57:21,196
beta.

1923
00:57:21,876 --> 0:57:23,416
There are still a few more talks

1924
00:57:23,416 --> 0:57:24,556
this afternoon about machine

1925
00:57:24,556 --> 0:57:26,026
learning and all of us will be

1926
00:57:26,026 --> 0:57:28,196
available in the coming hour at

1927
00:57:28,196 --> 0:57:29,546
the User Interface Design Lab to

1928
00:57:29,546 --> 0:57:30,706
answer any questions you might

1929
00:57:30,706 --> 0:57:30,946
have.

1930
00:57:31,226 --> 0:57:32,916
So make sure to come by and say

1931
00:57:32,916 --> 0:57:33,146
hi.

1932
00:57:33,866 --> 0:57:34,936
Thank you all for attending and

1933
00:57:34,936 --> 0:57:35,966
enjoy the last day of WWDC.

1934
00:57:36,016 --> 0:57:38,000
[ Applause ]
