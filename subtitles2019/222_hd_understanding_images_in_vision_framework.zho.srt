1
00:00:00,506 --> 0:00:04,500
[音乐]

2
00:00:07,516 --> 0:00:14,196
[掌声]

3
00:00:14,696 --> 0:00:15,336
&gt;&gt; 早上好！

4
00:00:15,676 --> 0:00:16,976
我叫 Brittany Weinert

5
00:00:16,976 --> 0:00:18,866
我是来自 Vision 框架团队的软件工程师

6
00:00:19,566 --> 0:00:20,596
今年 我们团队

7
00:00:20,596 --> 0:00:22,086
带来了许多新技术和新产品

8
00:00:22,086 --> 0:00:23,316
相信你会感到兴奋并且爱上它们

9
00:00:23,916 --> 0:00:24,846
因为我们有太多

10
00:00:24,846 --> 0:00:26,076
新东西要介绍

11
00:00:26,076 --> 0:00:27,106
所以我们接下来

12
00:00:27,106 --> 0:00:27,516
将深入研究这些新特点

13
00:00:27,876 --> 0:00:29,256
如果你对我们完全不了解

14
00:00:29,256 --> 0:00:30,706
也不要担心

15
00:00:30,706 --> 0:00:31,466
我想你依然可以跟得上

16
00:00:31,466 --> 0:00:33,096
我们希望

17
00:00:33,096 --> 0:00:34,216
我们今天推出的这些新功能

18
00:00:34,216 --> 0:00:35,956
能够激励你对我们团队有更多了解

19
00:00:35,956 --> 0:00:39,606
并且能够将其运用到你的 App 当中

20
00:00:39,836 --> 0:00:40,906
今天我们将谈论到

21
00:00:40,906 --> 0:00:44,206
四个全新的主题 

22
00:00:44,416 --> 0:00:46,066
显着性 图像分类  

23
00:00:46,066 --> 0:00:47,556
图像相似性和面部质量

24
00:00:47,556 --> 0:00:49,456
我们也做了一些技术升级

25
00:00:49,456 --> 0:00:50,796
包括目标跟踪程序 面部标记

26
00:00:50,796 --> 0:00:52,826
新的探测器以及

27
00:00:52,826 --> 0:00:55,196
优化了的 Core ML 机器学习框架支持系统

28
00:00:55,196 --> 0:00:59,276
今天 我要和大家分享的是显著性

29
00:00:59,736 --> 0:01:00,836
让我们从定义开始

30
00:00:59,736 --> 0:01:00,836
让我们从定义开始

31
00:01:01,386 --> 0:01:02,806
我要给你们看一张照片

32
00:01:02,806 --> 0:01:06,426
需要你们注意

33
00:01:06,426 --> 0:01:08,036
第一眼就吸引你们的地方

34
00:01:08,526 --> 0:01:13,046
当你第一次看到这张照片

35
00:01:13,046 --> 0:01:14,126
三只海雀坐在悬崖上

36
00:01:14,126 --> 0:01:15,696
你有没有注意到

37
00:01:15,696 --> 0:01:16,366
自己首先看到的是什么

38
00:01:17,326 --> 0:01:19,956
根据我们的模型

39
00:01:19,956 --> 0:01:21,226
大部分人首先看到的

40
00:01:21,226 --> 0:01:21,476
是海雀的脸

41
00:01:22,436 --> 0:01:23,556
这就是显著性

42
00:01:24,006 --> 0:01:26,366
它包含两种类型

43
00:01:26,766 --> 0:01:28,156
基于注意力类

44
00:01:28,216 --> 0:01:28,566
以及基于客体性类型

45
00:01:29,086 --> 0:01:30,976
我们在海雀图像上

46
00:01:30,976 --> 0:01:32,566
看到的覆盖层叫做热图

47
00:01:32,566 --> 0:01:35,686
是基于注意力的显著性产生的

48
00:01:36,146 --> 0:01:37,216
但在我们探讨更多的可视化示例之前

49
00:01:37,216 --> 0:01:40,246
我想先介绍每个算法的基础知识

50
00:01:41,776 --> 0:01:45,186
基于注意力的显著性

51
00:01:45,186 --> 0:01:47,786
是人类预期的显著性

52
00:01:47,786 --> 0:01:49,436
我的意思是

53
00:01:49,436 --> 0:01:51,006
基于注意力的显著性模型

54
00:01:51,006 --> 0:01:53,566
是产生于人们在看到一系列图像时

55
00:01:53,906 --> 0:01:55,446
所看到的东西

56
00:01:56,326 --> 0:01:57,646
这意味着当人们看到图像时 

57
00:01:57,646 --> 0:02:00,026
热图会反射

58
00:01:57,646 --> 0:02:00,026
热图会反射

59
00:02:00,026 --> 0:02:01,646
并突出显示他们首先看到的地方

60
00:02:02,676 --> 0:02:04,556
另一方面

61
00:02:04,556 --> 0:02:06,006
基于对象的显著性

62
00:02:06,056 --> 0:02:07,976
在图像中进行主题分割训练

63
00:02:08,515 --> 0:02:13,176
以突出前景对象或图像的主题

64
00:02:13,766 --> 0:02:15,996
因此在热图中

65
00:02:15,996 --> 0:02:17,656
主体或前景对象会突出显示

66
00:02:18,226 --> 0:02:19,916
现在我们来看一些例子

67
00:02:20,496 --> 0:02:23,946
这是早先的海雀

68
00:02:24,636 --> 0:02:26,956
这是覆盖在图像上的

69
00:02:26,956 --> 0:02:28,436
基于注意力的热图

70
00:02:29,446 --> 0:02:33,046
这是基于对象的热图

71
00:02:33,046 --> 0:02:34,806
正如我所说 

72
00:02:34,806 --> 0:02:36,056
人们往往先看海雀的脸

73
00:02:36,346 --> 0:02:37,686
所以海雀头部周围的区域

74
00:02:37,686 --> 0:02:39,956
在基于注意力的热图上非常明显

75
00:02:40,896 --> 0:02:42,286
对于对象

76
00:02:42,286 --> 0:02:43,676
我们只是试图找到主题

77
00:02:43,676 --> 0:02:45,246
在这种情况下

78
00:02:45,246 --> 0:02:45,636
这是三个海雀

79
00:02:45,786 --> 0:02:47,266
所以所有的海雀都会突出显示

80
00:02:48,486 --> 0:02:50,206
下面让我们来看看

81
00:02:50,206 --> 0:02:51,206
显著性在人物图像中的表现

82
00:02:51,746 --> 0:02:56,556
基于注意力的显著性

83
00:02:57,046 --> 0:02:58,656
人民面部周围的区域

84
00:02:58,656 --> 0:02:59,926
往往是最突出的

85
00:03:00,536 --> 0:03:02,146
不出所料

86
00:03:02,146 --> 0:03:04,146
因为我们倾向于先看人的脸

87
00:03:04,846 --> 0:03:06,616
对于基于对象的显著性

88
00:03:06,616 --> 0:03:08,196
如果这个人是形象的主体

89
00:03:08,196 --> 0:03:10,386
整个人应该突出显示

90
00:03:12,596 --> 0:03:14,896
因此我要说

91
00:03:14,896 --> 0:03:16,266
基于注意力的显著性

92
00:03:16,266 --> 0:03:17,846
在两种显著性中更为复杂

93
00:03:18,436 --> 0:03:19,926
因为它是由许多

94
00:03:19,926 --> 0:03:21,386
人为因素决定的

95
00:03:23,846 --> 0:03:25,126
决定注意力的显着性

96
00:03:25,126 --> 0:03:26,236
以及突出与否的主要因素

97
00:03:26,236 --> 0:03:28,426
是对比度 面部

98
00:03:28,916 --> 0:03:31,046
主体 视野和光线

99
00:03:32,416 --> 0:03:33,666
但有趣的是

100
00:03:33,666 --> 0:03:36,026
它也可能受到感知运动的影响

101
00:03:36,556 --> 0:03:39,816
在这个例子中

102
00:03:39,816 --> 0:03:42,436
伞的颜色真的很亮眼

103
00:03:42,436 --> 0:03:43,716
所以伞周围的区域是显著的

104
00:03:44,026 --> 0:03:45,636
不过路也很突出

105
00:03:46,006 --> 0:03:47,666
因为我们的眼睛

106
00:03:47,666 --> 0:03:48,796
试图追踪伞的前进方向

107
00:03:50,706 --> 0:03:52,216
对于基于对象的显著性

108
00:03:52,216 --> 0:03:55,096
我们只是找出了打伞的人

109
00:03:55,316 --> 0:03:56,876
我可以整天这样做

110
00:03:56,876 --> 0:03:58,456
给你们展示更多的例子

111
00:03:58,456 --> 0:03:59,456
但老实说

112
00:03:59,456 --> 0:04:00,966
想要理解显著性

113
00:03:59,456 --> 0:04:00,966
想要理解显著性

114
00:04:00,966 --> 0:04:01,766
最好的方法还是自己尝试一下

115
00:04:02,476 --> 0:04:03,776
在这里我鼓励大家

116
00:04:03,776 --> 0:04:05,726
下载 Saliency App 

117
00:04:05,726 --> 0:04:07,136
并在你们自己的照片库中试试

118
00:04:07,796 --> 0:04:10,536
接下来让我们来研究一下

119
00:04:10,536 --> 0:04:12,236
从显著性请求中返回的内容

120
00:04:12,236 --> 0:04:15,726
主要是热图

121
00:04:16,055 --> 0:04:17,375
到目前为止我向你展示的图像里

122
00:04:17,375 --> 0:04:19,596
热图已经缩放

123
00:04:19,596 --> 0:04:22,356
重叠 着色

124
00:04:22,356 --> 0:04:25,996
并放到图像上

125
00:04:25,996 --> 0:04:27,536
但实际上 

126
00:04:27,536 --> 0:04:29,656
热图是一个非常小的

127
00:04:29,656 --> 0:04:32,206
CV 像素缓冲

128
00:04:32,206 --> 0:04:34,186
由 0 到 1 的浮点数组成

129
00:04:34,186 --> 0:04:37,246
0 表示不存在

130
00:04:37,526 --> 0:04:39,746
1 表示最显著

131
00:04:40,256 --> 0:04:44,066
还需要进行额外的处理

132
00:04:44,066 --> 0:04:45,206
才能做得到

133
00:04:45,206 --> 0:04:46,746
和你在这里看到的效果完全一样

134
00:04:47,526 --> 0:04:48,896
不过现在我们一起来看

135
00:04:48,946 --> 0:04:51,716
如何提出最基本的请求

136
00:04:53,256 --> 0:04:56,686
好了  现在首先我们开始

137
00:04:56,686 --> 0:04:58,776
使用一个 VNImageRequestHandler

138
00:04:58,776 --> 0:05:00,206
来处理一个单一的图像

139
00:04:58,776 --> 0:05:00,206
来处理一个单一的图像

140
00:05:01,266 --> 0:05:04,346
下一步 选择你想要运行的算法

141
00:05:04,346 --> 0:05:06,516
在这种情况下选择 AttentionBasedSaliency

142
00:05:06,566 --> 0:05:09,256
如果你一直想使用同样的算法

143
00:05:09,256 --> 0:05:10,976
则设置 revision

144
00:05:12,976 --> 0:05:14,896
接下来

145
00:05:15,196 --> 0:05:17,466
你可以像通常那样执行请求

146
00:05:17,466 --> 0:05:19,476
如果成功了

147
00:05:19,476 --> 0:05:20,996
那么就应用 VNSaliencyImageObservation

148
00:05:20,996 --> 0:05:24,266
填充请求中的结果属性

149
00:05:25,326 --> 0:05:28,396
要访问热图

150
00:05:28,396 --> 0:05:30,086
你需要像这样

151
00:05:30,086 --> 0:05:32,906
调用 VNSaliencyImageObservation上的

152
00:05:35,256 --> 0:05:35,356
pixelBuffer 属性

153
00:05:35,596 --> 0:05:37,516
如果你想做基于对象的显著性

154
00:05:37,516 --> 0:05:39,316
那你就 必须

155
00:05:39,316 --> 0:05:44,246
要将请求名称和 revision 更改为 Objectness

156
00:05:45,426 --> 0:05:46,676
因此 值得注意的是

157
00:05:46,736 --> 0:05:48,206
基于注意力的是

158
00:05:48,206 --> 0:05:50,106
VNGenerateAttentionBasedSaliencyImageRequest

159
00:05:50,106 --> 0:05:50,836
基于对象的事

160
00:05:50,836 --> 0:05:53,076
VNGenerateObjectnessBasedSaliencyImageRequest

161
00:05:53,796 --> 0:05:56,856
那么现在 除了热图之外

162
00:05:56,856 --> 0:05:58,226
我们来看另一个工具

163
00:05:58,226 --> 0:05:58,856
即边界框

164
00:05:59,476 --> 0:06:03,196
边界框能够标记出来

165
00:05:59,476 --> 0:06:03,196
边界框能够标记出来

166
00:06:03,196 --> 0:06:04,796
图像的所有显著区域

167
00:06:05,156 --> 0:06:06,726
对于以注意力为基础的显著性来说

168
00:06:06,726 --> 0:06:08,006
你应该始终有一个边界框

169
00:06:08,006 --> 0:06:09,866
那就以对象为基础的显著性来说

170
00:06:09,866 --> 0:06:11,116
你可以增加到用三个边框

171
00:06:11,116 --> 0:06:12,616
加以标记

172
00:06:13,726 --> 0:06:15,826
边界框在

173
00:06:15,826 --> 0:06:18,546
相对于图像的标准化坐标空间中

174
00:06:18,546 --> 0:06:23,316
原始图像和左下角是原点

175
00:06:23,316 --> 0:06:25,066
非常类似于 Vision 中

176
00:06:25,066 --> 0:06:27,186
其他算法返回的边界框

177
00:06:27,956 --> 0:06:30,716
所以我写了一个小方法

178
00:06:30,716 --> 0:06:32,266
演示如何得到边界框

179
00:06:32,266 --> 0:06:33,146
并且使用它们

180
00:06:33,916 --> 0:06:35,346
这里我们有一个

181
00:06:35,346 --> 0:06:37,386
VNSaliencyImageObservation

182
00:06:37,826 --> 0:06:40,156
你只需进入

183
00:06:40,156 --> 0:06:41,706
访问其 salientObjects 属性

184
00:06:41,706 --> 0:06:43,876
之后就会得到

185
00:06:44,036 --> 0:06:46,456
一个边界框列表

186
00:06:46,456 --> 0:06:48,296
你可以像这样进行使用

187
00:06:48,296 --> 0:06:50,566
好了 现在你已经知道了

188
00:06:50,566 --> 0:06:53,136
如何去制定一个请求

189
00:06:53,136 --> 0:06:54,906
也明白了什么是显著性

190
00:06:55,686 --> 0:06:57,736
那接下来我们就一起来看一些用例

191
00:06:58,346 --> 0:07:02,606
首先为了有趣一点

192
00:06:58,346 --> 0:07:02,606
首先为了有趣一点

193
00:07:02,816 --> 0:07:05,816
你可以使用显著性作为一个图形蒙版

194
00:07:06,076 --> 0:07:07,056
来编辑你的照片

195
00:07:07,056 --> 0:07:09,496
这里你拥有热图

196
00:07:10,216 --> 0:07:14,306
在左侧

197
00:07:14,306 --> 0:07:15,886
我已经降低了非显著区域的饱和度

198
00:07:15,886 --> 0:07:17,716
而在右侧

199
00:07:17,716 --> 0:07:19,416
我为所有非显著区域

200
00:07:19,416 --> 0:07:20,636
添加了高斯模糊

201
00:07:21,006 --> 0:07:22,576
它真的让主体显眼了

202
00:07:25,536 --> 0:07:27,246
另一个显著性的例子是

203
00:07:27,246 --> 0:07:29,156
你可以增强你照片的观看体验

204
00:07:30,106 --> 0:07:32,036
就比如你现在在家里

205
00:07:32,036 --> 0:07:33,846
坐在沙发 

206
00:07:33,946 --> 0:07:35,236
哪怕你的电视或者电脑

207
00:07:35,236 --> 0:07:38,216
已经进入待机模式

208
00:07:38,216 --> 0:07:39,906
它也在进入你的照片库

209
00:07:40,336 --> 0:07:41,706
很多时候

210
00:07:41,706 --> 0:07:44,446
这些照片显示的算法

211
00:07:44,446 --> 0:07:45,306
可能看似有点笨拙

212
00:07:45,306 --> 0:07:47,006
它们放大图像中看似随机的部分

213
00:07:47,006 --> 0:07:51,346
而不是 总是你所期望的

214
00:07:52,086 --> 0:07:53,546
但是有了显著性的帮助 

215
00:07:53,546 --> 0:07:56,026
你总是能够知道主题在哪里

216
00:07:56,026 --> 0:07:57,036
所以 你可以得到

217
00:07:57,136 --> 0:08:02,576
像这样更像纪录片的效果

218
00:07:57,136 --> 0:08:02,576
像这样更像纪录片的效果

219
00:08:02,766 --> 0:08:04,616
最后

220
00:08:04,616 --> 0:08:06,206
显著性对其他视觉算法非常有用

221
00:08:07,486 --> 0:08:09,246
假设我们有一个图像

222
00:08:09,246 --> 0:08:11,236
我们想要在图像中

223
00:08:11,236 --> 0:08:11,806
对对象进行分类

224
00:08:12,966 --> 0:08:14,426
我们通过可以运行基于对象的显著性

225
00:08:14,426 --> 0:08:16,226
来获取图像中的对象

226
00:08:16,226 --> 0:08:19,196
将图像裁剪到

227
00:08:19,196 --> 0:08:22,456
基于对象的显著性返回的边界框

228
00:08:22,456 --> 0:08:25,006
并通过运行算法

229
00:08:25,366 --> 0:08:26,816
以及运行图像分类算法

230
00:08:26,886 --> 0:08:31,536
找出对象是什么

231
00:08:32,316 --> 0:08:33,996
因此 由于边界框

232
00:08:34,285 --> 0:08:35,576
你不仅能够知道

233
00:08:35,576 --> 0:08:36,885
它们在图像中的位置

234
00:08:36,885 --> 0:08:39,905
而且还可以通过

235
00:08:39,905 --> 0:08:42,035
挑选其中包含这些对象的边界框

236
00:08:45,286 --> 0:08:46,756
现在 你已经可以使用 Core ML 对事物进行分类

237
00:08:46,756 --> 0:08:48,996
但今年

238
00:08:48,996 --> 0:08:50,606
Vision 已经为提供了新的图像分类技术

239
00:08:50,606 --> 0:08:54,126
由 Rohan 来介绍

240
00:08:55,516 --> 0:09:02,736
[掌声]

241
00:08:55,516 --> 0:09:02,736
[掌声]

242
00:09:03,236 --> 0:09:04,696
&gt;&gt; 早上好

243
00:09:05,136 --> 0:09:06,806
我叫 Rohan Chandra

244
00:09:06,806 --> 0:09:08,476
是一名来自 Vision 团队的研究员

245
00:09:09,266 --> 0:09:10,846
今天 我将要跟大家谈论

246
00:09:10,846 --> 0:09:12,106
今年 Vision API 提供的

247
00:09:12,106 --> 0:09:13,476
一些新的

248
00:09:13,476 --> 0:09:15,606
图像分类请求

249
00:09:16,806 --> 0:09:18,856
现在作为一项任务 

250
00:09:18,856 --> 0:09:20,626
图像分类基本上是回答了这个问题

251
00:09:20,626 --> 0:09:23,916
我的图像中的对象是什么

252
00:09:25,066 --> 0:09:28,206
许多人已经对图像分类比较熟悉

253
00:09:28,626 --> 0:09:30,146
你可能已经根据自己的数据

254
00:09:30,146 --> 0:09:31,776
使用过 Create ML 或者 Core ML 

255
00:09:31,776 --> 0:09:33,306
来训练自己的分类网络

256
00:09:33,356 --> 0:09:35,366
正如我们去年

257
00:09:35,366 --> 0:09:37,046
在 Vision with Core ML 中展示的

258
00:09:38,156 --> 0:09:39,306
其他人

259
00:09:39,356 --> 0:09:40,456
可能对图像分类感兴趣

260
00:09:40,456 --> 0:09:41,636
但觉得自己

261
00:09:41,636 --> 0:09:45,116
缺乏开发自己网络的资源或专业知识

262
00:09:45,896 --> 0:09:48,716
在实践中 

263
00:09:48,716 --> 0:09:50,026
从零开始开发一个大规模的分类网络

264
00:09:50,026 --> 0:09:51,576
可能需要数百万张图像的标注

265
00:09:51,676 --> 0:09:55,246
数千小时的培训

266
00:09:55,246 --> 0:09:57,566
以及非常专业的开发领域知识

267
00:09:58,736 --> 0:10:00,606
我们这里 Apple 

268
00:09:58,736 --> 0:10:00,606
我们这里 Apple 

269
00:10:00,606 --> 0:10:02,296
已经完成了这个过程

270
00:10:02,336 --> 0:10:03,646
所以我们希望与你们

271
00:10:03,646 --> 0:10:06,706
分享我们的大规模终端分类网络

272
00:10:07,176 --> 0:10:08,206
以便你可以利用这项技术

273
00:10:08,206 --> 0:10:09,936
而无需投入

274
00:10:09,936 --> 0:10:11,396
大量时间或资源

275
00:10:11,396 --> 0:10:13,306
来自己开发

276
00:10:14,296 --> 0:10:17,346
我们还努力在 API 中添加工具

277
00:10:17,346 --> 0:10:19,216
以帮助你

278
00:10:19,216 --> 0:10:20,646
用有意义的方式

279
00:10:20,646 --> 0:10:21,806
对 App 的结果进行推测和理解

280
00:10:22,986 --> 0:10:24,116
现在我们在这里

281
00:10:24,116 --> 0:10:25,796
讨论的网络

282
00:10:25,846 --> 0:10:27,326
实际上是我们自己

283
00:10:27,326 --> 0:10:29,306
用来为照片搜索体验提供支持的网络

284
00:10:30,096 --> 0:10:31,136
这是我们专门开发的网络

285
00:10:31,136 --> 0:10:32,646
为了在设备上高效运行

286
00:10:32,646 --> 0:10:36,236
而不需要任何服务端处理

287
00:10:36,956 --> 0:10:38,446
我们还开发了它

288
00:10:38,446 --> 0:10:39,746
来识别超过一千种

289
00:10:39,746 --> 0:10:41,136
不同类别的对象

290
00:10:42,516 --> 0:10:44,396
现在值得注意的是

291
00:10:44,516 --> 0:10:45,776
这是一个多标签网络

292
00:10:45,776 --> 0:10:49,936
能够识别单个图像中的多个对象

293
00:10:49,936 --> 0:10:51,986
而不是更典型的单标签网络

294
00:10:51,986 --> 0:10:53,976
试图专注于识别图像中的

295
00:10:53,976 --> 0:10:58,226
单个大型中心对象

296
00:10:59,586 --> 0:11:01,076
现在

297
00:10:59,586 --> 0:11:01,076
现在

298
00:11:01,076 --> 0:11:03,226
当我们谈论这个新的分类 API 时

299
00:11:03,226 --> 0:11:04,186
我认为首先想到的问题之一

300
00:11:04,186 --> 0:11:07,426
是它可以实际识别的对象是什么

301
00:11:08,336 --> 0:11:09,916
那么 

302
00:11:09,916 --> 0:11:12,426
分类器可以预测的对象集称为分类系统

303
00:11:13,436 --> 0:11:15,106
分类系统具有层次结构

304
00:11:15,106 --> 0:11:17,936
在类之间具有方向关系

305
00:11:19,006 --> 0:11:22,226
这些关系基于共享的语义含义

306
00:11:22,916 --> 0:11:26,436
例如狗类可能会有像比格犬

307
00:11:26,616 --> 0:11:29,156
卷毛狗 哈士奇狗和其他子类

308
00:11:30,256 --> 0:11:31,866
从这个意义上来说

309
00:11:31,896 --> 0:11:33,226
父类倾向于更一般化

310
00:11:33,226 --> 0:11:36,106
而子类则是父类更具体的实例

311
00:11:37,046 --> 0:11:38,466
当然你也可以使用

312
00:11:38,466 --> 0:11:41,756
ImageRequest.known 分类查看整个分类

313
00:11:43,136 --> 0:11:44,416
现在

314
00:11:44,416 --> 0:11:46,276
在我们构建分类法时

315
00:11:46,276 --> 0:11:47,226
我们应用了一些特定的规则

316
00:11:48,656 --> 0:11:49,816
首先这些分类

317
00:11:49,856 --> 0:11:51,516
必须在视觉上可识别

318
00:11:52,836 --> 0:11:54,456
也就是说 我们要避免更多抽象概念

319
00:11:54,456 --> 0:11:56,306
如假日或节日

320
00:11:57,426 --> 0:11:59,196
我们也避免任何

321
00:11:59,196 --> 0:11:59,876
可能被认为

322
00:11:59,876 --> 0:12:01,746
具有争议性或冒犯性的类别

323
00:11:59,876 --> 0:12:01,746
具有争议性或冒犯性的类别

324
00:12:01,746 --> 0:12:03,066
以及与名字 名词

325
00:12:03,066 --> 0:12:06,256
形容词或基本形状有关的类别

326
00:12:07,346 --> 0:12:09,266
最后我们省略了职业

327
00:12:09,526 --> 0:12:11,076
这在一开始可能看起来很奇怪

328
00:12:11,716 --> 0:12:12,856
但是如果

329
00:12:12,856 --> 0:12:14,266
我们询问工程师的样子

330
00:12:14,266 --> 0:12:16,216
请考虑你会得到什么样的答案

331
00:12:16,556 --> 0:12:18,496
除了睡眠不足

332
00:12:18,496 --> 0:12:19,546
和通常粘在电脑屏幕上之外

333
00:12:19,546 --> 0:12:20,856
可能没有一个更简单的描述

334
00:12:20,856 --> 0:12:24,476
可以适用于每一位工程师

335
00:12:25,556 --> 0:12:26,646
让我们看一下你需要使用的代码

336
00:12:26,646 --> 0:12:28,706
以便对图像进行分类

337
00:12:31,636 --> 0:12:33,526
为原图像添加 ImageRequestHandler

338
00:12:34,196 --> 0:12:35,026
下一步

339
00:12:35,026 --> 0:12:36,836
执行 VNClassifyImageRequest

340
00:12:36,836 --> 0:12:38,066
并检索你的观察结果

341
00:12:38,746 --> 0:12:40,036
现在 在本例中

342
00:12:40,036 --> 0:12:41,406
你实际上得到了一组观察结果

343
00:12:41,696 --> 0:12:42,976
分类系统中的每个类

344
00:12:42,976 --> 0:12:45,456
都有一个观察结果及其相关的置信度

345
00:12:46,366 --> 0:12:47,876
在单标签问题中

346
00:12:47,876 --> 0:12:48,946
你可能会期望

347
00:12:48,946 --> 0:12:50,906
这些概率总和为 1

348
00:12:50,906 --> 0:12:51,896
但这是一个多标签分类网络

349
00:12:51,936 --> 0:12:53,876
每个预测都是

350
00:12:53,876 --> 0:12:55,376
与特定类相关的

351
00:12:55,376 --> 0:12:57,416
独立置信度

352
00:12:58,416 --> 0:13:00,096
因此它们不会总和为 1

353
00:12:58,416 --> 0:13:00,096
因此它们不会总和为 1

354
00:13:00,406 --> 0:13:01,466
而是要在一个类中

355
00:13:01,466 --> 0:13:02,956
进行比较

356
00:13:02,956 --> 0:13:04,256
而不是跨越不同的类

357
00:13:04,496 --> 0:13:06,196
所以我们不能简单地

358
00:13:06,196 --> 0:13:08,816
用最大值来确定我们的最终预测

359
00:13:09,696 --> 0:13:11,166
你可能想知道

360
00:13:11,166 --> 0:13:12,806
我如何处理这么多类别

361
00:13:12,856 --> 0:13:13,756
这么多的数字

362
00:13:14,516 --> 0:13:15,836
我们在 API 中

363
00:13:15,836 --> 0:13:17,066
实现了一些关键工具

364
00:13:17,066 --> 0:13:19,086
来帮助你理解结果

365
00:13:20,376 --> 0:13:22,026
现在为了在 API 中

366
00:13:22,086 --> 0:13:23,916
讨论这些工具

367
00:13:23,916 --> 0:13:25,586
我们首先需要定义一些基本术语

368
00:13:26,266 --> 0:13:28,716
第一个是

369
00:13:28,716 --> 0:13:30,576
当你对一个类有信心时

370
00:13:30,576 --> 0:13:31,936
我们通常会将它

371
00:13:31,936 --> 0:13:33,606
与特定于类的阈值进行比较

372
00:13:33,606 --> 0:13:35,186
我们将其称为操作点

373
00:13:35,976 --> 0:13:37,976
如果类的置信度高于阈值

374
00:13:37,976 --> 0:13:39,466
那么我们就说

375
00:13:39,466 --> 0:13:40,796
类存在于图像中

376
00:13:41,266 --> 0:13:42,996
如果类置信度低于类阈值

377
00:13:42,996 --> 0:13:44,516
那么我们就说

378
00:13:44,516 --> 0:13:46,756
对象不在图像中

379
00:13:47,656 --> 0:13:49,316
从这个意义上说 

380
00:13:49,316 --> 0:13:50,866
我们想要选取阈值

381
00:13:50,866 --> 0:13:52,346
使得具有目标类的对象

382
00:13:52,346 --> 0:13:53,456
通常具有高于阈值的置信度

383
00:13:53,456 --> 0:13:55,646
而没有目标类的图像

384
00:13:55,736 --> 0:13:59,406
通常具有低于阈值的分数

385
00:14:00,376 --> 0:14:02,216
然而

386
00:14:02,216 --> 0:14:03,866
机器学习并非一帆风顺

387
00:14:03,866 --> 0:14:05,636
而且在某些情况下

388
00:14:05,636 --> 0:14:07,136
网络不确定

389
00:14:07,136 --> 0:14:08,276
置信度也相应降低

390
00:14:09,056 --> 0:14:10,496
例如

391
00:14:10,536 --> 0:14:12,156
当对象所处光线奇怪

392
00:14:12,156 --> 0:14:14,006
或者出现角度奇怪时

393
00:14:14,006 --> 0:14:14,506
就可能发生这种情况

394
00:14:15,176 --> 0:14:16,426
那么 我们如何选择阈值呢

395
00:14:17,596 --> 0:14:18,306
根据我们对阈值的选择

396
00:14:18,436 --> 0:14:19,756
产生三种不同类型的搜索

397
00:14:19,756 --> 0:14:20,926
我们可以有三种不同的制度

398
00:14:20,926 --> 0:14:23,326
和方法

399
00:14:24,416 --> 0:14:25,426
为了使这

400
00:14:25,426 --> 0:14:26,816
更具体一点

401
00:14:26,876 --> 0:14:28,696
假设我有一个图像库

402
00:14:28,806 --> 0:14:30,346
我已经对其进行了分类

403
00:14:30,386 --> 0:14:31,246
并存储了结果

404
00:14:32,026 --> 0:14:33,296
假设在这个特定的案例中

405
00:14:33,296 --> 0:14:35,866
我正在寻找摩托车的图像

406
00:14:36,726 --> 0:14:37,626
现在我要选择阈值

407
00:14:37,626 --> 0:14:39,466
这样摩托车图像的置信度

408
00:14:39,466 --> 0:14:40,776
通常高于此阈值

409
00:14:40,776 --> 0:14:41,826
而没有摩托车图像的

410
00:14:41,876 --> 0:14:46,946
置信度通常低于此阈值

411
00:14:47,596 --> 0:14:49,186
那么如果我选择一个低阈值

412
00:14:49,186 --> 0:14:50,006
会发生什么

413
00:14:50,826 --> 0:14:52,286
正如你在我身后看到的

414
00:14:52,286 --> 0:14:54,246
当我应用这个低阈值时

415
00:14:54,246 --> 0:14:55,356
我确实得到了

416
00:14:55,356 --> 0:14:56,926
我的摩托车图像

417
00:14:56,926 --> 0:14:58,696
但我也得到了右下角的

418
00:14:58,696 --> 0:14:59,246
这些轻型摩托的图像

419
00:14:59,246 --> 0:15:01,106
如果我的用户

420
00:14:59,246 --> 0:15:01,106
如果我的用户

421
00:15:01,106 --> 0:15:02,106
是摩托车爱好者

422
00:15:02,106 --> 0:15:03,386
他们可能会对这个结果感到有点恼火

423
00:15:04,516 --> 0:15:06,186
当我们谈论

424
00:15:06,186 --> 0:15:07,806
试图最大化

425
00:15:07,806 --> 0:15:09,346
在整个库检索的

426
00:15:09,346 --> 0:15:11,156
目标类别的百分比的搜索时

427
00:15:11,246 --> 0:15:12,656
并不关心

428
00:15:12,656 --> 0:15:14,196
这些错过的预测

429
00:15:14,196 --> 0:15:15,596
我们就说摩托车存在时

430
00:15:15,596 --> 0:15:16,266
实际上不存在

431
00:15:16,656 --> 0:15:18,036
我们通常谈论的

432
00:15:18,036 --> 0:15:19,076
是高召回搜索

433
00:15:20,156 --> 0:15:22,236
现在我可以通过

434
00:15:22,236 --> 0:15:23,806
简单地返回尽可能多的图像

435
00:15:23,806 --> 0:15:25,086
来最大程度地召回

436
00:15:25,176 --> 0:15:26,526
但是我会得到

437
00:15:26,526 --> 0:15:27,516
大量的这些错误预测

438
00:15:27,516 --> 0:15:28,806
我是说我的目标类

439
00:15:28,806 --> 0:15:30,436
实际不存在但显示存在

440
00:15:30,436 --> 0:15:31,666
所以我们需要

441
00:15:31,666 --> 0:15:32,676
找到一个更平衡的阈值来召回

442
00:15:33,586 --> 0:15:34,956
让我们来看看

443
00:15:34,956 --> 0:15:36,596
如何更改代码

444
00:15:36,596 --> 0:15:38,256
以执行此高召回搜索

445
00:15:39,716 --> 0:15:41,566
这里我有和以前

446
00:15:41,566 --> 0:15:43,466
相同的代码片段

447
00:15:43,606 --> 0:15:45,516
但这次我执行了

448
00:15:45,516 --> 0:15:47,206
hasMinimumPrecision

449
00:15:47,206 --> 0:15:48,666
和特定召回值的筛选

450
00:15:49,496 --> 0:15:51,526
对于我的观察数组中的

451
00:15:51,526 --> 0:15:53,646
每个观察

452
00:15:53,646 --> 0:15:55,166
只有当与类相关的置信度

453
00:15:55,166 --> 0:15:56,336
达到我指定的召回级别时

454
00:15:56,656 --> 0:15:58,896
它才能够被保留下来

455
00:15:59,646 --> 0:16:01,526
现在确定这一点

456
00:15:59,646 --> 0:16:01,526
现在确定这一点

457
00:16:01,526 --> 0:16:02,856
所需的实际操作点

458
00:16:02,856 --> 0:16:04,196
对于每个类都是不同的

459
00:16:04,196 --> 0:16:06,106
这是我们根据

460
00:16:06,106 --> 0:16:07,756
对分类系统中每个类的

461
00:16:07,756 --> 0:16:08,986
网络分类情况

462
00:16:08,986 --> 0:16:11,116
确定的

463
00:16:12,086 --> 0:16:13,736
不过过滤系统

464
00:16:13,736 --> 0:16:14,696
会自动为你处理

465
00:16:15,026 --> 0:16:16,756
你所要做的就只是

466
00:16:16,756 --> 0:16:18,696
指定你想操作的召回级别

467
00:16:19,736 --> 0:16:20,686
所以我们在这里讨论的

468
00:16:20,686 --> 0:16:22,426
是一种高召回搜索

469
00:16:22,426 --> 0:16:24,196
但是如果我有一个

470
00:16:24,196 --> 0:16:25,646
不能容忍这些错误预测的 App

471
00:16:25,646 --> 0:16:26,786
比如我说摩托车

472
00:16:26,786 --> 0:16:27,696
存在但实际上不存在

473
00:16:28,106 --> 0:16:29,746
也就是说我要绝对确定

474
00:16:29,746 --> 0:16:31,456
我检索的图像

475
00:16:31,626 --> 0:16:33,246
实际上确实包含一辆摩托车

476
00:16:34,026 --> 0:16:35,326
那么让我们回到

477
00:16:35,326 --> 0:16:36,956
我们的图像库

478
00:16:36,956 --> 0:16:38,236
看看如果应用更高的阈值

479
00:16:38,266 --> 0:16:39,186
会发生什么

480
00:16:39,956 --> 0:16:41,406
正如你在我身后看到的

481
00:16:41,406 --> 0:16:43,186
当我应用我的高阈值时

482
00:16:43,186 --> 0:16:44,776
实际上我只得到摩托车图像

483
00:16:45,106 --> 0:16:46,506
但总体上

484
00:16:46,506 --> 0:16:47,166
我得到的图像要少得多

485
00:16:48,536 --> 0:16:50,176
当我们谈论一种搜索

486
00:16:50,176 --> 0:16:51,816
试图最大限度地

487
00:16:51,876 --> 0:16:53,506
提高检索图像中

488
00:16:53,506 --> 0:16:55,586
目标类的百分比

489
00:16:55,586 --> 0:16:57,146
忽略一些

490
00:16:57,146 --> 0:16:58,486
更模糊的图像

491
00:16:58,536 --> 0:16:59,696
这些图像可能包含目标类

492
00:16:59,696 --> 0:17:01,216
那么我们说

493
00:16:59,696 --> 0:17:01,216
那么我们说

494
00:17:01,216 --> 0:17:03,736
这通常谈论的是高准确度搜索

495
00:17:04,445 --> 0:17:06,296
又一次就像高召回一样

496
00:17:06,296 --> 0:17:07,316
我们需要找到一个

497
00:17:07,316 --> 0:17:08,656
更平衡的操作点

498
00:17:08,656 --> 0:17:10,435
在这个点上我可以接受

499
00:17:10,435 --> 0:17:11,486
目标类出现在我的结果中的可能性

500
00:17:11,486 --> 0:17:12,886
但我得到的图像

501
00:17:12,986 --> 0:17:13,756
并不会太少

502
00:17:14,816 --> 0:17:16,116
现在让我们来看一下

503
00:17:16,116 --> 0:17:17,816
如何通过修改代码

504
00:17:17,816 --> 0:17:19,586
来执行这个高精度的搜索

505
00:17:20,175 --> 0:17:22,546
这里有相同的代码片段

506
00:17:22,715 --> 0:17:24,215
但这次我的过滤筛选

507
00:17:24,215 --> 0:17:26,366
是用 hasMinimumRecall

508
00:17:26,366 --> 0:17:27,935
和我指定的准确度值完成的

509
00:17:28,806 --> 0:17:30,636
同样只有当与之相关的

510
00:17:30,636 --> 0:17:32,186
置信度达到

511
00:17:32,186 --> 0:17:33,786
我指定的准确度水平时

512
00:17:33,786 --> 0:17:35,406
我才保留观察结果

513
00:17:36,166 --> 0:17:37,546
对于每一个类

514
00:17:37,546 --> 0:17:38,806
实际需要的阈值

515
00:17:38,806 --> 0:17:40,286
都是不同的

516
00:17:40,286 --> 0:17:41,646
但是过滤系统会自动

517
00:17:41,646 --> 0:17:42,256
为我处理这个问题

518
00:17:42,626 --> 0:17:44,056
我需要做的就是

519
00:17:44,056 --> 0:17:45,616
告诉它我想要的操作精度

520
00:17:46,936 --> 0:17:48,576
所以我们在这里

521
00:17:48,576 --> 0:17:49,846
谈到了两个不同的极端

522
00:17:49,906 --> 0:17:51,236
一个是高召回

523
00:17:51,236 --> 0:17:53,516
另一个是高准确度

524
00:17:53,516 --> 0:17:55,216
但在实践中

525
00:17:55,416 --> 0:17:57,076
最好找到两者之间的平衡点

526
00:17:58,096 --> 0:17:59,496
那么我们现在就来看看如何做到这一点

527
00:17:59,496 --> 0:18:01,966
了解发生了什么

528
00:17:59,496 --> 0:18:01,966
了解发生了什么

529
00:18:02,246 --> 0:18:03,266
我首先需要介绍一下

530
00:18:03,396 --> 0:18:04,836
什么是准确度

531
00:18:04,836 --> 0:18:05,646
和召回曲线

532
00:18:06,346 --> 0:18:08,976
在实践中

533
00:18:08,976 --> 0:18:10,406
需要做出权衡

534
00:18:10,406 --> 0:18:11,986
增加一单位的准确度或召回率

535
00:18:11,986 --> 0:18:14,196
会导致另一方面的减少

536
00:18:14,646 --> 0:18:16,376
我可以将这种权衡

537
00:18:16,376 --> 0:18:17,956
用图形表示

538
00:18:18,026 --> 0:18:19,586
对于每个操作点

539
00:18:19,586 --> 0:18:21,336
我可以计算相应的准确度和召回率

540
00:18:22,016 --> 0:18:23,476
例如

541
00:18:23,506 --> 0:18:25,156
当我实现召回 0.7 的操作点

542
00:18:25,156 --> 0:18:27,116
我发现我得到了

543
00:18:27,116 --> 0:18:29,436
相应的 0.74 准确度

544
00:18:29,436 --> 0:18:31,886
我可以通过

545
00:18:31,886 --> 0:18:33,486
计算多个操作点

546
00:18:33,486 --> 0:18:35,016
来形成我的完整曲线

547
00:18:36,246 --> 0:18:37,956
正如我之前所说

548
00:18:37,956 --> 0:18:39,846
我希望在这条曲线上

549
00:18:40,056 --> 0:18:41,056
找到一个平衡点

550
00:18:41,056 --> 0:18:42,286
它可以达到对我的 App

551
00:18:42,286 --> 0:18:43,536
有意义的召回和准确度

552
00:18:44,486 --> 0:18:45,726
下面让我们看看

553
00:18:45,756 --> 0:18:47,366
如何更改代码来实现它

554
00:18:47,366 --> 0:18:48,636
以及准确度和召回曲线

555
00:18:48,636 --> 0:18:52,146
如何发挥作用

556
00:18:52,286 --> 0:18:54,276
我在这里使用

557
00:18:54,276 --> 0:18:55,896
hasMinimumPrecision 进行过滤

558
00:18:55,896 --> 0:18:57,546
在这里我指定了最小准确度

559
00:18:57,546 --> 0:18:57,976
和召回值

560
00:18:58,416 --> 0:19:00,746
当我指定

561
00:18:58,416 --> 0:19:00,746
当我指定

562
00:19:00,746 --> 0:19:02,486
MinimumPrecision 时

563
00:19:02,486 --> 0:19:04,286
我实际上是在图表中

564
00:19:04,286 --> 0:19:05,976
选择我想要操作的区域

565
00:19:06,866 --> 0:19:08,236
当我用 forRecall

566
00:19:08,286 --> 0:19:10,226
选择一个召回点时

567
00:19:10,296 --> 0:19:11,896
我会沿着曲线选择一个点

568
00:19:11,896 --> 0:19:13,006
作为我的操作点

569
00:19:13,876 --> 0:19:15,516
现在如果操作点

570
00:19:15,576 --> 0:19:16,666
在我选择的有效区域中

571
00:19:16,666 --> 0:19:18,166
那么这就是过滤系统

572
00:19:18,166 --> 0:19:19,616
在查看特定类时

573
00:19:19,616 --> 0:19:21,626
将应用的阈值

574
00:19:22,586 --> 0:19:24,076
如果操作点

575
00:19:24,076 --> 0:19:25,636
不在有效区域中

576
00:19:25,636 --> 0:19:27,046
则没有满足

577
00:19:27,046 --> 0:19:28,536
我所述约束的操作点

578
00:19:28,536 --> 0:19:29,606
并且该类将始终

579
00:19:29,606 --> 0:19:30,816
从我的结果中过滤掉

580
00:19:31,856 --> 0:19:33,556
从这个意义上讲

581
00:19:33,556 --> 0:19:35,166
你需要做的就是

582
00:19:35,166 --> 0:19:36,476
提供你想要操作的

583
00:19:36,476 --> 0:19:37,686
准确度和召回率

584
00:19:37,686 --> 0:19:38,626
过滤系统将

585
00:19:38,626 --> 0:19:40,796
自动为你确定必要的阈值

586
00:19:41,386 --> 0:19:44,906
总而言之

587
00:19:44,906 --> 0:19:46,226
我在执行图像分类时

588
00:19:46,226 --> 0:19:47,656
得到的观察

589
00:19:47,986 --> 0:19:49,106
实际上是一个观察数组

590
00:19:49,106 --> 0:19:50,656
分类中的每个类

591
00:19:50,656 --> 0:19:51,736
都有一个值

592
00:19:52,876 --> 0:19:54,106
因为这是一个多标签的问题

593
00:19:54,206 --> 0:19:56,456
所以置信度之和不等于1

594
00:19:57,166 --> 0:19:58,716
相反，我们有独立的

595
00:19:58,716 --> 0:20:00,376
置信值

596
00:19:58,716 --> 0:20:00,376
置信值

597
00:20:00,376 --> 0:20:02,616
在0到1之间每个类都有一个

598
00:20:02,616 --> 0:20:04,096
我们需要了解准确度和召回

599
00:20:04,096 --> 0:20:05,656
以及它们如何

600
00:20:05,656 --> 0:20:07,676
应用于我们的特定用例

601
00:20:07,676 --> 0:20:08,606
以便应用

602
00:20:08,606 --> 0:20:10,226
hasMinimumPrecision 

603
00:20:10,226 --> 0:20:11,856
或 hasMinimumRecall 进行过滤筛选

604
00:20:11,856 --> 0:20:13,166
这对我们的 App 有意义

605
00:20:13,606 --> 0:20:15,846
所以这就总结了

606
00:20:16,146 --> 0:20:17,496
图像分类部分

607
00:20:17,986 --> 0:20:19,416
我想换个时间

608
00:20:19,416 --> 0:20:20,676
再探讨相关的话题

609
00:20:21,226 --> 0:20:23,976
图像相似性

610
00:20:26,216 --> 0:20:27,216
当我们谈论

611
00:20:27,216 --> 0:20:29,076
图像相似性时

612
00:20:29,076 --> 0:20:30,406
我们真正的意思是

613
00:20:30,406 --> 0:20:32,476
一种描述图像内容的方法

614
00:20:32,476 --> 0:20:34,956
和比较这些描述的另一种方法

615
00:20:36,116 --> 0:20:38,106
描述图像内容

616
00:20:38,106 --> 0:20:39,676
最基本方法是

617
00:20:39,676 --> 0:20:41,986
使用源像素本身

618
00:20:43,536 --> 0:20:45,286
也就是说 我可以搜索

619
00:20:45,286 --> 0:20:47,076
其他具有接近

620
00:20:47,076 --> 0:20:49,196
或完全相同像素值的图像

621
00:20:49,256 --> 0:20:50,076
并检索它们

622
00:20:51,146 --> 0:20:52,136
然而如果我以这种方式进行搜索

623
00:20:52,136 --> 0:20:54,046
它会非常脆弱

624
00:20:54,046 --> 0:20:55,886
很容易被旋转

625
00:20:55,986 --> 0:20:57,646
或光照增强等

626
00:20:57,916 --> 0:20:59,466
微小变化所影响

627
00:20:59,466 --> 0:21:00,656
这些变化会大幅改变像素值

628
00:20:59,466 --> 0:21:00,656
这些变化会大幅改变像素值

629
00:21:00,656 --> 0:21:02,256
但不会改变图像中的

630
00:21:02,256 --> 0:21:03,376
语义内容

631
00:21:04,266 --> 0:21:05,536
我真正想要的是

632
00:21:05,536 --> 0:21:07,246
对图像内容的

633
00:21:07,246 --> 0:21:08,836
更高级描述

634
00:21:08,836 --> 0:21:10,426
可能有点儿像自然语言

635
00:21:10,936 --> 0:21:13,126
我可以使用我之前描述的

636
00:21:13,126 --> 0:21:14,696
图像分类 API

637
00:21:14,696 --> 0:21:16,426
以便提取

638
00:21:16,426 --> 0:21:18,626
描述我的图像的一组词

639
00:21:19,446 --> 0:21:20,936
然后我可以检索

640
00:21:20,936 --> 0:21:23,316
具有类似分类的其他图像

641
00:21:23,756 --> 0:21:25,106
我甚至可以

642
00:21:25,106 --> 0:21:26,256
将它与词向量结合起来

643
00:21:26,326 --> 0:21:27,716
来解释

644
00:21:27,716 --> 0:21:29,386
类似但不完全匹配的单词

645
00:21:29,386 --> 0:21:30,356
如 cat 和 kitten

646
00:21:30,696 --> 0:21:31,896
如果我执行这样的搜索

647
00:21:31,896 --> 0:21:33,556
我可能会在一般意义上

648
00:21:33,556 --> 0:21:35,046
得到类似的对象

649
00:21:35,446 --> 0:21:36,406
但这些对象的

650
00:21:36,406 --> 0:21:37,556
出现方式以及

651
00:21:37,556 --> 0:21:39,066
它们之间的关系

652
00:21:39,066 --> 0:21:40,026
可能会有很大差异

653
00:21:40,966 --> 0:21:43,036
同样我也会受到

654
00:21:43,036 --> 0:21:44,586
分类方法的限制

655
00:21:45,386 --> 0:21:46,736
也就是说

656
00:21:46,736 --> 0:21:48,426
在我的图像中

657
00:21:48,426 --> 0:21:49,756
出现的任何不在

658
00:21:49,756 --> 0:21:51,686
分类网络分类系统中的对象

659
00:21:51,786 --> 0:21:52,876
都不能在这样的搜索中表示出来

660
00:21:54,216 --> 0:21:55,486
我真正想要的是

661
00:21:55,486 --> 0:21:56,966
对图像中出现的对象的

662
00:21:56,996 --> 0:21:58,176
高级描述

663
00:21:58,386 --> 0:21:59,856
这些对象没有

664
00:21:59,856 --> 0:22:01,096
固定在精确的像素值上

665
00:21:59,856 --> 0:22:01,096
固定在精确的像素值上

666
00:22:01,096 --> 0:22:01,556
但仍然受到注意

667
00:22:02,256 --> 0:22:04,006
我也希望这适用于

668
00:22:04,006 --> 0:22:05,896
任何自然图像

669
00:22:05,896 --> 0:22:07,266
而不仅仅是在特定的分类中

670
00:22:07,746 --> 0:22:09,946
事实证明

671
00:22:09,946 --> 0:22:11,756
这种表征学习

672
00:22:11,756 --> 0:22:12,676
是我们分类网络

673
00:22:12,676 --> 0:22:14,016
作为其训练过程的一部分

674
00:22:14,016 --> 0:22:15,786
自然产生的

675
00:22:16,786 --> 0:22:18,736
网络的上层

676
00:22:18,976 --> 0:22:20,356
包含执行分类所需的所有显著信息

677
00:22:20,356 --> 0:22:22,166
同时丢弃

678
00:22:22,166 --> 0:22:23,686
对该任务没有帮助的

679
00:22:23,756 --> 0:22:25,466
任何冗余

680
00:22:25,466 --> 0:22:28,336
或不必要信息

681
00:22:28,386 --> 0:22:29,596
我们可以利用

682
00:22:29,596 --> 0:22:31,136
这些上层作为特征描述符

683
00:22:31,136 --> 0:22:32,456
就是我们所说的

684
00:22:32,456 --> 0:22:34,156
FeaturePrint

685
00:22:35,196 --> 0:22:36,706
FeaturePrint 是一个向量

686
00:22:36,706 --> 0:22:37,946
这些内容

687
00:22:37,946 --> 0:22:39,666
不受特定分类系统的约束

688
00:22:39,666 --> 0:22:40,696
甚至不受分类网络

689
00:22:40,696 --> 0:22:43,636
所训练的分类系统限制

690
00:22:43,896 --> 0:22:45,106
它只是

691
00:22:45,106 --> 0:22:46,486
在训练过程中

692
00:22:46,486 --> 0:22:47,736
利用网络对图像的了解

693
00:22:48,816 --> 0:22:50,296
如果我们查看这对图像

694
00:22:50,296 --> 0:22:51,696
我们可以比较

695
00:22:51,696 --> 0:22:52,786
它们 FeaturePrint 的相似程度

696
00:22:52,786 --> 0:22:54,236
并且值越小

697
00:22:54,236 --> 0:22:55,486
两个图像

698
00:22:55,486 --> 0:22:57,296
在语义上越相似

699
00:22:58,046 --> 0:22:59,316
我们可以看到

700
00:22:59,316 --> 0:23:00,556
尽管这两张猫的图像

701
00:22:59,316 --> 0:23:00,556
尽管这两张猫的图像

702
00:23:00,556 --> 0:23:02,026
在视觉上是不同的

703
00:23:02,026 --> 0:23:03,506
但它们具有

704
00:23:03,746 --> 0:23:05,526
比视觉上相似的不同动物

705
00:23:05,596 --> 0:23:06,636
更相似的 FeaturePrint

706
00:23:07,136 --> 0:23:09,416
为了使这个更具体一点

707
00:23:09,416 --> 0:23:10,646
下面我们来看

708
00:23:10,646 --> 0:23:11,686
一个具体的例子

709
00:23:12,396 --> 0:23:13,476
假设我在屏幕上有

710
00:23:13,476 --> 0:23:15,186
源图像

711
00:23:15,186 --> 0:23:16,706
我想找到其他

712
00:23:16,706 --> 0:23:17,476
语义相似的的图像

713
00:23:18,336 --> 0:23:19,656
我将选取一个图像库

714
00:23:19,656 --> 0:23:21,196
计算每个图像的

715
00:23:21,196 --> 0:23:22,896
FeaturePrint

716
00:23:22,896 --> 0:23:24,476
然后检索那些

717
00:23:24,476 --> 0:23:26,296
与源图像具有

718
00:23:26,296 --> 0:23:26,986
最相似 FeaturePrint 的图像

719
00:23:27,746 --> 0:23:29,136
当我用咖啡厅里

720
00:23:29,136 --> 0:23:29,926
一位绅士的图像做这件事时

721
00:23:29,926 --> 0:23:31,876
我发现我也同时得到了

722
00:23:31,926 --> 0:23:33,176
咖啡厅和餐厅里

723
00:23:33,176 --> 0:23:33,946
其他人的图像

724
00:23:34,886 --> 0:23:36,266
如果我把注意力

725
00:23:36,266 --> 0:23:38,246
集中在一份报纸上

726
00:23:38,246 --> 0:23:39,586
我就会看到其他报纸的图片

727
00:23:40,246 --> 0:23:42,156
如果我把焦点放在茶壶上

728
00:23:42,236 --> 0:23:43,906
我会看到其他的茶壶图像

729
00:23:45,326 --> 0:23:46,616
现在我想邀请 

730
00:23:46,616 --> 0:23:48,616
Vision 团队在舞台上

731
00:23:48,616 --> 0:23:50,116
帮我做个演示

732
00:23:50,116 --> 0:23:51,546
演示下如何进一步扩展

733
00:23:51,546 --> 0:23:52,646
图像相似性

734
00:23:54,516 --> 0:23:58,166
[掌声]

735
00:23:58,666 --> 0:23:59,466
&gt;&gt; 大家好

736
00:23:59,886 --> 0:24:00,846
我叫 Brett 

737
00:23:59,886 --> 0:24:00,846
我叫 Brett 

738
00:24:00,846 --> 0:24:02,046
今天我们将用一种非常有趣的方式

739
00:24:02,046 --> 0:24:03,546
来展示图像相似性

740
00:24:03,546 --> 0:24:05,286
我们创造性地称之为

741
00:24:05,286 --> 0:24:06,846
图像相似性游戏

742
00:24:07,876 --> 0:24:09,086
这里是玩法说明

743
00:24:09,676 --> 0:24:11,066
你在一张纸上画些东西

744
00:24:11,066 --> 0:24:14,026
然后请几个朋友

745
00:24:14,026 --> 0:24:15,166
尽可能的重新创作出

746
00:24:15,166 --> 0:24:15,726
你的原作

747
00:24:16,296 --> 0:24:17,976
所以我将从画原图开始

748
00:24:30,096 --> 0:24:33,496
好 接下来点击 Continue

749
00:24:33,496 --> 0:24:33,976
将其作为原始文件进行扫描

750
00:24:42,046 --> 0:24:42,976
然后保存

751
00:24:44,526 --> 0:24:46,456
现在我的团队

752
00:24:46,456 --> 0:24:47,886
将扮演参赛者的角色 

753
00:24:48,156 --> 0:24:48,976
他们将尽可能地把这个画得最好

754
00:24:54,266 --> 0:24:55,206
当他们正在绘图时

755
00:24:55,206 --> 0:24:57,316
我要告诉你

756
00:24:57,316 --> 0:24:58,476
这个示例 App 

757
00:24:58,536 --> 0:25:00,576
现在作为示例代码

758
00:24:58,536 --> 0:25:00,576
现在作为示例代码

759
00:25:00,576 --> 0:25:04,356
可在开发者文档网站上提供给你

760
00:25:04,356 --> 0:25:05,986
而且我们正在使用

761
00:25:05,986 --> 0:25:07,586
VisionKit 文档扫描仪

762
00:25:07,586 --> 0:25:08,886
扫描我们的绘图

763
00:25:08,886 --> 0:25:09,966
你可以在我们的

764
00:25:09,966 --> 0:25:11,226
文字识别会议中了解更多

765
00:25:12,706 --> 0:25:15,076
再给他们

766
00:25:16,636 --> 0:25:16,886
几秒钟的时间

767
00:25:16,966 --> 0:25:19,896
五 四 三 好

768
00:25:19,926 --> 0:25:20,186
我想他们都已经完成了

769
00:25:20,266 --> 0:25:22,186
好的现在我们把它们拿出来

770
00:25:22,186 --> 0:25:24,266
开始扫描

771
00:25:24,996 --> 0:25:26,256
第一位参赛者作品

772
00:25:31,046 --> 0:25:31,576
非常不错 [掌声]

773
00:25:32,756 --> 0:25:33,406
这有可能会是冠军

774
00:25:33,406 --> 0:25:35,256
再来看看第二位选手的

775
00:25:38,296 --> 0:25:39,786
也很不错

776
00:25:40,096 --> 0:25:40,506
很棒

777
00:25:41,256 --> 0:25:43,256
[掌声]

778
00:25:43,496 --> 0:25:44,706
接下来三号选手

779
00:25:48,056 --> 0:25:50,056
[笑声和掌声]

780
00:25:50,096 --> 0:25:50,976
我觉得也很好

781
00:25:51,016 --> 0:25:52,586
[掌声]

782
00:25:52,586 --> 0:25:53,696
第四位

783
00:25:57,046 --> 0:25:58,136
这个我就不知道了

784
00:25:58,196 --> 0:26:00,836
我们要看会出现什么情况

785
00:25:58,196 --> 0:26:00,836
我们要看会出现什么情况

786
00:26:01,151 --> 0:26:03,151
[掌声]

787
00:26:03,286 --> 0:26:03,836
好了

788
00:26:03,836 --> 0:26:07,006
我们把这些保存下来

789
00:26:07,006 --> 0:26:08,286
其实可以发现

790
00:26:08,286 --> 0:26:09,636
获胜者是一号参赛者

791
00:26:09,636 --> 0:26:10,446
恭喜

792
00:26:11,016 --> 0:26:12,736
[掌声]

793
00:26:12,736 --> 0:26:14,716
现在我划过去

794
00:26:14,796 --> 0:26:16,076
我们可以看到

795
00:26:16,076 --> 0:26:17,656
这些面孔在语义上更相似

796
00:26:18,336 --> 0:26:19,996
它们更接近原始绘图

797
00:26:20,096 --> 0:26:21,376
而树在语义上是不同的

798
00:26:21,376 --> 0:26:22,416
这与原来的图像

799
00:26:22,416 --> 0:26:22,686
有一些差别

800
00:26:23,336 --> 0:26:24,426
这就是图像相似度游戏

801
00:26:24,426 --> 0:26:25,376
下面让我们把舞台

802
00:26:25,376 --> 0:26:25,726
交给我的伙伴 Rohan

803
00:26:26,516 --> 0:26:31,776
[掌声]

804
00:26:32,276 --> 0:26:32,876
&gt;&gt; 谢谢大家

805
00:26:33,476 --> 0:26:34,736
我想快速浏览

806
00:26:34,736 --> 0:26:35,846
演示 App 中的一个片段

807
00:26:35,846 --> 0:26:37,196
向大家展示

808
00:26:37,196 --> 0:26:38,546
我们如何确定获胜的选手

809
00:26:39,686 --> 0:26:41,906
这里我有一部分代码

810
00:26:41,906 --> 0:26:43,506
它比较了每个参赛者绘画的

811
00:26:43,506 --> 0:26:45,286
FeaturePrint

812
00:26:45,396 --> 0:26:47,666
和 Brett 绘画的 FeaturePrint

813
00:26:48,326 --> 0:26:49,006
现在我通过 App 中定义的

814
00:26:49,006 --> 0:26:51,396
featureprintObservationForImage 函数

815
00:26:51,396 --> 0:26:54,466
提取了参赛者的 FeaturePrint

816
00:26:55,186 --> 0:26:56,956
一旦我有了每一个 FeaturePrint

817
00:26:57,226 --> 0:26:58,666
我就需要确定它

818
00:26:58,666 --> 0:27:00,056
与原始绘图的相似程度

819
00:26:58,666 --> 0:27:00,056
与原始绘图的相似程度

820
00:27:00,056 --> 0:27:01,846
我可以使用 computeDistance 

821
00:27:01,846 --> 0:27:03,246
来做到这一点

822
00:27:03,246 --> 0:27:04,576
它会反馈给我一个浮点值

823
00:27:05,166 --> 0:27:05,826
浮点值越小

824
00:27:05,826 --> 0:27:08,646
说明两个图像越相似

825
00:27:09,236 --> 0:27:10,456
因此 一旦我为每个参赛者

826
00:27:10,456 --> 0:27:11,636
确定了这个

827
00:27:11,636 --> 0:27:13,116
我则只需要对它们进行排序

828
00:27:13,116 --> 0:27:14,276
就可以确定获胜者

829
00:27:15,336 --> 0:27:17,006
好了 这就是

830
00:27:17,156 --> 0:27:18,156
关于图像相似性部分的阐释

831
00:27:18,486 --> 0:27:19,536
现在有请我的搭档 Sergey 

832
00:27:19,536 --> 0:27:20,816
由他和大家共同探讨

833
00:27:20,816 --> 0:27:21,866
人脸识别技术的

834
00:27:21,866 --> 0:27:22,836
变化和发展

835
00:27:23,516 --> 0:27:28,500
[掌声]

836
00:27:33,056 --> 0:27:33,876
&gt;&gt; 大家上午好

837
00:27:34,196 --> 0:27:35,226
我是 Sergey Kamensky

838
00:27:35,226 --> 0:27:36,196
是来自 Vision 框架团队的

839
00:27:36,196 --> 0:27:36,926
一名软件工程师

840
00:27:37,356 --> 0:27:38,456
我很高兴

841
00:27:38,456 --> 0:27:39,786
今天能与大家分享

842
00:27:39,876 --> 0:27:40,926
今年框架中的更多新特性

843
00:27:41,096 --> 0:27:43,606
我们先来谈谈人脸识别技术

844
00:27:44,206 --> 0:27:45,956
记不记得 两年前

845
00:27:45,956 --> 0:27:47,836
当我们介绍 Vision 框架时

846
00:27:47,836 --> 0:27:49,806
我们还谈到了人脸检测识别

847
00:27:50,266 --> 0:27:51,596
今年

848
00:27:51,596 --> 0:27:52,796
我们将对该算法进行新的修改

849
00:27:53,176 --> 0:27:54,806
那么 变化是什么呢

850
00:27:55,916 --> 0:27:57,786
首先

851
00:27:57,926 --> 0:27:59,616
与之前的 65 点消除相比

852
00:27:59,616 --> 0:28:01,426
我们现在发展到了

853
00:27:59,616 --> 0:28:01,426
我们现在发展到了

854
00:28:01,426 --> 0:28:02,166
76 点消除

855
00:28:02,166 --> 0:28:04,116
76 点消除给了我们

856
00:28:04,116 --> 0:28:05,146
更大的密度

857
00:28:05,146 --> 0:28:06,686
来表示不同的面部区域

858
00:28:07,556 --> 0:28:09,556
其次我们现在报告

859
00:28:09,556 --> 0:28:11,306
每个标志点的置信度得分

860
00:28:11,306 --> 0:28:12,886
正如我们之前报告的那样

861
00:28:12,886 --> 0:28:14,836
这是对单个平均置信度得分的对比

862
00:28:14,836 --> 0:28:16,646
不过最大的改进

863
00:28:16,646 --> 0:28:18,126
来自瞳孔检测

864
00:28:18,706 --> 0:28:19,996
如你所见 

865
00:28:19,996 --> 0:28:21,296
右侧的图像检测到瞳孔的

866
00:28:21,296 --> 0:28:23,066
准确度要高得多

867
00:28:23,516 --> 0:28:25,886
我们来看看

868
00:28:25,886 --> 0:28:26,416
客户端代码示例

869
00:28:27,986 --> 0:28:29,676
这个代码片段

870
00:28:29,676 --> 0:28:31,056
将在整个演示过程中重复

871
00:28:31,056 --> 0:28:33,206
所以我们第一次将逐行进行

872
00:28:33,496 --> 0:28:36,246
另外，我在我的样本中

873
00:28:36,246 --> 0:28:37,946
没有使用边界条件

874
00:28:37,946 --> 0:28:39,376
只是为了简化演示

875
00:28:39,376 --> 0:28:40,736
那么在开发 App 时

876
00:28:40,786 --> 0:28:42,276
你也许应该使用适当的错误处理

877
00:28:42,276 --> 0:28:43,516
来避免不想要的边界情况

878
00:28:44,256 --> 0:28:45,156
下面我们回到示例上来

879
00:28:46,276 --> 0:28:47,276
为了得到你的

880
00:28:47,276 --> 0:28:48,866
面部标志

881
00:28:48,866 --> 0:28:49,416
首先你需要创建一个

882
00:28:49,416 --> 0:28:50,716
DetectFaceLandmarksRequest

883
00:28:51,296 --> 0:28:52,536
然后，你需要创建

884
00:28:52,536 --> 0:28:54,036
ImageRequestHandler

885
00:28:54,036 --> 0:28:56,096
将需要处理的图像传递给它

886
00:28:56,096 --> 0:28:58,346
然后使用

887
00:28:58,626 --> 0:28:59,626
该请求处理器

888
00:28:59,626 --> 0:29:00,776
来处理你的请求

889
00:28:59,626 --> 0:29:00,776
来处理你的请求

890
00:29:01,426 --> 0:29:03,606
最后 你需要查看结果

891
00:29:04,386 --> 0:29:05,446
这张人脸

892
00:29:05,446 --> 0:29:06,736
在 Vision 框架中的

893
00:29:06,736 --> 0:29:08,156
所有相关结果

894
00:29:08,196 --> 0:29:09,946
都将以 faceObservation 的形式出现

895
00:29:10,506 --> 0:29:12,006
faceObservation 得出一些

896
00:29:12,006 --> 0:29:13,486
检测到的对象观察

897
00:29:13,856 --> 0:29:15,006
它继承了边界框属性

898
00:29:15,046 --> 0:29:16,396
并且还在其级别上

899
00:29:16,396 --> 0:29:17,766
添加了几个其他属性

900
00:29:17,766 --> 0:29:19,886
来描述人脸

901
00:29:20,766 --> 0:29:21,806
这次我们对

902
00:29:21,806 --> 0:29:22,636
标记属性感兴趣

903
00:29:23,136 --> 0:29:24,366
标记属性属于

904
00:29:24,426 --> 0:29:25,976
FaceLandmarks2D 类

905
00:29:26,176 --> 0:29:28,166
FaceLandmarks2D 类

906
00:29:28,416 --> 0:29:29,896
由置信度得分组成

907
00:29:30,086 --> 0:29:31,356
这是整个集合

908
00:29:31,356 --> 0:29:32,476
和多个面部区域的

909
00:29:32,476 --> 0:29:34,716
单个平均置信度得分

910
00:29:34,716 --> 0:29:37,296
其中每个面部区域

911
00:29:37,296 --> 0:29:40,176
由 FaceLandmarksRegion2D 类表示

912
00:29:40,636 --> 0:29:41,856
让我们仔细看一下

913
00:29:41,856 --> 0:29:44,536
这个类的属性

914
00:29:44,716 --> 0:29:46,166
首先是 pointCount

915
00:29:46,666 --> 0:29:48,156
PointCount 将告诉你

916
00:29:48,156 --> 0:29:49,546
有多少点代表

917
00:29:49,546 --> 0:29:50,756
特定的面部区域

918
00:29:50,996 --> 0:29:52,256
此属性将显示不同的值 

919
00:29:52,256 --> 0:29:53,616
具体取决于

920
00:29:53,616 --> 0:29:55,366
你如何配置请求

921
00:29:55,366 --> 0:29:56,936
比如 65 点消除

922
00:29:56,936 --> 0:29:58,376
还是 76 点消除

923
00:29:59,646 --> 0:30:01,146
normalizedPoints 属性

924
00:29:59,646 --> 0:30:01,146
normalizedPoints 属性

925
00:30:01,916 --> 0:30:05,596
将表示实际的标记点

926
00:30:05,596 --> 0:30:07,326
而 precisionEstimatesPerPoint 将表示

927
00:30:07,326 --> 0:30:08,916
实际标记点的

928
00:30:08,916 --> 0:30:10,416
实际置信度得分

929
00:30:11,456 --> 0:30:12,666
我们来看看所需的代码

930
00:30:12,666 --> 0:30:14,496
这与上一张幻灯片中

931
00:30:14,496 --> 0:30:16,106
代码片段相同

932
00:30:16,106 --> 0:30:17,056
但现在我们将从

933
00:30:17,056 --> 0:30:18,196
一个稍微不同的角度来看待它

934
00:30:18,576 --> 0:30:20,266
我们想看看

935
00:30:20,266 --> 0:30:22,696
在 Vision 框架中如何修改算法

936
00:30:23,446 --> 0:30:25,056
如果你获取此代码段

937
00:30:25,056 --> 0:30:26,276
并用去年的 SDK

938
00:30:26,276 --> 0:30:28,076
重新编译它

939
00:30:28,076 --> 0:30:30,086
你将得到的请求对象

940
00:30:30,086 --> 0:30:31,806
将配置如下

941
00:30:31,806 --> 0:30:33,196
Revision 属性将设置为 

942
00:30:33,196 --> 0:30:34,786
Revision2

943
00:30:34,786 --> 0:30:36,136
Cancellation 属性将设置为

944
00:30:36,136 --> 0:30:37,816
Cancellation65Points

945
00:30:38,396 --> 0:30:39,056
从技术上讲

946
00:30:39,056 --> 0:30:40,286
去年我们没有消除属性

947
00:30:40,286 --> 0:30:41,456
但如果我们这样做的话

948
00:30:41,456 --> 0:30:42,616
可以将它设置为一个单一的值

949
00:30:43,426 --> 0:30:45,726
另一方面 如果你获取相同的代码片段

950
00:30:45,726 --> 0:30:49,136
并用今年的 SDK 重新编译它

951
00:30:49,686 --> 0:30:50,336
你将得到的是 

952
00:30:50,336 --> 0:30:51,856
Revision 属性将被设置为

953
00:30:51,856 --> 0:30:52,846
Revision3

954
00:30:52,846 --> 0:30:55,056
Cancellation 属性

955
00:30:55,056 --> 0:30:57,076
将被设置为 Cancellation76Points

956
00:30:58,626 --> 0:30:59,706
这实际上代表了

957
00:30:59,706 --> 0:31:00,846
Vision 框架在默认情况下

958
00:30:59,706 --> 0:31:00,846
Vision 框架在默认情况下

959
00:31:00,846 --> 0:31:04,006
如何处理算法修订的原理

960
00:31:04,096 --> 0:31:05,766
如果你不能指定修订版

961
00:31:05,766 --> 0:31:07,716
我们将提供

962
00:31:07,716 --> 0:31:09,366
编译和链接代码所依据的

963
00:31:09,366 --> 0:31:11,916
SDK支持的最新版本

964
00:31:12,116 --> 0:31:14,066
当然

965
00:31:14,066 --> 0:31:14,856
我们始终建议

966
00:31:14,856 --> 0:31:15,836
明确设置这些属性

967
00:31:16,116 --> 0:31:17,106
这只是为了保证

968
00:31:17,106 --> 0:31:18,656
将来的确定性行为

969
00:31:19,386 --> 0:31:22,266
现在让我们使用

970
00:31:22,266 --> 0:31:23,556
今年开发的一项新指标

971
00:31:23,556 --> 0:31:24,286
面部捕捉质量

972
00:31:24,786 --> 0:31:25,946
屏幕上有两个图像

973
00:31:26,296 --> 0:31:27,416
你可以清楚地看到

974
00:31:27,416 --> 0:31:28,536
其中一幅图像

975
00:31:28,536 --> 0:31:29,436
是在更好的照明

976
00:31:29,436 --> 0:31:29,996
和聚焦条件下拍摄的

977
00:31:30,846 --> 0:31:32,016
我们想开发一个度量标准

978
00:31:32,016 --> 0:31:33,376
将图像作为一个整体来看待

979
00:31:33,376 --> 0:31:35,016
并给出一个分数

980
00:31:35,016 --> 0:31:36,796
用来说明面部捕捉质量

981
00:31:36,796 --> 0:31:37,966
的好坏

982
00:31:37,966 --> 0:31:40,366
因此 

983
00:31:40,366 --> 0:31:41,596
我们提出了面部捕捉质量指标

984
00:31:42,466 --> 0:31:43,656
我们用这种方法

985
00:31:43,656 --> 0:31:45,026
训练我们的模型

986
00:31:45,026 --> 0:31:46,876
因此如果图像是用弱光

987
00:31:46,876 --> 0:31:48,456
或弱焦点拍摄的

988
00:31:48,456 --> 0:31:50,206
或一个人有负面表情

989
00:31:50,236 --> 0:31:51,936
那他们的得分就会更低

990
00:31:52,896 --> 0:31:54,316
如果我们在这两个图像上

991
00:31:54,346 --> 0:31:55,626
运行此指标

992
00:31:55,626 --> 0:31:56,206
我们将得到我们的分数

993
00:31:57,036 --> 0:31:58,316
这些是浮点数

994
00:31:58,636 --> 0:31:59,636
你可以将它们相互比较

995
00:31:59,636 --> 0:32:00,776
你可以说

996
00:31:59,636 --> 0:32:00,776
你可以说

997
00:32:00,776 --> 0:32:02,856
得分更高的图像

998
00:32:02,856 --> 0:32:04,716
就是质量更好的图像

999
00:32:05,346 --> 0:32:08,136
接下来我们看一下代码示例

1000
00:32:09,606 --> 0:32:10,756
这与我们之前看到的

1001
00:32:10,756 --> 0:32:11,986
几张幻灯片非常相似

1002
00:32:12,196 --> 0:32:13,416
不同之处

1003
00:32:13,416 --> 0:32:16,136
在于请求类型和结果

1004
00:32:16,846 --> 0:32:18,526
由于我们使用的图像没变

1005
00:32:18,736 --> 0:32:19,556
我们可以再次查看 faceObservation 的结果

1006
00:32:19,556 --> 0:32:20,826
但现在我们要看一下

1007
00:32:20,826 --> 0:32:21,686
faceObservation 的不同属性

1008
00:32:21,686 --> 0:32:24,626
即面部捕捉质量属性

1009
00:32:24,876 --> 0:32:27,946
我们一起来看些其他的例子

1010
00:32:28,616 --> 0:32:29,646
假设

1011
00:32:29,646 --> 0:32:30,586
我有一系列图像

1012
00:32:30,586 --> 0:32:31,976
可以通过自拍相机

1013
00:32:31,976 --> 0:32:34,316
或照片连拍中的连拍模式获得

1014
00:32:34,626 --> 0:32:36,166
你要问自己一个问题

1015
00:32:36,426 --> 0:32:37,816
哪张照片

1016
00:32:37,816 --> 0:32:38,566
拍摄的质量最好

1017
00:32:39,686 --> 0:32:40,966
现在你可以做的是

1018
00:32:40,966 --> 0:32:42,166
对每一张图像运行我们的算法

1019
00:32:42,506 --> 0:32:45,366
分配分数 对它们进行排名

1020
00:32:45,366 --> 0:32:47,876
最亮的地方的图像

1021
00:32:47,876 --> 0:32:50,896
是以最佳质量捕捉的图像

1022
00:32:50,966 --> 0:32:52,436
我们试着去理解

1023
00:32:52,436 --> 0:32:54,026
如何解释

1024
00:32:54,026 --> 0:32:55,856
面部捕捉质量指标的结果

1025
00:32:56,586 --> 0:32:58,206
在幻灯片上

1026
00:32:58,356 --> 0:32:58,886
我有两个图像序列

1027
00:32:59,426 --> 0:33:00,726
每个序列都是同一个人的

1028
00:32:59,426 --> 0:33:00,726
每个序列都是同一个人的

1029
00:33:00,726 --> 0:33:02,146
每个序列由

1030
00:33:02,146 --> 0:33:03,456
在面部捕获质量方面

1031
00:33:03,456 --> 0:33:07,216
得分最低和最高的图像表示

1032
00:33:08,116 --> 0:33:09,076
对于这些范围

1033
00:33:09,076 --> 0:33:09,496
我们能说些什么呢

1034
00:33:10,816 --> 0:33:12,356
存在一些重叠区域 

1035
00:33:12,356 --> 0:33:13,696
但也有一些区域

1036
00:33:13,696 --> 0:33:15,006
属于某一区域

1037
00:33:15,006 --> 0:33:16,336
而不属于另一个区域

1038
00:33:16,516 --> 0:33:18,216
如果你有另一个序列

1039
00:33:18,536 --> 0:33:19,306
可能会发生

1040
00:33:19,306 --> 0:33:20,926
完全没有重叠区域的情况

1041
00:33:21,856 --> 0:33:22,976
在这里我提出的观点是

1042
00:33:22,976 --> 0:33:24,686
不应将面部捕捉质量

1043
00:33:24,686 --> 0:33:26,906
与阈值进行比较

1044
00:33:28,126 --> 0:33:29,466
在这个特定的例子中

1045
00:33:29,466 --> 0:33:32,096
如果我选择 0.52

1046
00:33:32,326 --> 0:33:33,876
我将会错过左边的所有图像

1047
00:33:33,876 --> 0:33:36,556
而且我几乎可以得到

1048
00:33:36,556 --> 0:33:37,406
刚刚超过右边中点的

1049
00:33:37,406 --> 0:33:38,626
所有图像

1050
00:33:39,996 --> 0:33:41,676
但是什么是面部捕捉质量呢

1051
00:33:42,706 --> 0:33:44,246
我们定义面部捕获质量

1052
00:33:44,246 --> 0:33:46,286
是对同一主题的比较

1053
00:33:46,326 --> 0:33:47,766
或排名测量

1054
00:33:48,186 --> 0:33:49,756
现在比较和相同

1055
00:33:49,756 --> 0:33:51,086
是这句话的关键词

1056
00:33:51,526 --> 0:33:53,216
如果你在想

1057
00:33:53,216 --> 0:33:54,476
很酷我有这个很棒的新指标

1058
00:33:54,476 --> 0:33:55,606
我将用它开发我的

1059
00:33:55,606 --> 0:33:55,726
选美比赛 App

1060
00:33:56,946 --> 0:33:58,126
我想可能不是一个好主意

1061
00:33:58,606 --> 0:33:59,636
在选美比赛 App 中

1062
00:33:59,636 --> 0:34:01,296
你必须比较不同人的面孔

1063
00:33:59,636 --> 0:34:01,296
你必须比较不同人的面孔

1064
00:34:01,296 --> 0:34:02,806
而这并不是该指标

1065
00:34:02,806 --> 0:34:04,626
开发和设计的目的

1066
00:34:06,266 --> 0:34:07,696
这就是人脸技术的改变和发展

1067
00:34:09,295 --> 0:34:10,076
下面我们来看一下

1068
00:34:10,076 --> 0:34:11,146
今年新增加的

1069
00:34:11,146 --> 0:34:11,335
探测技术

1070
00:34:12,896 --> 0:34:15,466
我们正在推出人体探测器

1071
00:34:15,466 --> 0:34:16,726
探测由人头和躯干组成的

1072
00:34:16,726 --> 0:34:18,076
人体上半身

1073
00:34:18,076 --> 0:34:20,386
还有一个宠物探测器

1074
00:34:20,795 --> 0:34:22,326
一个探测猫和狗的

1075
00:34:22,696 --> 0:34:23,406
动物探测器

1076
00:34:23,735 --> 0:34:24,755
动物探测器能为你提供边界框

1077
00:34:24,755 --> 0:34:26,166
除了边界框之外

1078
00:34:26,166 --> 0:34:27,286
它还为你提供了一个标签

1079
00:34:27,286 --> 0:34:30,235
说明检测到哪种动物

1080
00:34:31,795 --> 0:34:32,746
我们来看看

1081
00:34:32,746 --> 0:34:33,226
客户端代码示例

1082
00:34:35,956 --> 0:34:37,916
两个片段

1083
00:34:37,916 --> 0:34:39,366
一个用于人体探测器

1084
00:34:39,366 --> 0:34:39,815
一个用于动物探测器

1085
00:34:40,326 --> 0:34:41,386
与我们之前的情况非常相似

1086
00:34:41,386 --> 0:34:41,835
非常相似

1087
00:34:42,016 --> 0:34:43,646
同样差异在于你创建的

1088
00:34:43,646 --> 0:34:46,386
请求类型和结果中

1089
00:34:47,186 --> 0:34:49,525
现在对于人类探测器部分

1090
00:34:49,826 --> 0:34:51,275
我们关心的只是边界框

1091
00:34:51,886 --> 0:34:53,166
因此我们用其为

1092
00:34:53,306 --> 0:34:54,606
DetectedObjectObservation 服务

1093
00:34:55,726 --> 0:34:56,706
另一方面

1094
00:34:56,706 --> 0:34:57,706
对于动物探测器

1095
00:34:57,706 --> 0:34:58,986
我们也需要标签

1096
00:34:59,676 --> 0:35:01,206
因此我们使用

1097
00:34:59,676 --> 0:35:01,206
因此我们使用

1098
00:35:01,206 --> 0:35:02,496
源于检测到的对象观察的

1099
00:35:02,496 --> 0:35:03,076
RecognizedObjectObservation

1100
00:35:03,186 --> 0:35:04,536
它延续了边界框

1101
00:35:04,536 --> 0:35:07,296
但也在边界框上添加了标签属性

1102
00:35:07,806 --> 0:35:10,486
这是新的探测器

1103
00:35:11,246 --> 0:35:12,456
下面让我们来看看

1104
00:35:12,456 --> 0:35:13,336
今年有关追踪的新内容

1105
00:35:14,216 --> 0:35:15,126
我们正在为追踪技术

1106
00:35:15,126 --> 0:35:16,556
设计一个新修订版

1107
00:35:16,556 --> 0:35:17,996
今年的变化是

1108
00:35:17,996 --> 0:35:19,516
边界框扩展区域

1109
00:35:19,516 --> 0:35:20,396
有所改进

1110
00:35:21,266 --> 0:35:22,386
我们现在可以更好的处理

1111
00:35:22,386 --> 0:35:22,976
遮挡问题

1112
00:35:23,536 --> 0:35:25,716
我们这次是以

1113
00:35:25,716 --> 0:35:26,176
机器学习为基础的

1114
00:35:26,886 --> 0:35:28,086
我们可以在多个

1115
00:35:28,086 --> 0:35:29,046
处理器设备上

1116
00:35:29,046 --> 0:35:29,476
以低功耗运行

1117
00:35:29,476 --> 0:35:32,516
我们来看一个示例

1118
00:35:32,956 --> 0:35:35,056
我这里有一个微视频剪辑

1119
00:35:35,056 --> 0:35:36,446
视频中一个男人在森林里跑步

1120
00:35:36,606 --> 0:35:38,646
他有时出现在树后

1121
00:35:38,976 --> 0:35:40,126
如你所见

1122
00:35:40,126 --> 0:35:41,606
跟踪器能够成功地

1123
00:35:41,606 --> 0:35:42,926
重新捕捉跟踪对象

1124
00:35:42,926 --> 0:35:44,186
并继续跟踪序列

1125
00:35:46,016 --> 0:35:47,046
[掌声]

1126
00:35:47,046 --> 0:35:47,486
谢谢

1127
00:35:48,516 --> 0:35:51,976
[掌声]

1128
00:35:52,476 --> 0:35:54,256
我们来看看客户端代码示例

1129
00:35:54,676 --> 0:35:56,046
这与我们去年

1130
00:35:56,046 --> 0:35:56,996
展示的片段完全相同

1131
00:35:57,256 --> 0:35:58,306
它代表了

1132
00:35:58,306 --> 0:35:59,416
你可以想象到的

1133
00:35:59,416 --> 0:35:59,946
最简单的跟踪序列

1134
00:36:00,166 --> 0:36:01,106
它会连续 5 帧

1135
00:36:01,106 --> 0:36:02,786
跟踪你感兴趣的对象

1136
00:36:03,986 --> 0:36:05,356
我想逐行解释

1137
00:36:05,356 --> 0:36:07,116
但在这里我想强调两点

1138
00:36:07,486 --> 0:36:09,116
首先是我们使用

1139
00:36:09,116 --> 0:36:10,036
SequenceRequestHandler

1140
00:36:11,066 --> 0:36:12,046
这与我们目前

1141
00:36:12,046 --> 0:36:13,226
在整个演示中使用的

1142
00:36:13,226 --> 0:36:14,226
ImageRequestHandler

1143
00:36:14,226 --> 0:36:14,816
完全不同

1144
00:36:15,336 --> 0:36:16,616
SequenceRequestHandler

1145
00:36:16,616 --> 0:36:17,986
在处理帧序列时用于 Vision

1146
00:36:17,986 --> 0:36:19,216
你需要在

1147
00:36:19,216 --> 0:36:20,426
帧与帧之间

1148
00:36:20,426 --> 0:36:21,496
缓存一些信息

1149
00:36:22,826 --> 0:36:24,356
第二点是

1150
00:36:24,356 --> 0:36:25,106
在实现跟踪序列时

1151
00:36:25,106 --> 0:36:26,836
需要从迭代编号 n 中

1152
00:36:26,836 --> 0:36:28,356
获取结果

1153
00:36:28,356 --> 0:36:30,026
并将其作为输入

1154
00:36:30,026 --> 0:36:31,276
提供给持续时间数 n+1

1155
00:36:31,976 --> 0:36:35,216
当然如果你使用

1156
00:36:35,216 --> 0:36:36,106
当前版本 SDK 重新编译

1157
00:36:36,106 --> 0:36:37,646
则默认情况下

1158
00:36:37,646 --> 0:36:38,846
请求的 revision

1159
00:36:38,846 --> 0:36:40,316
将设置为 Revision2

1160
00:36:40,496 --> 0:36:41,836
不过我们也

1161
00:36:41,836 --> 0:36:42,426
建议明确设置它

1162
00:36:42,966 --> 0:36:45,106
这就是跟踪

1163
00:36:46,136 --> 0:36:47,376
让我们来看看

1164
00:36:47,376 --> 0:36:50,396
有关 Vision 和 Core ML 集成的消息

1165
00:36:51,146 --> 0:36:52,766
去年 我们展示了

1166
00:36:52,766 --> 0:36:53,836
Vision 和 Core ML 的集成

1167
00:36:53,836 --> 0:36:55,496
并展示了

1168
00:36:55,496 --> 0:36:57,066
如何通过 Vision API 

1169
00:36:57,066 --> 0:36:57,596
运行 Core ML 模型

1170
00:36:57,596 --> 0:36:59,776
这样做的好处是

1171
00:36:59,776 --> 0:37:01,706
你可以使用 1 到 5 个

1172
00:36:59,776 --> 0:37:01,706
你可以使用 1 到 5 个

1173
00:37:01,816 --> 0:37:03,236
不同的图像请求处理程序重载

1174
00:37:03,236 --> 0:37:04,916
来将你手中的图像

1175
00:37:04,916 --> 0:37:06,266
转换为 Core ML 模型所需的

1176
00:37:06,586 --> 0:37:08,976
图像类型

1177
00:37:08,976 --> 0:37:11,266
大小和颜色方案

1178
00:37:12,246 --> 0:37:13,256
我们将为你运行推理

1179
00:37:13,256 --> 0:37:15,016
并将来自 Core ML 模型的

1180
00:37:15,016 --> 0:37:16,436
输出或结果

1181
00:37:16,436 --> 0:37:17,886
打包到 Vision 观察值中

1182
00:37:20,716 --> 0:37:22,426
现在假设你有

1183
00:37:22,426 --> 0:37:23,696
一个不同的任务

1184
00:37:23,696 --> 0:37:24,646
例如你想进行

1185
00:37:24,646 --> 0:37:26,066
图像样式传输

1186
00:37:26,066 --> 0:37:27,416
你需要至少有两个图像

1187
00:37:27,416 --> 0:37:29,056
即图像内容和图像样式

1188
00:37:29,256 --> 0:37:30,356
你可能还需要

1189
00:37:30,356 --> 0:37:31,816
一些混合比例

1190
00:37:31,926 --> 0:37:33,826
说明在内容上需要应用多少样式

1191
00:37:34,306 --> 0:37:35,506
所以现在我有三个参数

1192
00:37:36,826 --> 0:37:37,886
今年

1193
00:37:37,886 --> 0:37:39,666
我们将推出 API

1194
00:37:39,666 --> 0:37:41,716
在这里我们可以

1195
00:37:41,896 --> 0:37:43,476
通过 Vision 将多个输入用于 Core ML

1196
00:37:43,476 --> 0:37:44,426
这包括多个图像输入

1197
00:37:44,426 --> 0:37:47,696
此外在输出部分

1198
00:37:48,186 --> 0:37:49,686
这个示例仅显示了一个输出

1199
00:37:49,686 --> 0:37:50,616
但是如果你有多个

1200
00:37:50,616 --> 0:37:52,116
特别是如果你有

1201
00:37:52,116 --> 0:37:53,376
多个相同类型

1202
00:37:53,376 --> 0:37:55,256
那么当它们后面

1203
00:37:55,256 --> 0:37:56,356
以观察形式出现时

1204
00:37:56,356 --> 0:37:57,946
很难区分开来

1205
00:37:58,416 --> 0:37:59,616
那么今年我们做了什么

1206
00:37:59,616 --> 0:38:00,646
我们在观察中

1207
00:37:59,616 --> 0:38:00,646
我们在观察中

1208
00:38:00,646 --> 0:38:02,616
引入了一个新的字段

1209
00:38:02,616 --> 0:38:04,376
该字段完全映射到

1210
00:38:04,376 --> 0:38:05,056
输出部分中显示的名称

1211
00:38:06,216 --> 0:38:08,316
我们来看看输入和输出

1212
00:38:08,316 --> 0:38:09,316
我们将在下一张幻灯片中

1213
00:38:09,316 --> 0:38:09,576
使用它们

1214
00:38:12,936 --> 0:38:14,136
这是一段代码片段

1215
00:38:14,176 --> 0:38:16,816
它表示如何通过 Vision

1216
00:38:16,956 --> 0:38:17,396
使用 Core ML

1217
00:38:18,676 --> 0:38:20,236
突出显示的部分

1218
00:38:20,446 --> 0:38:21,256
显示了今年的新内容

1219
00:38:21,716 --> 0:38:22,796
我们现在先暂时保存它们

1220
00:38:22,796 --> 0:38:23,896
我们去查看代码

1221
00:38:23,896 --> 0:38:25,316
稍后再返回

1222
00:38:26,156 --> 0:38:27,466
要通过 Vision 运行 Core ML

1223
00:38:27,466 --> 0:38:29,446
首先需要

1224
00:38:29,446 --> 0:38:30,236
记录 Core ML 模型

1225
00:38:31,266 --> 0:38:32,926
然后你需要围绕它创建

1226
00:38:32,926 --> 0:38:34,956
Vision CoreMLmodel 包装器

1227
00:38:35,726 --> 0:38:37,476
之后你需要创建

1228
00:38:37,476 --> 0:38:39,346
Vision CoreMLRequest

1229
00:38:39,346 --> 0:38:39,676
并传入该包装器

1230
00:38:41,266 --> 0:38:42,326
下一步

1231
00:38:42,556 --> 0:38:44,136
创建 ImageRequestHandler

1232
00:38:44,136 --> 0:38:45,446
处理你的请求

1233
00:38:45,446 --> 0:38:46,086
然后查看结果

1234
00:38:47,386 --> 0:38:49,596
现在 使用我们今年添加的新 API

1235
00:38:49,596 --> 0:38:51,596
只有你去年

1236
00:38:51,596 --> 0:38:53,116
可以使用的图像是默认图像

1237
00:38:53,116 --> 0:38:54,986
或者主图像

1238
00:38:54,986 --> 0:38:56,276
是传递给 ImageRequestHandler 的图像

1239
00:38:56,476 --> 0:38:58,046
但这也是需要

1240
00:38:58,046 --> 0:38:59,526
将图像名称分配给

1241
00:38:59,526 --> 0:39:01,126
CoreMLModel 包装器的

1242
00:38:59,526 --> 0:39:01,126
CoreMLModel 包装器的

1243
00:39:01,506 --> 0:39:03,796
inputImageFeatureName 字段

1244
00:39:05,026 --> 0:39:06,886
所有其他参数

1245
00:39:06,886 --> 0:39:08,646
无论图像与否

1246
00:39:08,716 --> 0:39:10,406
都必须通过 CoreMLmodel 包装的

1247
00:39:10,446 --> 0:39:12,016
featureProvider 属性传递

1248
00:39:12,376 --> 0:39:14,036
如你所见

1249
00:39:14,036 --> 0:39:15,436
图像样式和混合比例

1250
00:39:15,436 --> 0:39:15,666
是以这种方式传递的

1251
00:39:16,956 --> 0:39:18,336
最后当你查看结果时

1252
00:39:18,336 --> 0:39:19,506
你可以查看

1253
00:39:19,786 --> 0:39:21,386
出现的观察结果的

1254
00:39:21,386 --> 0:39:22,646
featureName

1255
00:39:22,646 --> 0:39:24,186
并且你可以在此情况下

1256
00:39:24,186 --> 0:39:25,246
将其与 imageResult 进行比较

1257
00:39:25,506 --> 0:39:26,656
这正是 Core ML 输出部分中

1258
00:39:26,656 --> 0:39:27,706
出现的名称

1259
00:39:27,756 --> 0:39:29,066
这样你就可以

1260
00:39:29,096 --> 0:39:30,666
相应地处理结果

1261
00:39:32,636 --> 0:39:33,626
今天的演示

1262
00:39:33,626 --> 0:39:34,666
到这里就要结束了

1263
00:39:34,966 --> 0:39:36,076
想要获得其他详细信息

1264
00:39:36,076 --> 0:39:37,136
请参阅幻灯片上的链接

1265
00:39:37,466 --> 0:39:39,976
谢谢大家

1266
00:39:40,016 --> 0:39:42,000
[掌声]
