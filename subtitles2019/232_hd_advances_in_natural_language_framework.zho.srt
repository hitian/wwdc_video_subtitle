1
00:00:00,506 --> 0:00:04,500
[音乐]

2
00:00:07,516 --> 0:00:11,286
[掌声]

3
00:00:11,786 --> 0:00:13,076
&gt;&gt; 大家下午好

4
00:00:13,546 --> 0:00:14,736
欢迎来到我们的讲演

5
00:00:14,736 --> 0:00:15,976
自然语言处理

6
00:00:16,566 --> 0:00:18,266
我叫 Vivek 我的同事 

7
00:00:18,266 --> 0:00:19,656
Doug Davidson 会和我一起

8
00:00:19,656 --> 0:00:20,806
完成讲演

9
00:00:21,646 --> 0:00:23,086
我们开始吧

10
00:00:23,716 --> 0:00:27,796
如你所知 文本无处不在 随处可见

11
00:00:28,436 --> 0:00:29,826
用户在 App 中  

12
00:00:29,826 --> 0:00:31,256
和文本交互的模式

13
00:00:31,906 --> 0:00:34,796
主要有两个

14
00:00:34,796 --> 0:00:36,376
一是通过自然语言输入

15
00:00:36,456 --> 0:00:42,296
用户在 App 内写入文本 或生成文本

16
00:00:43,176 --> 0:00:46,136
比如用户可能在 App 里 

17
00:00:46,136 --> 0:00:48,266
用键盘键入文本

18
00:00:48,526 --> 0:00:49,996
这种 App 有很多

19
00:00:49,996 --> 0:00:51,506
比如 《信息》

20
00:00:51,506 --> 0:00:53,566
用户写入文本然后分享给其他人

21
00:00:53,566 --> 0:00:55,456
还有《备忘录》

22
00:00:55,456 --> 0:00:56,956
或者任何效率 App 

23
00:00:56,956 --> 0:00:58,776
部分功能需要你键入文本

24
00:01:00,246 --> 0:01:01,136
另一种用户

25
00:01:01,136 --> 0:01:03,126
和 App 内文本互动的方式

26
00:01:03,126 --> 0:01:05,626
是通过自然语言输出

27
00:01:06,046 --> 0:01:07,696
App 把文本内容

28
00:01:07,696 --> 0:01:09,126
提供给用户

29
00:01:09,286 --> 0:01:11,706
用户使用或者读取这个文本

30
00:01:12,836 --> 0:01:14,256
这类 App 有

31
00:01:14,436 --> 0:01:17,346
比如《新闻》

32
00:01:17,346 --> 0:01:18,466
信息或者文本展示给用户

33
00:01:18,466 --> 0:01:20,796
用户阅读这个信息

34
00:01:21,666 --> 0:01:24,116
所以 不论是文本输入还是输出

35
00:01:24,226 --> 0:01:26,686
为了从原始文本中

36
00:01:26,686 --> 0:01:28,366
提取可操作的情报

37
00:01:28,546 --> 0:01:30,346
自然语言处理

38
00:01:30,346 --> 0:01:32,146
都是非常重要的

39
00:01:32,146 --> 0:01:34,816
去年 我们介绍了

40
00:01:34,816 --> 0:01:36,086
自然语言框架

41
00:01:36,796 --> 0:01:38,236
自然语言框架

42
00:01:38,406 --> 0:01:41,016
是 Apple 所有平台通用的 

43
00:01:41,016 --> 0:01:42,796
可处理所有东西的自然语言处理的主力

44
00:01:43,406 --> 0:01:44,476
所以我们提供了几个

45
00:01:44,546 --> 0:01:46,256
基础的 NLP 模块

46
00:01:46,696 --> 0:01:48,276
比如语言识别

47
00:01:48,276 --> 0:01:51,666
分词 词性标注等等

48
00:01:51,666 --> 0:01:53,036
我们展示了这些基础功能

49
00:01:53,036 --> 0:01:55,796
并跨语种提供这些功能

50
00:01:55,796 --> 0:01:58,456
这是通过无缝融合 

51
00:01:58,456 --> 0:02:00,156
语言学和机器学习实现的

52
00:01:58,456 --> 0:02:00,156
语言学和机器学习实现的

53
00:02:00,156 --> 0:02:01,516
所以你可以只关注  

54
00:02:01,516 --> 0:02:03,356
通过使用这些 API

55
00:02:03,356 --> 0:02:05,526
搭建你的 App

56
00:02:05,526 --> 0:02:07,706
繁重的工作 我们在幕后处理

57
00:02:08,336 --> 0:02:09,955
现在 如果你退一步

58
00:02:09,955 --> 0:02:11,306
看看所有这些功能

59
00:02:11,536 --> 0:02:13,466
实际上 如果你看看

60
00:02:13,466 --> 0:02:14,936
大部分 NLP 功能

61
00:02:14,936 --> 0:02:16,676
它们可以分为

62
00:02:17,006 --> 0:02:19,176
两大类任务

63
00:02:19,176 --> 0:02:20,936
第一类是文本分类

64
00:02:21,066 --> 0:02:22,796
文本分类的目的是

65
00:02:22,796 --> 0:02:24,866
给一个文本

66
00:02:24,866 --> 0:02:26,286
这个文本可以是一个句子

67
00:02:26,286 --> 0:02:28,266
可以是一个段落

68
00:02:28,266 --> 0:02:30,256
或者一个文件

69
00:02:30,256 --> 0:02:31,566
你想向这个文本

70
00:02:31,566 --> 0:02:33,336
分配标签

71
00:02:33,336 --> 0:02:35,096
这些标签可以是情感标签

72
00:02:35,096 --> 0:02:37,096
可以是话题标签 任何你想分配的标签

73
00:02:38,426 --> 0:02:40,036
另一类 NLP 任务被称为

74
00:02:40,036 --> 0:02:41,696
单词标注

75
00:02:41,696 --> 0:02:43,576
这里的任务

76
00:02:43,576 --> 0:02:45,176
或这里的目的是

77
00:02:45,176 --> 0:02:46,756
给一系列词 也被称为 token

78
00:02:46,756 --> 0:02:48,606
我们想给这个序列里的

79
00:02:48,606 --> 0:02:51,456
每个 token 分配一个标签

80
00:02:51,656 --> 0:02:54,366
今年 在文本分类

81
00:02:54,366 --> 0:02:55,986
和单词标记中 

82
00:02:55,986 --> 0:02:57,776
我们都有新的 API

83
00:02:57,776 --> 0:02:59,646
首先 我们先从

84
00:02:59,646 --> 0:03:01,616
情感分析开始

85
00:02:59,646 --> 0:03:01,616
情感分析开始

86
00:03:02,356 --> 0:03:04,016
情感分析是一个文本分类 API

87
00:03:04,016 --> 0:03:05,666
这是个新的 API

88
00:03:05,666 --> 0:03:07,946
是这样运作的

89
00:03:08,536 --> 0:03:09,766
你要做的是

90
00:03:09,766 --> 0:03:11,496
把文本传输到这个 API

91
00:03:11,496 --> 0:03:14,016
API 分析文本

92
00:03:14,046 --> 0:03:15,816
给你一个

93
00:03:15,816 --> 0:03:16,656
情感分值

94
00:03:17,466 --> 0:03:18,666
这个情感分值

95
00:03:18,786 --> 0:03:20,536
捕捉文本里的 

96
00:03:20,536 --> 0:03:21,686
情感程度

97
00:03:22,786 --> 0:03:24,106
情感分值

98
00:03:24,106 --> 0:03:26,626
从负一到正一不等

99
00:03:26,626 --> 0:03:27,516
表示情感的程度

100
00:03:28,216 --> 0:03:29,516
比如 -1.0 表明是一个

101
00:03:29,516 --> 0:03:31,396
非常强烈的消极情感

102
00:03:31,396 --> 0:03:33,256
1.0 表明是

103
00:03:33,256 --> 0:03:34,336
非常强烈的积极情感

104
00:03:34,886 --> 0:03:36,056
所以基本上 

105
00:03:36,056 --> 0:03:37,506
我们提供一个分值 

106
00:03:37,506 --> 0:03:38,846
让你为 App 测定分值

107
00:03:39,036 --> 0:03:41,356
举个例子 如果有一句话

108
00:03:41,356 --> 0:03:42,836
比如 我们在夏威夷

109
00:03:42,836 --> 0:03:44,386
和家人玩得很开心

110
00:03:44,586 --> 0:03:45,946
API 可能会给出 0.8 分

111
00:03:45,946 --> 0:03:48,166
表明这句话

112
00:03:48,166 --> 0:03:50,476
是一句积极的句子

113
00:03:51,616 --> 0:03:52,756
相反 如果这句话是

114
00:03:52,796 --> 0:03:54,266
我们在夏威夷玩得不好

115
00:03:54,266 --> 0:03:55,656
因为妈妈扭伤了她的脚踝

116
00:03:55,656 --> 0:03:57,166
这不是一句积极的话

117
00:03:57,166 --> 0:03:59,156
所以你得到 -0.8 分

118
00:03:59,156 --> 0:04:00,956
然后你就可以判定

119
00:03:59,156 --> 0:04:00,956
然后你就可以判定

120
00:04:00,956 --> 0:04:02,836
这是一个消极的情感

121
00:04:04,126 --> 0:04:05,576
非常棒 你怎么用呢

122
00:04:06,376 --> 0:04:07,676
用起来真的很简单

123
00:04:08,006 --> 0:04:09,646
对于你们 习惯使用 

124
00:04:09,646 --> 0:04:11,036
NaturalLanguage 的人

125
00:04:11,036 --> 0:04:11,886
这会非常简单

126
00:04:12,416 --> 0:04:13,736
导入 NaturalLanguage

127
00:04:14,236 --> 0:04:15,496
创建一个实例 NLTagger

128
00:04:15,536 --> 0:04:17,766
现在你做的就是

129
00:04:17,836 --> 0:04:19,646
指定一个新的标签方案

130
00:04:19,786 --> 0:04:22,456
这个标签方案被称为情感分值

131
00:04:23,266 --> 0:04:25,116
然后把想分析的字符串 

132
00:04:25,116 --> 0:04:26,466
附加到 tagger

133
00:04:26,466 --> 0:04:28,256
然后你只需要

134
00:04:28,256 --> 0:04:29,576
在句子层面

135
00:04:29,906 --> 0:04:32,446
或者在段落层面请求情感分值

136
00:04:33,476 --> 0:04:34,996
看一下实际运行情况

137
00:04:36,316 --> 0:04:37,576
这儿有一个

138
00:04:37,576 --> 0:04:38,516
假设 App

139
00:04:38,876 --> 0:04:40,106
它是个奶酪 App

140
00:04:40,596 --> 0:04:42,766
作为这个 App 的一部分

141
00:04:42,766 --> 0:04:44,246
用户可以做很多事情

142
00:04:44,536 --> 0:04:46,136
他们可以写关于奶酪的笔记

143
00:04:46,416 --> 0:04:47,946
他们可以写评价

144
00:04:47,946 --> 0:04:49,756
发表对不同奶酪的看法

145
00:04:50,176 --> 0:04:51,496
尽管这个 App 

146
00:04:51,496 --> 0:04:53,156
是关于奶酪的

147
00:04:53,156 --> 0:04:56,276
但它并不油腻 它处理的是精细的奶酪

148
00:04:56,656 --> 0:04:57,846
我将要为大家展示的是

149
00:04:57,846 --> 0:05:00,646
一个用户在写评价

150
00:04:57,846 --> 0:05:00,646
一个用户在写评价

151
00:05:01,066 --> 0:05:03,006
在他写评价时

152
00:05:03,006 --> 0:05:05,036
文本传送到

153
00:05:05,036 --> 0:05:06,286
情感分类 API

154
00:05:06,286 --> 0:05:09,246
我们得到一个分值

155
00:05:09,316 --> 0:05:11,416
根据情感分值给文本上色

156
00:05:11,726 --> 0:05:13,646
我们看 如果你键入类似

157
00:05:13,646 --> 0:05:15,496
很好 非常美味的评语

158
00:05:16,336 --> 0:05:19,016
你可以看到

159
00:05:19,016 --> 0:05:20,386
这是一个积极的情感

160
00:05:21,796 --> 0:05:24,886
相反 如果你键入

161
00:05:24,886 --> 0:05:28,306
入口还不错

162
00:05:28,306 --> 0:05:30,296
但是后味很糟糕

163
00:05:30,296 --> 0:05:32,006
可以看到这是个消极情感

164
00:05:32,276 --> 0:05:33,936
你可以看到

165
00:05:33,936 --> 0:05:36,386
这些都是实时发生的

166
00:05:36,746 --> 0:05:38,046
这是因为 API 

167
00:05:38,046 --> 0:05:39,156
性能非常出色

168
00:05:39,556 --> 0:05:41,126
它实际使用神经网络模型

169
00:05:41,126 --> 0:05:43,206
所有 Apple 平台上

170
00:05:43,206 --> 0:05:44,516
都激活了硬件

171
00:05:44,516 --> 0:05:46,336
所以基本上

172
00:05:46,336 --> 0:05:47,616
你可以实时做这些

173
00:05:48,756 --> 0:05:49,846
我们支持情感分析 API

174
00:05:49,846 --> 0:05:51,936
用于七种不同的语言

175
00:05:51,936 --> 0:05:53,966
英语 法语

176
00:05:54,016 --> 0:05:55,596
意大利语 德语 西班牙语

177
00:05:55,636 --> 0:05:57,646
葡萄牙语和简体中文

178
00:05:58,176 --> 0:05:59,436
我觉得你们肯定会喜欢这个

179
00:06:00,516 --> 0:06:06,546
[掌声]

180
00:06:07,046 --> 0:06:08,126
当然 所有的这些

181
00:06:08,126 --> 0:06:09,656
都全部发生在设备上

182
00:06:09,656 --> 0:06:11,536
用户数据不需要离开设备

183
00:06:11,896 --> 0:06:13,786
设备本身就能提供这项强大的功能

184
00:06:14,756 --> 0:06:16,306
我想简短地讨论一下

185
00:06:16,306 --> 0:06:17,926
语言素材

186
00:06:18,296 --> 0:06:19,746
我刚才已经提及 

187
00:06:19,746 --> 0:06:21,246
NLP 功能是多种多样的

188
00:06:21,896 --> 0:06:23,476
支持多种

189
00:06:23,476 --> 0:06:24,646
不同的语言

190
00:06:25,176 --> 0:06:26,186
现在 对于用户

191
00:06:26,186 --> 0:06:28,006
我们确保他们能够拥有

192
00:06:28,006 --> 0:06:29,986
自己感兴趣的

193
00:06:29,986 --> 0:06:31,286
语言的素材

194
00:06:31,856 --> 0:06:33,316
但是对于大家来说 

195
00:06:33,316 --> 0:06:34,686
出于开发的目的 

196
00:06:34,686 --> 0:06:36,846
可能对按需素材更感兴趣

197
00:06:36,846 --> 0:06:39,286
实际上 这是大家的一个普遍请求

198
00:06:39,286 --> 0:06:40,716
所以我们要介绍一个

199
00:06:40,716 --> 0:06:42,746
新的便捷 API

200
00:06:42,746 --> 0:06:44,236
叫做请求素材

201
00:06:44,686 --> 0:06:46,076
你可以按照自己的需求

202
00:06:46,076 --> 0:06:49,216
触发一个特定素材的下载

203
00:06:49,436 --> 0:06:50,896
你只需指定

204
00:06:50,896 --> 0:06:51,836
自己喜欢的

205
00:06:51,836 --> 0:06:53,306
语言和标签方案

206
00:06:53,306 --> 0:06:55,186
我们会在后台复刻一个下载

207
00:06:55,276 --> 0:06:56,476
然后你就可以

208
00:06:56,476 --> 0:06:57,736
及时在设备上获取素材

209
00:06:57,736 --> 0:07:00,226
这会帮助你开发 App 

210
00:06:57,736 --> 0:07:00,226
这会帮助你开发 App 

211
00:07:00,226 --> 0:07:01,866
并提高你

212
00:07:01,866 --> 0:07:03,476
搭建 App 的效率

213
00:07:05,156 --> 0:07:07,226
我刚才介绍的是文本分类

214
00:07:07,226 --> 0:07:09,856
现在我们进入单词标记部分

215
00:07:10,956 --> 0:07:12,486
回顾一下

216
00:07:12,486 --> 0:07:14,616
单词标记是一个任务

217
00:07:14,616 --> 0:07:16,116
给定一系列 token  

218
00:07:16,116 --> 0:07:17,576
我们想要给序列中每一个 token

219
00:07:17,576 --> 0:07:18,896
分配标签

220
00:07:19,086 --> 0:07:21,776
就像这个例子

221
00:07:21,856 --> 0:07:23,056
我们可以给许多 token

222
00:07:23,056 --> 0:07:24,246
分配不同的标签

223
00:07:24,376 --> 0:07:25,546
Timothy 是一个人名

224
00:07:25,546 --> 0:07:27,326
瑞士是一个地方

225
00:07:27,326 --> 0:07:29,366
这句话里还有很多名词

226
00:07:30,466 --> 0:07:31,226
很好

227
00:07:31,226 --> 0:07:32,736
如果你只是想用我们的 API

228
00:07:32,736 --> 0:07:36,346
做命名实体识别

229
00:07:36,346 --> 0:07:39,616
或用 API 做词性标注也可以  

230
00:07:39,616 --> 0:07:43,096
但这里还有几个例子 你可以做更适合你任务的东西

231
00:07:43,826 --> 0:07:46,456
你想知道的不仅是格吕耶尔 干酪 是两个名词

232
00:07:46,816 --> 0:07:48,296
你还想知道

233
00:07:48,296 --> 0:07:50,426
它是种瑞士奶酪 我们在搭建一个奶酪 App

234
00:07:50,426 --> 0:07:53,136
你当然想得到这个信息

235
00:07:53,856 --> 0:07:55,626
但是 默认 tagger 

236
00:07:55,626 --> 0:07:56,696
不包含任何关于奶酪的信息

237
00:07:56,756 --> 0:07:57,966
那我们要如何

238
00:07:57,966 --> 0:07:58,846
提供这个信息呢

239
00:07:59,446 --> 0:08:00,926
所以自然语言框架里

240
00:07:59,446 --> 0:08:00,926
所以自然语言框架里

241
00:08:01,336 --> 0:08:02,526
有一个新功能

242
00:08:02,526 --> 0:08:04,366
我们称之为文本目录

243
00:08:05,646 --> 0:08:07,326
文本目录非常简单

244
00:08:07,866 --> 0:08:09,446
你只需要提供

245
00:08:09,446 --> 0:08:11,446
一个自定义列表

246
00:08:11,446 --> 0:08:13,036
可能是一个非常大的实体列表

247
00:08:13,276 --> 0:08:14,546
列表中的每一个实体

248
00:08:14,546 --> 0:08:15,666
都有一个标签

249
00:08:16,556 --> 0:08:18,476
实际情况下

250
00:08:18,476 --> 0:08:20,026
这些列表可能是数百万

251
00:08:20,026 --> 0:08:21,046
甚至是几亿

252
00:08:21,856 --> 0:08:23,416
你需要做的是

253
00:08:23,676 --> 0:08:25,646
把这种词库传送到 Create ML

254
00:08:25,646 --> 0:08:27,596
创建一个 MLGazetteer 实例

255
00:08:27,596 --> 0:08:29,316
Gazetteer 只是

256
00:08:29,316 --> 0:08:30,746
文本目录的术语

257
00:08:31,026 --> 0:08:32,196
两个名称可以混用

258
00:08:32,196 --> 0:08:34,015
你得到的输出就是

259
00:08:34,015 --> 0:08:35,436
文本目录

260
00:08:35,785 --> 0:08:37,145
这是输入词库的

261
00:08:37,216 --> 0:08:40,456
非常精简和有效的形式

262
00:08:41,015 --> 0:08:43,736
非常简单

263
00:08:43,856 --> 0:08:45,486
你要做的就是先提供这个词库

264
00:08:45,486 --> 0:08:46,956
我们不能在这里

265
00:08:46,956 --> 0:08:48,066
展现数百万的实例

266
00:08:48,066 --> 0:08:49,166
我们只能用几个实例作例子

267
00:08:49,166 --> 0:08:50,546
但是它可以是

268
00:08:50,546 --> 0:08:52,086
一个非常非常大的词库

269
00:08:53,406 --> 0:08:54,916
然后你就可以

270
00:08:54,916 --> 0:08:56,946
创建 MLGazetteer 实例

271
00:08:56,946 --> 0:08:59,326
传送词库 将它写到磁盘

272
00:08:59,776 --> 0:09:01,026
这一切看起来都没有什么危险 

273
00:08:59,776 --> 0:09:01,026
这一切看起来都没有什么危险 

274
00:09:01,026 --> 0:09:02,386
你可能在想

275
00:09:02,386 --> 0:09:03,386
我只是把词库写到磁盘

276
00:09:03,386 --> 0:09:04,236
这是在做什么

277
00:09:05,176 --> 0:09:06,866
如果你的调用正确

278
00:09:06,866 --> 0:09:07,896
会发生一些神奇的事情

279
00:09:08,516 --> 0:09:10,536
Create ML 在内部调用自然语言

280
00:09:10,596 --> 0:09:12,246
自然语言会把这个

281
00:09:12,246 --> 0:09:13,916
非常大的词库

282
00:09:13,956 --> 0:09:15,566
压缩到一个 

283
00:09:15,706 --> 0:09:17,306
Bloom 过滤器

284
00:09:17,306 --> 0:09:18,286
这是一种非常紧凑的形式

285
00:09:18,286 --> 0:09:19,946
那么你得到的输出

286
00:09:19,946 --> 0:09:21,586
就是文本目录

287
00:09:22,426 --> 0:09:24,046
实际上 我们已经用了这个方法

288
00:09:24,046 --> 0:09:25,146
并达到了效果

289
00:09:25,146 --> 0:09:27,466
我们已经压缩了

290
00:09:27,876 --> 0:09:29,166
维基百科几乎所有的

291
00:09:29,166 --> 0:09:30,986
人名 机构名称 位置 

292
00:09:30,986 --> 0:09:32,536
差不多有 250 万个

293
00:09:32,536 --> 0:09:33,916
压缩到

294
00:09:34,306 --> 0:09:35,726
磁盘上的 2 兆

295
00:09:36,136 --> 0:09:40,136
在某种程度上 你一直在使用这个模型

296
00:09:40,356 --> 0:09:41,466
当你在 NaturalLanguage 里

297
00:09:41,466 --> 0:09:42,476
结合统计模型

298
00:09:42,476 --> 0:09:44,306
调用命名实例识别 API 时

299
00:09:44,306 --> 0:09:47,146
你就在用这个 Bloom 过滤器

300
00:09:47,146 --> 0:09:48,636
和 Gazetteer

301
00:09:48,636 --> 0:09:51,816
现在我们把这个能力给了你

302
00:09:51,896 --> 0:09:53,906
一旦你创建了 Gazetteer 

303
00:09:53,906 --> 0:09:57,276
或文本目录 用起来就会特别简单

304
00:09:58,116 --> 0:09:59,666
通过指定文本目录的路径 

305
00:09:59,696 --> 0:10:01,376
刚才写到磁盘上的文本目录

306
00:09:59,696 --> 0:10:01,376
刚才写到磁盘上的文本目录

307
00:10:01,376 --> 0:10:03,576
你创建了一个 

308
00:10:03,576 --> 0:10:06,616
MLGazetteer 实例

309
00:10:06,616 --> 0:10:08,706
你可以在这里用自己喜欢的标签方案

310
00:10:08,706 --> 0:10:09,826
可以是词汇类 名称类型

311
00:10:09,826 --> 0:10:11,246
任何标签方案都可以  

312
00:10:11,246 --> 0:10:12,836
只需要把 Gazetteer

313
00:10:12,836 --> 0:10:14,756
附加到标签方案

314
00:10:15,636 --> 0:10:17,696
然后 每当有一个文本  

315
00:10:17,696 --> 0:10:19,456
这个自定义的 Gazetteer

316
00:10:19,456 --> 0:10:23,916
会覆盖 NaturalLanguage 提供的默认标签

317
00:10:25,246 --> 0:10:27,526
这样 你就可以自定义你的 App

318
00:10:28,646 --> 0:10:29,456
现在 返回奶酪 App

319
00:10:29,486 --> 0:10:30,876
如果有一句话

320
00:10:30,876 --> 0:10:32,626
比如 比卡芒贝尔奶酪

321
00:10:32,626 --> 0:10:34,196
或牛乳奶酪更淡

322
00:10:34,196 --> 0:10:36,796
你可以使用奶酪的文本目录

323
00:10:36,846 --> 0:10:39,956
识别一个是法国奶酪

324
00:10:39,956 --> 0:10:41,116
另一个是瑞士奶酪

325
00:10:41,116 --> 0:10:42,596
你也许可以创建一个超链接

326
00:10:42,596 --> 0:10:44,076
通过这种方式 

327
00:10:44,076 --> 0:10:46,116
制作一个更好的 App

328
00:10:46,586 --> 0:10:49,306
这是一种在 NaturalLanguage 里

329
00:10:49,306 --> 0:10:51,686
使用文字目录

330
00:10:51,686 --> 0:10:53,066
标注单词的方法

331
00:10:53,626 --> 0:10:57,246
我刚才介绍了文本分类

332
00:10:57,246 --> 0:10:58,676
介绍了单词标注

333
00:10:59,356 --> 0:11:01,086
但是近几年 NLP 领域

334
00:10:59,356 --> 0:11:01,086
但是近几年 NLP 领域

335
00:11:01,086 --> 0:11:02,846
发生了巨大的变化

336
00:11:02,846 --> 0:11:04,676
有两个催化剂

337
00:11:04,676 --> 0:11:06,586
促成了这一变化 

338
00:11:07,436 --> 0:11:08,916
一是单词嵌入概念

339
00:11:08,916 --> 0:11:12,006
单词嵌入只是单词的

340
00:11:12,006 --> 0:11:13,206
向量表示

341
00:11:13,206 --> 0:11:15,876
另一个是 NLP 里

342
00:11:15,876 --> 0:11:17,986
神经网络的使用

343
00:11:19,056 --> 0:11:21,056
我很高兴地宣布

344
00:11:21,386 --> 0:11:22,576
今年你可以

345
00:11:22,576 --> 0:11:24,056
通过 NaturalLanguage

346
00:11:24,056 --> 0:11:25,366
使用这些功能搭建 App

347
00:11:25,946 --> 0:11:30,166
我们先从单词嵌入开始 谢谢

348
00:11:31,596 --> 0:11:33,606
在我们进入

349
00:11:33,606 --> 0:11:34,996
单词嵌入部分之前

350
00:11:34,996 --> 0:11:36,426
我想通过几张幻灯片

351
00:11:36,426 --> 0:11:37,726
解释一下什么是嵌入

352
00:11:37,726 --> 0:11:40,336
理论层面上 

353
00:11:40,336 --> 0:11:43,946
嵌入只不过是把离散对象集合

354
00:11:43,946 --> 0:11:45,376
映射到持续向量表示

355
00:11:46,046 --> 0:11:48,026
我们有这些离散对象

356
00:11:48,466 --> 0:11:50,356
这个集合里的每个对象

357
00:11:50,356 --> 0:11:52,536
可以用有限向量表示

358
00:11:52,536 --> 0:11:54,176
在这个例子中 我们是用 

359
00:11:54,176 --> 0:11:55,466
3D 向量展示的

360
00:11:56,146 --> 0:11:57,096
用 3D 是因为  

361
00:11:57,146 --> 0:11:59,236
设计和看起来比较容易

362
00:11:59,236 --> 0:12:01,176
但是实际上 这些向量

363
00:11:59,236 --> 0:12:01,176
但是实际上 这些向量

364
00:12:01,176 --> 0:12:02,476
可以是任意维度 

365
00:12:02,786 --> 0:12:04,556
可以是 100D 300D

366
00:12:04,556 --> 0:12:05,966
甚至在某些情况下

367
00:12:06,266 --> 0:12:07,656
是 1000D 向量

368
00:12:08,756 --> 0:12:10,146
这些嵌入的特性是非常条理

369
00:12:10,146 --> 0:12:11,926
当你要设计

370
00:12:11,926 --> 0:12:13,326
这些嵌入时

371
00:12:14,176 --> 0:12:15,456
语义相似的对象

372
00:12:15,556 --> 0:12:17,316
会聚在一起

373
00:12:18,446 --> 0:12:19,976
在这个例子中

374
00:12:19,976 --> 0:12:21,306
油漆罐和油漆刷

375
00:12:21,306 --> 0:12:23,246
聚在了一起

376
00:12:24,376 --> 0:12:28,276
运动鞋 和高跟鞋聚在了一起

377
00:12:28,716 --> 0:12:30,896
所以 嵌入的特性非常条理

378
00:12:31,096 --> 0:12:33,086
嵌入的这个特性

379
00:12:33,086 --> 0:12:34,806
不仅可以用于单词

380
00:12:34,806 --> 0:12:36,046
实际上还可用于

381
00:12:36,046 --> 0:12:37,126
不同的形式

382
00:12:37,646 --> 0:12:39,066
可以是图片嵌入

383
00:12:39,166 --> 0:12:40,826
当你有一张图片 

384
00:12:40,826 --> 0:12:42,876
通过 VGG 网络

385
00:12:42,876 --> 0:12:43,946
或者任何卷积神经网络传输时

386
00:12:43,946 --> 0:12:45,456
你得到的输出

387
00:12:45,456 --> 0:12:48,046
就是这个特性图像嵌入

388
00:12:48,986 --> 0:12:51,696
同样 你可以嵌入单词 短语 

389
00:12:51,696 --> 0:12:54,016
当你在做推荐系统时

390
00:12:54,016 --> 0:12:55,676
里面有歌曲名 

391
00:12:55,676 --> 0:12:57,136
或者产品名

392
00:12:57,136 --> 0:13:00,076
他们都是通过向量表示的

393
00:12:57,136 --> 0:13:00,076
他们都是通过向量表示的

394
00:13:00,526 --> 0:13:02,066
所以 它们只是嵌入

395
00:13:02,926 --> 0:13:04,326
总的来说

396
00:13:04,326 --> 0:13:06,506
嵌入只是将字符串  

397
00:13:06,506 --> 0:13:08,376
映射到持续的数字序列

398
00:13:08,376 --> 0:13:10,556
或者数字向量

399
00:13:11,076 --> 0:13:15,026
我们已经在 iOS 12 里

400
00:13:15,066 --> 0:13:16,836
非常成功地使用了这些嵌入

401
00:13:16,836 --> 0:13:18,656
我将向大家介绍

402
00:13:18,656 --> 0:13:19,836
如何在照片中使用嵌入

403
00:13:21,226 --> 0:13:23,236
在照片搜索中

404
00:13:23,316 --> 0:13:24,426
当你输入一个想查找的词时

405
00:13:24,426 --> 0:13:26,366
比如说 雷雨的照片

406
00:13:26,416 --> 0:13:28,506
在屏幕下面

407
00:13:28,506 --> 0:13:29,936
照片库里的所有照片

408
00:13:29,936 --> 0:13:33,626
都通过卷积神经网络编了索引

409
00:13:33,626 --> 0:13:35,106
卷积神经网络的输出

410
00:13:35,106 --> 0:13:36,466
固定在一定数量的类上

411
00:13:36,466 --> 0:13:37,696
可能是 1000 个类

412
00:13:37,696 --> 0:13:40,556
也可能是 2000 个类

413
00:13:41,426 --> 0:13:42,406
如果你的卷积神经网络  

414
00:13:42,406 --> 0:13:43,666
不知道雷雨是什么 

415
00:13:43,666 --> 0:13:45,506
那么你永远无法找到

416
00:13:45,506 --> 0:13:46,716
雷雨的索引照片

417
00:13:46,716 --> 0:13:48,476
因为不知道雷雨这个单词

418
00:13:48,476 --> 0:13:50,396
但是因为有单词嵌入

419
00:13:50,396 --> 0:13:55,076
我们知道雷雨和天空多云相关

420
00:13:55,076 --> 0:13:56,986
这些标签

421
00:13:57,876 --> 0:13:59,996
你的卷积神经网络是明白的

422
00:14:00,386 --> 0:14:02,266
所以 在 iOS 12 中

423
00:14:02,266 --> 0:14:04,366
你可以使用单词嵌入

424
00:14:04,366 --> 0:14:07,076
在照片搜索中实现模糊搜索

425
00:14:07,706 --> 0:14:09,396
所以 通过单词嵌入  

426
00:14:09,666 --> 0:14:11,676
你可以找到

427
00:14:11,676 --> 0:14:12,876
自己想找的照片

428
00:14:12,876 --> 0:14:14,006
实际上 它可以应用到 

429
00:14:14,006 --> 0:14:15,216
所有的搜索 App

430
00:14:15,466 --> 0:14:16,716
如果你有一串字符 

431
00:14:16,716 --> 0:14:18,226
你想模糊搜索

432
00:14:18,226 --> 0:14:19,426
你可以关联原单词

433
00:14:19,426 --> 0:14:21,946
和与其相近的单词

434
00:14:22,656 --> 0:14:24,826
说到这儿 你可以用嵌入

435
00:14:24,826 --> 0:14:25,846
做些什么呢

436
00:14:26,676 --> 0:14:27,696
你可以用单词嵌入

437
00:14:27,696 --> 0:14:29,716
进行 4 项基本操作

438
00:14:30,556 --> 0:14:32,486
一是 你可以得到

439
00:14:32,716 --> 0:14:34,466
一个单词的向量

440
00:14:35,576 --> 0:14:37,486
二是 如果有两个单词

441
00:14:37,486 --> 0:14:38,716
你可以找到两个词的距离

442
00:14:39,116 --> 0:14:40,356
因为你可以查看 

443
00:14:40,356 --> 0:14:41,156
每个单词的

444
00:14:41,156 --> 0:14:42,356
对应向量

445
00:14:42,616 --> 0:14:44,446
比如说 猫和狗

446
00:14:44,446 --> 0:14:45,576
如果我想得到这两个词

447
00:14:45,576 --> 0:14:46,816
之间的距离

448
00:14:46,816 --> 0:14:47,866
那么这个距离

449
00:14:47,866 --> 0:14:48,996
应该挺近的

450
00:14:49,886 --> 0:14:51,976
但如果是 狗和靴子

451
00:14:51,976 --> 0:14:53,706
那么在语义场内 

452
00:14:53,706 --> 0:14:55,246
它们距离应该挺远

453
00:14:55,246 --> 0:14:57,366
那么你会得到更远一些的距离

454
00:14:58,546 --> 0:14:59,696
三是 你可以得到 

455
00:14:59,696 --> 0:15:01,086
和某个单词最相近的词

456
00:14:59,696 --> 0:15:01,086
和某个单词最相近的词

457
00:15:01,086 --> 0:15:02,916
这应该是到目前为止

458
00:15:02,916 --> 0:15:04,676
对于单词嵌入 

459
00:15:04,676 --> 0:15:06,106
最流行的用法

460
00:15:06,106 --> 0:15:07,516
我刚才展示的照片搜索 App

461
00:15:07,516 --> 0:15:10,236
正是在做这个

462
00:15:10,236 --> 0:15:12,936
寻找和某个单词最相近的单词

463
00:15:12,936 --> 0:15:15,836
最后一点是

464
00:15:15,836 --> 0:15:19,376
你可以得到一个向量的最近邻居

465
00:15:19,376 --> 0:15:20,776
假设你有一个句子

466
00:15:20,836 --> 0:15:22,116
句子里有好几个单词

467
00:15:22,116 --> 0:15:23,726
句子里每一个单词

468
00:15:23,726 --> 0:15:25,176
都可以得到一个单词嵌入

469
00:15:25,176 --> 0:15:26,276
你可以把它们

470
00:15:26,326 --> 0:15:27,516
总结起来

471
00:15:27,516 --> 0:15:28,886
这样你会得到一个新的向量

472
00:15:28,886 --> 0:15:31,036
有了这个向量

473
00:15:31,036 --> 0:15:32,466
你就可以得到

474
00:15:32,466 --> 0:15:33,266
所有和这个向量相近的单词

475
00:15:33,266 --> 0:15:35,576
这也是一种应用单词嵌入的方法

476
00:15:35,576 --> 0:15:38,186
单词嵌入内容很多 

477
00:15:38,186 --> 0:15:39,396
但是最重要的是   

478
00:15:39,396 --> 0:15:40,476
我们为你提供了这个功能

479
00:15:40,476 --> 0:15:42,436
你可以在 OS 上

480
00:15:42,436 --> 0:15:43,786
便捷地使用它

481
00:15:44,076 --> 0:15:45,326
很高兴告诉大家

482
00:15:45,456 --> 0:15:46,906
这些单词嵌入

483
00:15:46,906 --> 0:15:48,396
支持 7 种语言

484
00:15:48,396 --> 0:15:51,426
我刚才提到的所有功能

485
00:15:51,456 --> 0:15:53,366
都只需要一两行代码

486
00:15:53,496 --> 0:15:54,946
就可以实现

487
00:15:55,386 --> 0:15:56,686
单词嵌入支持 7 种语言

488
00:15:56,686 --> 0:15:58,406
英语 西班牙语

489
00:15:58,406 --> 0:15:59,686
法语 意大利语 德语

490
00:15:59,766 --> 0:16:01,706
葡萄牙语和简体中文

491
00:15:59,766 --> 0:16:01,706
葡萄牙语和简体中文

492
00:16:02,986 --> 0:16:04,146
非常棒

493
00:16:04,146 --> 0:16:05,546
OS 嵌入通常是在

494
00:16:05,626 --> 0:16:07,536
通用语料库上构建的

495
00:16:07,536 --> 0:16:09,866
文本数量庞大 有数亿单词

496
00:16:10,406 --> 0:16:11,936
所以他们对于

497
00:16:12,056 --> 0:16:14,126
某个词和其他词的关系

498
00:16:14,126 --> 0:16:15,156
有一个大致概念

499
00:16:15,876 --> 0:16:17,226
但是很多时候

500
00:16:17,226 --> 0:16:18,216
你想做一些更自定义的事

501
00:16:19,846 --> 0:16:21,016
也许你活跃在

502
00:16:21,216 --> 0:16:22,366
不同的领域

503
00:16:22,926 --> 0:16:25,566
医药领域 法律领域

504
00:16:25,566 --> 0:16:26,316
或金融领域

505
00:16:27,046 --> 0:16:28,286
如果你的领域各不相同

506
00:16:28,286 --> 0:16:30,316
那么你想在 App 中

507
00:16:30,316 --> 0:16:31,916
使用的词汇

508
00:16:31,916 --> 0:16:33,076
会非常不同

509
00:16:33,076 --> 0:16:34,806
也许你只是想  

510
00:16:34,806 --> 0:16:36,076
让一个 OS 不支持的语言

511
00:16:36,076 --> 0:16:37,426
实现单词嵌入

512
00:16:37,426 --> 0:16:40,886
你应该怎么做呢

513
00:16:40,886 --> 0:16:42,466
我们对此也有准备 

514
00:16:43,366 --> 0:16:45,786
你可以使用自定义单词嵌入

515
00:16:46,506 --> 0:16:47,246
对于熟悉单词嵌入

516
00:16:47,246 --> 0:16:50,166
见证这个领域发展的人

517
00:16:50,166 --> 0:16:51,856
有许多第三方工具

518
00:16:51,856 --> 0:16:53,576
可以训练自定义嵌入

519
00:16:53,576 --> 0:16:55,866
比如 word2vec GloVe fasttext

520
00:16:56,366 --> 0:16:57,906
所以 你可以用自己的文本

521
00:16:58,306 --> 0:16:59,666
也可以用自己在 

522
00:16:59,666 --> 0:17:01,726
Keras TensorFlow 或 PyTorch

523
00:16:59,666 --> 0:17:01,726
Keras TensorFlow 或 PyTorch

524
00:17:01,726 --> 0:17:01,793
训练的自定义神经网络

525
00:17:01,793 --> 0:17:04,756
你可以通过原始文本

526
00:17:04,756 --> 0:17:06,756
创建自己的嵌入

527
00:17:06,756 --> 0:17:07,935
你也可以从任一网站

528
00:17:07,935 --> 0:17:09,256
下载它们训练好的

529
00:17:09,256 --> 0:17:10,526
单词嵌入

530
00:17:11,205 --> 0:17:14,185
问题是 你要下载的这些嵌入

531
00:17:14,185 --> 0:17:15,866
体积非常非常大

532
00:17:16,256 --> 0:17:18,306
1GB 或 2GB 那么大

533
00:17:18,695 --> 0:17:19,826
但是你想非常精简有效地

534
00:17:19,826 --> 0:17:20,896
在你的 App 中使用它们

535
00:17:20,896 --> 0:17:23,266
我们实现了这一点

536
00:17:23,266 --> 0:17:26,915
这些来自第三方 App 的嵌入

537
00:17:26,915 --> 0:17:28,406
体积非常大

538
00:17:28,406 --> 0:17:30,076
我们自动将它们

539
00:17:30,076 --> 0:17:32,296
压缩成非常紧凑的格式

540
00:17:32,296 --> 0:17:33,576
有了这个格式

541
00:17:33,746 --> 0:17:35,446
你就可以像用 OS 嵌入一样

542
00:17:35,446 --> 0:17:36,716
使用它们了

543
00:17:37,436 --> 0:17:38,666
接下来将由 Doug    

544
00:17:38,666 --> 0:17:40,206
为大家介绍

545
00:17:40,206 --> 0:17:41,746
如何使用 OS 嵌入

546
00:17:41,746 --> 0:17:42,836
和自定义嵌入

547
00:17:42,936 --> 0:17:45,666
他会给大家做个示范

548
00:17:45,666 --> 0:17:46,786
后半段的讲演交给他了

549
00:17:47,426 --> 0:17:48,066
轮到你了 Doug

550
00:17:49,516 --> 0:17:55,836
[掌声]

551
00:17:56,336 --> 0:17:57,036
&gt;&gt; 好的

552
00:17:57,036 --> 0:17:58,196
我将通过一个演示 App

553
00:17:58,196 --> 0:18:00,066
向大家展示运行起来 

554
00:17:58,196 --> 0:18:00,066
向大家展示运行起来 

555
00:18:00,066 --> 0:18:00,976
是什么样的

556
00:18:01,656 --> 0:18:02,986
首先 我写了个

557
00:18:02,986 --> 0:18:04,796
非常小的演示 App 

558
00:18:04,796 --> 0:18:06,496
帮助大家理解

559
00:18:06,496 --> 0:18:08,566
单词嵌入

560
00:18:08,606 --> 0:18:10,456
在这儿键入一个单词

561
00:18:10,456 --> 0:18:11,636
它会显示最近的邻居

562
00:18:11,636 --> 0:18:13,676
嵌入空间内 

563
00:18:13,676 --> 0:18:15,296
离那个单词最近的邻居

564
00:18:15,636 --> 0:18:16,826
我们先从英语开始

565
00:18:16,826 --> 0:18:19,456
使用内置 OS 单词嵌入 

566
00:18:19,806 --> 0:18:21,466
我键入一个单词 比如椅子

567
00:18:21,666 --> 0:18:23,076
可以看到  

568
00:18:23,076 --> 0:18:24,476
它的邻居都是

569
00:18:24,476 --> 0:18:25,886
意思和它相近的单词

570
00:18:25,886 --> 0:18:28,246
椅子 沙发 长榻 等等

571
00:18:28,246 --> 0:18:31,116
我也可以键入自行车

572
00:18:31,796 --> 0:18:33,466
最近的邻居是

573
00:18:33,466 --> 0:18:34,666
自行车 摩托车 等等

574
00:18:34,666 --> 0:18:36,166
这些单词意思

575
00:18:36,166 --> 0:18:39,596
和自行车相近

576
00:18:40,406 --> 0:18:42,576
我也可以键入 书 就能得到和书意思相近的词

577
00:18:43,026 --> 0:18:44,616
从这里 我们可以看出

578
00:18:44,936 --> 0:18:46,746
内置 OS 单词嵌入

579
00:18:46,746 --> 0:18:48,316
显示原本的  

580
00:18:48,716 --> 0:18:50,826
词义和语言

581
00:18:50,826 --> 0:18:54,736
并识别出该语言在

582
00:18:54,776 --> 0:18:59,206
通用文本里相近的意思

583
00:19:00,206 --> 0:19:02,596
当然 

584
00:19:02,596 --> 0:19:04,686
这里我最感兴趣的是

585
00:19:05,066 --> 0:19:06,216
这些嵌入 

586
00:19:06,216 --> 0:19:08,916
和奶酪有什么关系

587
00:19:08,916 --> 0:19:09,976
毕竟我们做的是

588
00:19:09,976 --> 0:19:10,726
奶酪 App

589
00:19:11,426 --> 0:19:13,246
所以 我们键入一个 奶酪 单词

590
00:19:14,556 --> 0:19:16,156
看一下这里

591
00:19:16,156 --> 0:19:18,996
你立刻就能看到

592
00:19:18,996 --> 0:19:21,446
内置嵌入知道奶酪是什么

593
00:19:21,446 --> 0:19:24,506
但是我很失望

594
00:19:24,966 --> 0:19:26,196
这些嵌入只知道奶酪 

595
00:19:26,196 --> 0:19:28,206
但不了解任何细节

596
00:19:29,406 --> 0:19:31,086
否则 他们就不会把这些特殊的奶酪

597
00:19:31,086 --> 0:19:32,796
和奶酪相关的东西

598
00:19:32,796 --> 0:19:34,086
放在一起

599
00:19:34,086 --> 0:19:35,436
它们不应该放在一起

600
00:19:36,236 --> 0:19:37,706
我想要的效果是 

601
00:19:37,706 --> 0:19:39,136
它能明白 

602
00:19:39,136 --> 0:19:40,486
奶酪之间的关系

603
00:19:40,776 --> 0:19:42,046
所以 我就趁机训练  

604
00:19:42,046 --> 0:19:44,316
我自己的自定义奶酪嵌入

605
00:19:44,316 --> 0:19:46,446
它会基于相似性

606
00:19:46,446 --> 0:19:48,016
把奶酪放在一起

607
00:19:48,936 --> 0:19:50,116
切换到自定义嵌入

608
00:19:51,436 --> 0:19:52,616
这些是自定义奶酪嵌入里的 

609
00:19:52,616 --> 0:19:54,716
切德干酪的邻居

610
00:19:54,966 --> 0:19:55,806
这样就好多了

611
00:19:56,586 --> 0:19:57,786
可以看到 它把 

612
00:19:57,786 --> 0:19:59,646
和切德干酪

613
00:20:00,216 --> 0:20:01,596
口感相似的奶酪

614
00:20:01,596 --> 0:20:03,446
放在了一起

615
00:20:03,446 --> 0:20:05,676
比如 兰开夏奶酪 格洛斯特硬干酪和柴郡白干酪

616
00:20:06,666 --> 0:20:07,826
这是我们可以用在

617
00:20:07,826 --> 0:20:09,356
App 里的东西

618
00:20:09,946 --> 0:20:10,896
现在 我们再来看一下

619
00:20:10,896 --> 0:20:12,886
奶酪 App 是什么样的

620
00:20:15,426 --> 0:20:16,476
我在这个奶酪 App 上 

621
00:20:16,476 --> 0:20:18,266
尝试了一些想法

622
00:20:18,266 --> 0:20:20,676
看一下现在它的样子

623
00:20:21,226 --> 0:20:22,276
当用户键入时

624
00:20:22,276 --> 0:20:23,406
首先我会得到 

625
00:20:23,406 --> 0:20:25,486
一个情感分值

626
00:20:25,486 --> 0:20:27,826
看一下这是否是一句

627
00:20:27,826 --> 0:20:30,066
积极情感的话

628
00:20:30,066 --> 0:20:31,446
如果是的话 

629
00:20:31,446 --> 0:20:34,146
我会用我的标注 

630
00:20:34,146 --> 0:20:35,436
检查一下这句话

631
00:20:35,436 --> 0:20:39,156
当然 也会用我们的

632
00:20:39,156 --> 0:20:41,776
自定义奶酪 Gazetteer

633
00:20:41,776 --> 0:20:44,656
查看这句话里有没有提到什么奶酪

634
00:20:45,356 --> 0:20:47,076
我在找有没有提及奶酪

635
00:20:47,126 --> 0:20:48,826
如果用户真的提及了

636
00:20:48,826 --> 0:20:50,736
我就会把这个名字  

637
00:20:50,736 --> 0:20:52,296
传送到自定义奶酪嵌入

638
00:20:52,296 --> 0:20:56,366
查找相关奶酪

639
00:20:57,426 --> 0:20:58,336
听起来挺好的吧

640
00:20:58,336 --> 0:21:01,386
我们试一下

641
00:20:58,336 --> 0:21:01,386
我们试一下

642
00:21:01,656 --> 0:21:02,726
调出我们的奶酪 App

643
00:21:02,726 --> 0:21:08,816
去年我去荷兰旅游

644
00:21:08,816 --> 0:21:10,136
然后我爱上了

645
00:21:10,136 --> 0:21:11,796
荷兰奶酪

646
00:21:11,796 --> 0:21:16,986
我将把这个告诉我的 App

647
00:21:16,986 --> 0:21:19,616
这肯定是一句

648
00:21:19,616 --> 0:21:20,686
情感积极的话

649
00:21:20,686 --> 0:21:22,716
而且确实提及了

650
00:21:22,716 --> 0:21:24,006
一种特定的奶酪

651
00:21:24,416 --> 0:21:27,586
然后我的 App 

652
00:21:27,876 --> 0:21:29,336
就可以为我推荐

653
00:21:29,336 --> 0:21:31,216
和我刚才提到的奶酪

654
00:21:31,216 --> 0:21:32,296
相似的奶酪

655
00:21:33,376 --> 0:21:35,186
这展示了单词嵌入的功能

656
00:21:35,226 --> 0:21:36,946
不仅如此

657
00:21:36,946 --> 0:21:39,166
它还展示了自然的 多变的

658
00:21:39,166 --> 0:21:40,626
NaturalLanguage API

659
00:21:40,626 --> 0:21:43,566
是如何和 App 功能 

660
00:21:43,566 --> 0:21:44,906
结合到一起的

661
00:21:46,516 --> 0:21:50,736
[掌声]

662
00:21:51,236 --> 0:21:52,656
现在我们退回幻灯片

663
00:21:52,656 --> 0:21:54,976
我想要简单的回顾一下

664
00:21:54,976 --> 0:21:57,496
在 API 中 这个是什么样子的

665
00:21:58,466 --> 0:22:00,546
如果你想要用

666
00:21:58,466 --> 0:22:00,546
如果你想要用

667
00:22:00,666 --> 0:22:02,926
内置的 OS 单词嵌入

668
00:22:02,926 --> 0:22:04,996
非常简单 你要做的就是请求

669
00:22:05,206 --> 0:22:06,696
请求某个特定语言的

670
00:22:06,696 --> 0:22:07,876
单词嵌入

671
00:22:07,876 --> 0:22:08,346
我们就会给你

672
00:22:08,346 --> 0:22:10,976
一旦你有了这些 

673
00:22:10,976 --> 0:22:12,766
NLEmbedding 对象的其中一个

674
00:22:12,766 --> 0:22:14,106
你就可以用它做很多事情

675
00:22:14,536 --> 0:22:16,886
当然 你可以得到组件 向量组件

676
00:22:16,886 --> 0:22:19,856
和任何特定目录相对应

677
00:22:21,136 --> 0:22:22,476
你可以得到在嵌入空间中

678
00:22:22,476 --> 0:22:24,326
两个单词之间的距离

679
00:22:24,326 --> 0:22:26,436
是近还是远

680
00:22:27,346 --> 0:22:28,936
正如在奶酪 App 里所见

681
00:22:28,936 --> 0:22:30,676
你可以浏览 

682
00:22:30,676 --> 0:22:32,166
找到在这个嵌入空间内

683
00:22:32,596 --> 0:22:35,296
任何特定名称

684
00:22:35,426 --> 0:22:37,186
最近的邻居

685
00:22:37,796 --> 0:22:40,436
如果你想用自定义单词嵌入

686
00:22:40,436 --> 0:22:43,346
要创建它

687
00:22:43,346 --> 0:22:46,496
你可以用 Create ML

688
00:22:46,546 --> 0:22:48,486
当然你需要

689
00:22:48,486 --> 0:22:50,846
所有表示嵌入的向量

690
00:22:51,186 --> 0:22:52,596
我不能在这里 通过幻灯片 

691
00:22:52,596 --> 0:22:53,986
向大家展示所有的向量

692
00:22:53,986 --> 0:22:55,536
因为他们有 50

693
00:22:55,856 --> 0:22:57,736
或者 100 个组件那么长

694
00:22:57,736 --> 0:22:59,906
但是这儿有一个例子

695
00:22:59,906 --> 0:23:01,146
在实际操作中

696
00:22:59,906 --> 0:23:01,146
在实际操作中

697
00:23:01,176 --> 0:23:02,396
你可能从一个文件中引入它们

698
00:23:02,396 --> 0:23:04,486
使用各种 Create ML 工具

699
00:23:04,486 --> 0:23:06,656
从文件中

700
00:23:06,656 --> 0:23:07,746
加载数据

701
00:23:08,076 --> 0:23:12,476
然后你就可以从中

702
00:23:12,476 --> 0:23:15,336
创建一个单词嵌入对象

703
00:23:15,336 --> 0:23:16,376
将它写到磁盘上

704
00:23:16,376 --> 0:23:18,376
这么做时 发生了什么呢

705
00:23:19,036 --> 0:23:22,256
在实际操作中

706
00:23:22,256 --> 0:23:23,796
这些嵌入一般会比较大

707
00:23:23,796 --> 0:23:25,936
数百维

708
00:23:25,936 --> 0:23:27,206
乘以数千个条目

709
00:23:27,206 --> 0:23:30,066
体积会很大

710
00:23:30,066 --> 0:23:32,206
会占用很多磁盘空间

711
00:23:32,776 --> 0:23:35,286
搜索就会比较贵

712
00:23:35,856 --> 0:23:38,336
当你把它们  

713
00:23:38,336 --> 0:23:41,416
编译进单词嵌入对象时

714
00:23:41,416 --> 0:23:43,836
在内部 我们做的是

715
00:23:43,836 --> 0:23:46,346
用产品量化技术

716
00:23:46,346 --> 0:23:47,906
实现很高程度的压缩

717
00:23:47,906 --> 0:23:50,856
然后添加索引

718
00:23:50,856 --> 0:23:53,336
这样你就可以快速搜索

719
00:23:53,506 --> 0:23:55,456
最近的邻居

720
00:23:55,456 --> 0:23:56,926
就像我们的示范一样

721
00:23:57,606 --> 0:24:00,926
来试一下 我们有一些

722
00:23:57,606 --> 0:24:00,926
来试一下 我们有一些

723
00:24:00,926 --> 0:24:04,256
开源的 非常大的嵌入

724
00:24:04,446 --> 0:24:06,916
这些是 GloVe 和 fasttext 嵌入

725
00:24:06,916 --> 0:24:08,876
未压缩的格式

726
00:24:08,876 --> 0:24:11,046
有 1GB 或 2GB 大

727
00:24:11,686 --> 0:24:13,956
当我们把它压缩成 

728
00:24:13,956 --> 0:24:15,606
NL 压缩格式时

729
00:24:16,206 --> 0:24:17,736
它们只有几十兆

730
00:24:18,136 --> 0:24:19,086
只需要几毫秒

731
00:24:19,086 --> 0:24:20,146
你就可以搜索到

732
00:24:20,146 --> 0:24:21,176
离它们最近的邻居

733
00:24:23,056 --> 0:24:23,976
有个 Apple 的例子

734
00:24:24,516 --> 0:24:27,716
[掌声]

735
00:24:28,216 --> 0:24:30,146
有个 Apple 的例子

736
00:24:30,266 --> 0:24:32,406
Apple 做了许多播客

737
00:24:32,536 --> 0:24:34,516
我们和播客团队

738
00:24:34,516 --> 0:24:36,616
交流了一下

739
00:24:36,616 --> 0:24:38,226
他们有一个

740
00:24:38,226 --> 0:24:40,426
为播客制作的嵌入

741
00:24:40,426 --> 0:24:43,686
表示各种播客之间的相似性

742
00:24:44,346 --> 0:24:46,196
所以我们想试一下

743
00:24:46,226 --> 0:24:48,186
看看把这个嵌入

744
00:24:48,186 --> 0:24:49,886
做成 NL 嵌入格式

745
00:24:49,886 --> 0:24:51,826
会发生什么

746
00:24:52,456 --> 0:24:54,156
这个嵌入代表

747
00:24:54,156 --> 0:24:57,466
66,000 个不同的播客

748
00:24:57,466 --> 0:24:59,486
原格式有 167MB

749
00:24:59,526 --> 0:25:00,906
但是我们压缩了它

750
00:24:59,526 --> 0:25:00,906
但是我们压缩了它

751
00:25:00,906 --> 0:25:02,976
现在只占 3MB

752
00:25:03,326 --> 0:25:05,716
NL 嵌入的功能就是

753
00:25:05,716 --> 0:25:08,246
添加这些嵌入

754
00:25:08,246 --> 0:25:10,376
在你的 App 里

755
00:25:11,036 --> 0:25:13,236
设备上使用它们

756
00:25:16,716 --> 0:25:19,716
好的 接下来我想换一个话题

757
00:25:19,716 --> 0:25:22,646
介绍另一个

758
00:25:22,646 --> 0:25:24,976
和单词嵌入相关的东西

759
00:25:24,976 --> 0:25:28,036
那就是文本分类的迁移学习

760
00:25:29,356 --> 0:25:31,886
我想先谈一下

761
00:25:31,886 --> 0:25:33,516
我们是如何

762
00:25:33,516 --> 0:25:37,936
训练文本分类的

763
00:25:39,176 --> 0:25:40,886
在我们训练文本分类时

764
00:25:40,886 --> 0:25:44,076
我们给他一系列

765
00:25:44,076 --> 0:25:47,096
各种类的例子

766
00:25:47,636 --> 0:25:50,826
把这些数据传送给 Create ML

767
00:25:50,826 --> 0:25:52,946
Create ML 会调用自然语言

768
00:25:53,506 --> 0:25:55,766
训练分类器

769
00:25:55,876 --> 0:25:58,136
生成一个 Core ML 模型

770
00:25:58,506 --> 0:26:00,686
我们希望

771
00:25:58,506 --> 0:26:00,686
我们希望

772
00:26:00,686 --> 0:26:04,256
这些例子会充分提供

773
00:26:04,576 --> 0:26:06,626
关于各类的信息

774
00:26:06,686 --> 0:26:08,546
这样模型就可以概括

775
00:26:08,546 --> 0:26:11,916
分类它没有

776
00:26:11,916 --> 0:26:12,926
见过的例子

777
00:26:13,406 --> 0:26:15,126
当然 去年

778
00:26:15,366 --> 0:26:17,786
这个功能已经上市了

779
00:26:17,786 --> 0:26:20,566
我们有训练这些模型的算法

780
00:26:20,566 --> 0:26:23,136
最出名的是

781
00:26:23,136 --> 0:26:24,846
我们的标准算法 

782
00:26:24,846 --> 0:26:26,926
我们称之为 maxEnt 算法

783
00:26:26,926 --> 0:26:27,966
它基于逻辑压缩

784
00:26:28,336 --> 0:26:30,816
快速 强大 有效

785
00:26:31,666 --> 0:26:35,126
但是有一个问题

786
00:26:35,126 --> 0:26:36,876
除了你给的

787
00:26:36,876 --> 0:26:39,076
训练资料外

788
00:26:39,076 --> 0:26:41,846
它不知道其他的东西

789
00:26:42,116 --> 0:26:45,566
这样你就要确保

790
00:26:45,566 --> 0:26:46,876
你给他的训练材料

791
00:26:46,876 --> 0:26:50,736
包括你希望 

792
00:26:50,736 --> 0:26:52,296
在实际操作中

793
00:26:52,296 --> 0:26:55,156
能看到的所有东西

794
00:26:55,386 --> 0:26:57,336
在某种意义上说 

795
00:26:57,336 --> 0:26:58,736
我们提供算法 比较简单

796
00:26:58,736 --> 0:27:00,136
比较困难的部分留给了你们

797
00:26:58,736 --> 0:27:00,136
比较困难的部分留给了你们

798
00:27:00,136 --> 0:27:03,096
那就是提供训练数据

799
00:27:03,716 --> 0:27:06,916
但是如果我们用

800
00:27:06,916 --> 0:27:09,216
已知的语言知识 

801
00:27:09,216 --> 0:27:11,386
结合我们提供的

802
00:27:12,316 --> 0:27:16,966
体积小一点的训练材料

803
00:27:16,966 --> 0:27:18,526
来训练模型

804
00:27:18,526 --> 0:27:21,366
这样不是很好吗？

805
00:27:21,366 --> 0:27:24,616
通过两者的结合

806
00:27:24,696 --> 0:27:26,886
它会理解更多的实例

807
00:27:26,886 --> 0:27:31,176
即便是训练材料

808
00:27:31,176 --> 0:27:33,046
没有那么多

809
00:27:34,426 --> 0:27:37,866
这就是迁移学习的目标

810
00:27:38,576 --> 0:27:41,036
这是 NLP 重点研究领域

811
00:27:41,036 --> 0:27:43,876
很高兴 我们已经

812
00:27:43,876 --> 0:27:45,786
找到了一个解决办法

813
00:27:46,136 --> 0:27:47,326
现在介绍给大家

814
00:27:48,286 --> 0:27:50,426
自然语言训练模型

815
00:27:50,426 --> 0:27:52,946
生成一个 Core ML 模型

816
00:27:53,326 --> 0:27:54,916
但是我们怎么结合

817
00:27:54,916 --> 0:27:57,696
已知的语言知识呢

818
00:27:58,026 --> 0:27:58,886
我们从哪儿得到这个知识呢

819
00:27:59,946 --> 0:28:06,376
单词嵌入提供了许多语言的知识

820
00:27:59,946 --> 0:28:06,376
单词嵌入提供了许多语言的知识

821
00:28:06,376 --> 0:28:07,656
尤其是 它们知道

822
00:28:07,656 --> 0:28:10,626
单词的许多意思

823
00:28:11,296 --> 0:28:13,996
我们的方法是

824
00:28:13,996 --> 0:28:16,026
有单词嵌入

825
00:28:16,026 --> 0:28:18,056
和你提供的训练材料

826
00:28:18,056 --> 0:28:19,466
把他们放入单词嵌入

827
00:28:19,466 --> 0:28:21,686
在此之上

828
00:28:21,686 --> 0:28:25,596
训练神经网络模型

829
00:28:26,096 --> 0:28:27,986
这样我们就得到了

830
00:28:27,986 --> 0:28:30,456
迁移学习文本分类模型

831
00:28:31,556 --> 0:28:32,686
看起来比较复杂

832
00:28:32,686 --> 0:28:34,716
但是如果你想用 

833
00:28:34,716 --> 0:28:37,156
你只需要请求

834
00:28:38,496 --> 0:28:42,416
在训练迁移学习模型时

835
00:28:42,416 --> 0:28:45,436
你只需要在算法规范里

836
00:28:45,436 --> 0:28:46,876
改变一个参数

837
00:28:47,216 --> 0:28:49,366
现在这里有几个选择

838
00:28:50,056 --> 0:28:52,776
首先 显而易见

839
00:28:52,776 --> 0:28:54,696
你可以用

840
00:28:54,696 --> 0:28:56,806
表示单词原意的

841
00:28:57,296 --> 0:29:00,486
内置 OS 单词嵌入

842
00:28:57,296 --> 0:29:00,486
内置 OS 单词嵌入

843
00:29:00,656 --> 0:29:02,696
如果你有一个

844
00:29:02,696 --> 0:29:03,936
自定义单词嵌入

845
00:29:03,936 --> 0:29:06,106
也可以用

846
00:29:06,306 --> 0:29:10,066
我们知道一个给定单词

847
00:29:10,066 --> 0:29:11,236
可以有不同的意思

848
00:29:11,236 --> 0:29:12,456
这个词的意思

849
00:29:12,456 --> 0:29:13,586
取决于上下文

850
00:29:14,016 --> 0:29:15,316
比如 Apple 在这两句话中

851
00:29:15,316 --> 0:29:16,896
意思完全不同

852
00:29:18,206 --> 0:29:20,586
我们希望

853
00:29:20,586 --> 0:29:22,356
用在迁移学习上的嵌入

854
00:29:22,356 --> 0:29:24,766
可以根据单词的

855
00:29:25,386 --> 0:29:28,816
意思和上下文

856
00:29:28,816 --> 0:29:31,226
给这些单词不同的值

857
00:29:31,226 --> 0:29:32,366
当然 普通的单词嵌入

858
00:29:32,366 --> 0:29:34,966
只是将单词映射到向量

859
00:29:34,966 --> 0:29:37,306
不管单词是什么意思

860
00:29:37,306 --> 0:29:39,036
它只能给出

861
00:29:39,036 --> 0:29:40,076
同样的值

862
00:29:41,466 --> 0:29:44,876
但我们做的是

863
00:29:44,876 --> 0:29:46,866
训练一个特殊嵌入

864
00:29:46,976 --> 0:29:50,356
让它根据单词的

865
00:29:50,356 --> 0:29:51,656
意思和上下文

866
00:29:51,656 --> 0:29:52,866
给出不同的值

867
00:29:53,446 --> 0:29:54,886
大家可以感受到

868
00:29:54,886 --> 0:29:56,086
这个领域发展的有多快

869
00:29:56,086 --> 0:29:57,456
这只是一年前

870
00:29:57,456 --> 0:30:00,166
我们在研究的东西

871
00:29:57,456 --> 0:30:00,166
我们在研究的东西

872
00:30:00,166 --> 0:30:03,156
现在就可以公布出来

873
00:30:03,376 --> 0:30:04,796
如果你想用

874
00:30:04,796 --> 0:30:06,546
你只需要请求

875
00:30:07,076 --> 0:30:08,956
指定一个动态嵌入

876
00:30:09,186 --> 0:30:10,426
这个动态嵌入

877
00:30:10,956 --> 0:30:13,366
会根据单词

878
00:30:13,366 --> 0:30:15,106
所处的上下文

879
00:30:15,106 --> 0:30:17,766
改变单词的值

880
00:30:17,766 --> 0:30:20,016
这是做文本分类的迁移学习

881
00:30:20,016 --> 0:30:22,296
非常强大的一个技术

882
00:30:23,546 --> 0:30:25,206
看一下示范

883
00:30:27,056 --> 0:30:30,716
好的 这里是一些

884
00:30:30,716 --> 0:30:32,636
用 Create ML 训练文本分类

885
00:30:32,676 --> 0:30:34,566
非常标准的代码

886
00:30:34,566 --> 0:30:37,236
和我即将训练的东西

887
00:30:37,236 --> 0:30:39,056
它基于一个数据集

888
00:30:39,056 --> 0:30:41,326
从一个名为 DBpedia 的

889
00:30:41,766 --> 0:30:43,836
开源百科全书中得到的

890
00:30:44,676 --> 0:30:46,486
它包含许多话题的

891
00:30:46,486 --> 0:30:47,426
简短词条

892
00:30:47,496 --> 0:30:49,456
一些是关于人

893
00:30:49,456 --> 0:30:51,096
艺术家 作家 

894
00:30:51,096 --> 0:30:52,616
植物 动物等等

895
00:30:52,616 --> 0:30:55,146
这里的任务是

896
00:30:55,146 --> 0:30:56,746
根据词条确定分类

897
00:30:56,876 --> 0:30:59,186
是一个人 还是一个作家

898
00:30:59,186 --> 0:31:01,886
或艺术家 等等

899
00:30:59,186 --> 0:31:01,886
或艺术家 等等

900
00:31:01,956 --> 0:31:04,186
有 14 种不同的类

901
00:31:04,186 --> 0:31:05,546
我想通过 

902
00:31:05,546 --> 0:31:07,036
200 个实例

903
00:31:07,036 --> 0:31:08,406
尝试训练分类器

904
00:31:08,906 --> 0:31:11,016
这是一个非常困难的任务

905
00:31:11,016 --> 0:31:13,766
我们用现有的 maxEnt 模型

906
00:31:13,766 --> 0:31:16,236
来尝试一下

907
00:31:17,356 --> 0:31:19,006
快速写入发送

908
00:31:19,436 --> 0:31:21,316
开始 结束了

909
00:31:22,776 --> 0:31:25,296
非常快 非常简单

910
00:31:25,296 --> 0:31:30,966
看一下测试集的表现情况

911
00:31:31,286 --> 0:31:34,116
77% 的准确率

912
00:31:35,116 --> 0:31:39,796
还可以 但是能更好吗

913
00:31:39,796 --> 0:31:41,756
稍微改动一下

914
00:31:41,756 --> 0:31:43,926
这里的代码

915
00:31:44,266 --> 0:31:46,856
不再使用 maxEnt 模型

916
00:31:46,856 --> 0:31:47,986
而是使用

917
00:31:47,986 --> 0:31:49,526
带有动态嵌入的迁移学习

918
00:31:49,526 --> 0:31:53,266
开始试一下

919
00:31:54,736 --> 0:31:56,186
正如我刚才所说

920
00:31:56,186 --> 0:31:57,956
这是在训练一个神经网络模型

921
00:31:58,136 --> 0:31:59,526
所以时间会久一点

922
00:32:00,256 --> 0:32:02,606
所以在它训练时

923
00:32:02,606 --> 0:32:04,286
我们可以细看一下

924
00:32:04,326 --> 0:32:05,916
正在训练的数据

925
00:32:06,326 --> 0:32:08,196
实际上

926
00:32:08,196 --> 0:32:09,646
在你训练神经网络模型时

927
00:32:09,916 --> 0:32:11,556
你需要关注

928
00:32:11,556 --> 0:32:14,216
训练使用的数据

929
00:32:14,596 --> 0:32:17,166
这个数据是

930
00:32:17,276 --> 0:32:19,386
跨多个类的随机实例

931
00:32:19,386 --> 0:32:22,646
我整理了一下

932
00:32:22,646 --> 0:32:23,846
所以每个类的

933
00:32:23,846 --> 0:32:26,676
实例数大致相同

934
00:32:27,186 --> 0:32:28,346
这是一个比较均衡的集

935
00:32:29,366 --> 0:32:30,626
我们的训练集

936
00:32:30,626 --> 0:32:31,926
另外我们还有一个

937
00:32:31,926 --> 0:32:34,146
单独的验证集

938
00:32:34,146 --> 0:32:36,226
也是跨类的随机实例

939
00:32:36,226 --> 0:32:38,546
也许没有训练集

940
00:32:38,546 --> 0:32:39,756
体积那么大

941
00:32:39,756 --> 0:32:41,296
但也是均衡的

942
00:32:41,986 --> 0:32:43,246
验证集在这种训练中

943
00:32:43,306 --> 0:32:45,536
尤为重要

944
00:32:45,996 --> 0:32:48,466
神经网络训练

945
00:32:48,516 --> 0:32:50,846
容易过度拟合

946
00:32:50,846 --> 0:32:52,666
会或多或少记住训练内容

947
00:32:52,666 --> 0:32:54,816
但不会概括总结

948
00:32:55,096 --> 0:32:56,616
验证集

949
00:32:56,616 --> 0:32:57,976
会确保它

950
00:32:57,976 --> 0:32:59,316
继续概括

951
00:33:00,146 --> 0:33:01,376
当然我们还有一个

952
00:33:01,376 --> 0:33:03,946
单独的测试集

953
00:33:03,946 --> 0:33:05,846
实例也是随机但均衡的

954
00:33:05,846 --> 0:33:10,196
当然 训练验证测试集之间

955
00:33:10,196 --> 0:33:12,006
没有重叠部分

956
00:33:12,006 --> 0:33:12,796
不然就是欺骗

957
00:33:13,966 --> 0:33:16,606
我们需要测试集

958
00:33:16,606 --> 0:33:17,576
查看我们在做什么

959
00:33:17,576 --> 0:33:19,586
尤其是在这个例子中

960
00:33:19,586 --> 0:33:22,296
有了测试集

961
00:33:22,296 --> 0:33:24,646
我们就知道迁移学习模型

962
00:33:24,646 --> 0:33:28,126
是不是比 maxEnt 模型好

963
00:33:28,596 --> 0:33:29,866
好像是结束了

964
00:33:29,866 --> 0:33:31,326
我们来看一下

965
00:33:31,606 --> 0:33:36,486
在这里我们可以看到

966
00:33:36,486 --> 0:33:38,576
迁移学习实现了

967
00:33:38,576 --> 0:33:41,106
86.5% 的正确率

968
00:33:41,186 --> 0:33:45,976
比 maxEnt 模型好很多

969
00:33:46,516 --> 0:33:51,796
[掌声]

970
00:33:52,296 --> 0:33:55,996
那么如何把这个应用到

971
00:33:55,996 --> 0:33:56,976
我们的奶酪 App 呢

972
00:33:57,056 --> 0:33:59,536
我已经有

973
00:33:59,576 --> 0:34:01,746
奶酪口味的笔记

974
00:33:59,576 --> 0:34:01,746
奶酪口味的笔记

975
00:34:03,146 --> 0:34:05,726
按它们提到的奶酪

976
00:34:05,756 --> 0:34:07,116
给每一个都标上了标签

977
00:34:07,396 --> 0:34:09,626
我将用这个

978
00:34:09,626 --> 0:34:10,676
训练分类器模型

979
00:34:10,676 --> 0:34:13,196
我的奶酪分类器模型

980
00:34:13,196 --> 0:34:16,146
会拿到一个句子

981
00:34:16,246 --> 0:34:18,216
尝试对他进行分类

982
00:34:18,216 --> 0:34:20,866
找到它最接近的是哪种奶酪

983
00:34:21,016 --> 0:34:24,036
把它放入我的奶酪 App

984
00:34:24,036 --> 0:34:28,826
我将在这个奶酪 App 里做的是

985
00:34:28,826 --> 0:34:32,366
如果用户

986
00:34:32,366 --> 0:34:33,545
没有特别提到一种奶酪

987
00:34:33,585 --> 0:34:34,936
那么我需要尝试找出

988
00:34:34,936 --> 0:34:36,746
他们想要哪种奶酪

989
00:34:37,255 --> 0:34:39,456
我要做的只是向模型请求

990
00:34:39,456 --> 0:34:43,226
文本的标签非常简单

991
00:34:43,226 --> 0:34:45,000
我们来试一下

992
00:35:01,046 --> 0:35:03,000
在这里输入一句话

993
00:35:15,486 --> 0:35:20,966
然后让奶酪器分类解析它

994
00:35:21,516 --> 0:35:23,536
奶酪分类器判定

995
00:35:23,696 --> 0:35:24,996
我想要的最接近

996
00:35:25,046 --> 0:35:26,716
卡芒贝尔奶酪

997
00:35:27,066 --> 0:35:29,066
然后我的奶酪嵌入

998
00:35:29,066 --> 0:35:30,516
会推荐一些

999
00:35:30,516 --> 0:35:31,936
其他相似的奶酪

1000
00:35:31,936 --> 0:35:32,756
布里奶酪等等

1001
00:35:34,156 --> 0:35:36,000
我也可以输入

1002
00:35:45,386 --> 0:35:46,826
质地结实的奶酪

1003
00:35:46,826 --> 0:35:47,946
它认定为是

1004
00:35:48,286 --> 0:35:49,826
切达奶酪

1005
00:35:50,036 --> 0:35:53,001
并推荐了一些相似的奶酪 [掌声]

1006
00:35:53,036 --> 0:35:54,386
这向我们展示了

1007
00:35:54,426 --> 0:35:57,596
文本分类结合其他

1008
00:35:57,596 --> 0:35:59,746
NaturalLanguage API 的

1009
00:35:59,956 --> 0:36:01,486
强大功能

1010
00:35:59,956 --> 0:36:01,486
强大功能

1011
00:36:01,736 --> 0:36:03,276
最后我将说明一些

1012
00:36:03,276 --> 0:36:05,366
使用文本分类

1013
00:36:05,456 --> 0:36:07,656
必须要考虑的问题

1014
00:36:07,656 --> 0:36:09,266
首先我们要注意

1015
00:36:09,266 --> 0:36:10,756
哪些语言支持迁移学习

1016
00:36:11,166 --> 0:36:12,846
考虑上下文

1017
00:36:12,846 --> 0:36:14,956
不论是通过静态嵌入

1018
00:36:14,956 --> 0:36:17,746
还是通过

1019
00:36:17,886 --> 0:36:19,576
动态嵌入

1020
00:36:20,026 --> 0:36:23,196
然后我想再说一下

1021
00:36:23,196 --> 0:36:25,056
数据方面的问题

1022
00:36:26,326 --> 0:36:28,446
处理数据的

1023
00:36:28,446 --> 0:36:29,626
第一要求是

1024
00:36:29,626 --> 0:36:31,676
你必须要了解

1025
00:36:31,676 --> 0:36:32,296
自己的领域

1026
00:36:33,506 --> 0:36:35,036
在实际操作中

1027
00:36:35,036 --> 0:36:36,176
你将遇到什么样的文本

1028
00:36:36,396 --> 0:36:37,626
是句子片段

1029
00:36:37,626 --> 0:36:39,086
还是完整的句子

1030
00:36:39,086 --> 0:36:41,556
还是多个句子

1031
00:36:41,556 --> 0:36:43,576
确保你的训练数据

1032
00:36:43,576 --> 0:36:45,526
和在实际情况中

1033
00:36:45,526 --> 0:36:47,106
可能遇到 需要分类的数据

1034
00:36:47,106 --> 0:36:48,576
尽量的相似

1035
00:36:49,646 --> 0:36:52,026
并且尽量全面地包括

1036
00:36:52,286 --> 0:36:53,646
你在 App 中 

1037
00:36:53,646 --> 0:36:55,866
可能遇到的

1038
00:36:55,866 --> 0:36:57,776
文本的变体

1039
00:36:58,906 --> 0:37:02,746
就像刚才在 DBpedia 例子中

1040
00:36:58,906 --> 0:37:02,746
就像刚才在 DBpedia 例子中

1041
00:37:02,746 --> 0:37:04,806
看到的那样

1042
00:37:05,266 --> 0:37:08,806
你要确保实例

1043
00:37:08,806 --> 0:37:11,316
尽量是随机的

1044
00:37:11,316 --> 0:37:13,606
训练集 验证集 测试集

1045
00:37:13,606 --> 0:37:15,266
尽量不同

1046
00:37:15,766 --> 0:37:17,696
这是基本的数据要求

1047
00:37:18,096 --> 0:37:21,706
如何知道

1048
00:37:21,706 --> 0:37:23,916
哪个算法最适合你呢 

1049
00:37:24,336 --> 0:37:25,616
普遍来讲 你需要尝试

1050
00:37:25,616 --> 0:37:28,426
但是也有一些参考

1051
00:37:28,626 --> 0:37:30,466
你可以先尝试 maxEnt 分类器

1052
00:37:30,466 --> 0:37:31,456
它速度很快

1053
00:37:31,456 --> 0:37:32,606
它会给你一个答案

1054
00:37:33,766 --> 0:37:36,356
但是 maxEnt 分类器能做什么呢

1055
00:37:36,896 --> 0:37:38,926
maxEnt 分类器

1056
00:37:38,926 --> 0:37:41,556
可以识别训练材料中

1057
00:37:42,146 --> 0:37:44,556
最常出现的词

1058
00:37:45,096 --> 0:37:46,806
比如说

1059
00:37:46,806 --> 0:37:49,756
你想训练识别积极和消极情感

1060
00:37:49,756 --> 0:37:51,486
它可能会注意到

1061
00:37:51,486 --> 0:37:52,676
爱和幸福是积极的

1062
00:37:52,736 --> 0:37:54,666
恨和不开心是消极的

1063
00:37:55,326 --> 0:37:57,406
如果实际使用中

1064
00:37:57,406 --> 0:37:59,106
遇到了这些单词

1065
00:37:59,106 --> 0:38:00,426
那么 maxEnt 分类器

1066
00:37:59,106 --> 0:38:00,426
那么 maxEnt 分类器

1067
00:38:00,426 --> 0:38:02,146
就会做得很好

1068
00:38:02,676 --> 0:38:08,146
迁移学习做了什么呢

1069
00:38:08,456 --> 0:38:10,546
它注意的是不是单词的意思呢

1070
00:38:11,056 --> 0:38:12,906
如果在实际操作中

1071
00:38:12,906 --> 0:38:14,476
遇到了用不同的单词

1072
00:38:14,476 --> 0:38:16,926
表达相同意思的情况

1073
00:38:17,016 --> 0:38:20,426
这时迁移学习模型

1074
00:38:20,426 --> 0:38:21,786
就会大放异彩

1075
00:38:21,786 --> 0:38:24,476
它做的就会

1076
00:38:24,476 --> 0:38:27,546
比普通的 maxEnt 模型好

1077
00:38:28,046 --> 0:38:33,086
总的来说 我们有一些新的 API

1078
00:38:33,086 --> 0:38:35,676
可以用于情感分析

1079
00:38:35,676 --> 0:38:38,706
可以结合 MLGazetteer 用于文本目录 

1080
00:38:38,706 --> 0:38:41,906
可以结合 NL 嵌入

1081
00:38:41,996 --> 0:38:45,376
用于单词嵌入

1082
00:38:45,376 --> 0:38:47,306
我们有一种新型的文本分类

1083
00:38:47,466 --> 0:38:49,046
因为可以用迁移学习

1084
00:38:49,716 --> 0:38:52,056
所以这个新类型特别强大

1085
00:38:53,096 --> 0:38:55,226
希望这些可以在你的 App 中

1086
00:38:55,226 --> 0:38:58,086
起到帮助作用

1087
00:38:58,426 --> 0:39:01,496
线上还有更多的信息

1088
00:38:58,426 --> 0:39:01,496
线上还有更多的信息

1089
00:39:01,496 --> 0:39:04,256
你也可以查看其他

1090
00:39:04,256 --> 0:39:05,256
相关的讲解

1091
00:39:06,356 --> 0:39:12,500
谢谢 [掌声]
