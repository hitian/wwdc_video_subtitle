1
00:00:00,506 --> 0:00:05,460
[音乐]

2
00:00:07,096 --> 0:00:07,976
&gt;&gt; 下午好

3
00:00:08,516 --> 0:00:14,716
[掌声]

4
00:00:15,216 --> 0:00:16,085
大家好

5
00:00:16,346 --> 0:00:18,076
欢迎来到我们

6
00:00:18,076 --> 0:00:19,636
介绍 ARKit 3 的会议

7
00:00:20,046 --> 0:00:21,286
我的名字是 Andreas

8
00:00:21,606 --> 0:00:23,466
我是 ARKit 团队的一名工程师

9
00:00:23,926 --> 0:00:25,196
今天我很高兴

10
00:00:25,196 --> 0:00:26,896
能在这里向大家介绍

11
00:00:26,896 --> 0:00:30,016
ARKit 的第三个主要发布

12
00:00:32,046 --> 0:00:33,926
当我们在 2017 年推出 ARKit 时

13
00:00:33,926 --> 0:00:37,196
我们将 iOS 打造成世界上

14
00:00:37,196 --> 0:00:39,976
最大的 AR 平台

15
00:00:39,976 --> 0:00:43,076
将 AR 带到了数亿台 iOS 设备上

16
00:00:43,236 --> 0:00:45,496
这对于你来说真的很重要

17
00:00:45,496 --> 0:00:47,516
因为你可以通过

18
00:00:47,516 --> 0:00:49,506
App 和你编写的游戏

19
00:00:49,506 --> 0:00:52,896
来接触广泛的受众

20
00:00:53,176 --> 0:00:54,626
我们的使命是

21
00:00:54,626 --> 0:00:55,946
从一开始就让你编写的

22
00:00:56,066 --> 0:00:58,056
第一个增强现实的 App

23
00:00:58,056 --> 0:01:00,106
变得很简单

24
00:00:58,056 --> 0:01:00,106
变得很简单

25
00:01:00,106 --> 0:01:01,186
即使你是一个新的开发人员

26
00:01:01,186 --> 0:01:03,626
但我们也希望为你提供

27
00:01:03,696 --> 0:01:05,906
你需要的手头工具

28
00:01:05,906 --> 0:01:09,126
来创建真正先进和复杂的体验

29
00:01:09,126 --> 0:01:12,116
所以如果你今天看看 App Store

30
00:01:12,116 --> 0:01:15,896
我们可以看到你做了一份了不起的工作

31
00:01:15,896 --> 0:01:17,606
你构建了很棒的 App 和游戏

32
00:01:18,126 --> 0:01:19,426
现在让我们来看看

33
00:01:19,426 --> 0:01:22,956
其中的一些

34
00:01:22,956 --> 0:01:25,086
将你的游戏理念带入到 AR 

35
00:01:25,086 --> 0:01:26,866
可以让它们更有

36
00:01:26,866 --> 0:01:29,426
吸引力和物理效果 如愤怒的小鸟

37
00:01:29,776 --> 0:01:31,226
你现在可以在 AR 中玩这个了

38
00:01:31,226 --> 0:01:32,446
实际上在这些建筑周围工作

39
00:01:32,446 --> 0:01:35,146
找到最佳

40
00:01:35,146 --> 0:01:38,076
射击地点 然后必须

41
00:01:38,076 --> 0:01:39,436
用自己的手弹射

42
00:01:39,436 --> 0:01:41,926
射出那些愤怒的小鸟

43
00:01:45,196 --> 0:01:47,426
对于大规模的用例

44
00:01:47,426 --> 0:01:48,806
ARKit 也非常有效

45
00:01:49,416 --> 0:01:52,236
iScape 是一种用于户外景观美化的工具

46
00:01:52,236 --> 0:01:54,856
我们可以放置灌木和树木

47
00:01:54,856 --> 0:01:56,116
并在你的花园或后院

48
00:01:56,116 --> 0:01:57,776
观看你的下一个

49
00:01:57,776 --> 0:02:02,976
花园改造项目在 AR 里开始运作

50
00:01:57,776 --> 0:02:02,976
花园改造项目在 AR 里开始运作

51
00:02:03,056 --> 0:02:05,706
去年 通过 ARKit 2 我们

52
00:02:05,706 --> 0:02:08,746
推出了 USDZ 一个新的 3D 文件

53
00:02:08,746 --> 0:02:11,766
格式 用于交换为 AR 

54
00:02:11,766 --> 0:02:13,106
制作的格式

55
00:02:14,126 --> 0:02:16,116
Wayfair 用它把 

56
00:02:16,116 --> 0:02:18,136
虚拟家具放在你的家里

57
00:02:18,136 --> 0:02:21,266
但是利用 ARKit 先进的

58
00:02:21,366 --> 0:02:22,806
场景理解功能

59
00:02:22,806 --> 0:02:24,226
比如环境纹理

60
00:02:24,396 --> 0:02:25,646
这些物品就真的会与你的

61
00:02:25,646 --> 0:02:27,626
客厅完美融合

62
00:02:27,776 --> 0:02:31,236
乐高正在用

63
00:02:31,236 --> 0:02:33,836
ARKit 的 3D 目标检测功能

64
00:02:35,326 --> 0:02:37,006
它可以找到你的物理乐高集

65
00:02:37,006 --> 0:02:38,696
并在 AR 中增强

66
00:02:38,696 --> 0:02:40,706
多亏 ARKit 的多用户支持

67
00:02:40,706 --> 0:02:42,456
你甚至可以

68
00:02:42,456 --> 0:02:44,906
和你的朋友们一起玩

69
00:02:45,396 --> 0:02:47,486
所以这些只是你创建的

70
00:02:47,486 --> 0:02:48,686
一些例子

71
00:02:49,476 --> 0:02:52,366
ARKit 帮助你处理所有的 

72
00:02:52,786 --> 0:02:54,296
技术细节

73
00:02:54,296 --> 0:02:56,266
为你做繁重的工作

74
00:02:56,266 --> 0:02:57,886
使得现实感增强

75
00:02:58,016 --> 0:02:59,886
以便你可以集中精神

76
00:03:00,186 --> 0:03:02,216
创建它们周围美妙的体验

77
00:03:03,016 --> 0:03:04,986
让我们快速回顾一下

78
00:03:05,186 --> 0:03:06,256
ARKit 为你提供的

79
00:03:06,256 --> 0:03:09,046
三大功能支柱

80
00:03:10,806 --> 0:03:12,376
首先是追踪

81
00:03:12,376 --> 0:03:16,206
追踪可以确定你的

82
00:03:16,206 --> 0:03:17,886
设备相对于环境的位置

83
00:03:17,886 --> 0:03:21,596
这样虚拟内容就可以准确定位

84
00:03:21,706 --> 0:03:24,026
并在摄像机图像上

85
00:03:24,026 --> 0:03:26,046
实时更新

86
00:03:27,186 --> 0:03:28,876
这就产生了

87
00:03:29,136 --> 0:03:30,876
虚拟内容实际上放置于

88
00:03:30,996 --> 0:03:32,736
现实世界的假象

89
00:03:33,906 --> 0:03:35,576
ARKit 还为你提供

90
00:03:35,636 --> 0:03:37,116
不同的追踪技术

91
00:03:37,116 --> 0:03:38,766
如道路世界追踪

92
00:03:39,136 --> 0:03:42,306
人脸追踪或图像追踪

93
00:03:42,996 --> 0:03:45,766
在追踪方面 我们有场景理解

94
00:03:47,606 --> 0:03:49,036
通过场景理解

95
00:03:49,036 --> 0:03:51,696
你可以识别场景中的表面

96
00:03:51,826 --> 0:03:54,266
图像和 3D 对象

97
00:03:54,266 --> 0:03:56,566
并将虚拟内容直接附加其上

98
00:03:58,196 --> 0:03:59,696
场景理解还可以

99
00:03:59,696 --> 0:04:01,416
了解环境中的光照

100
00:03:59,696 --> 0:04:01,416
了解环境中的光照

101
00:04:01,456 --> 0:04:03,556
甚至纹理

102
00:04:03,556 --> 0:04:06,226
以帮助你的内容看起来更加真实

103
00:04:06,486 --> 0:04:09,016
最后是渲染

104
00:04:09,146 --> 0:04:11,266
它让你的 3D 内容栩栩如生

105
00:04:12,806 --> 0:04:14,066
我们一直支持不同的

106
00:04:14,066 --> 0:04:17,406
渲染器 如 SceneKit SpriteKit 和 Metal

107
00:04:17,446 --> 0:04:19,906
现在 今年的

108
00:04:20,166 --> 0:04:22,176
RealityKit 也从一开始

109
00:04:22,176 --> 0:04:24,666
就在考虑增强现实

110
00:04:27,416 --> 0:04:29,046
因此今年随着 ARKit 的发布

111
00:04:29,046 --> 0:04:31,536
它们正在实现巨大的飞跃

112
00:04:32,416 --> 0:04:34,526
你的体验不仅

113
00:04:34,766 --> 0:04:36,236
看起来更好

114
00:04:36,316 --> 0:04:38,616
更自然 你还可以

115
00:04:38,616 --> 0:04:40,886
为之前无法

116
00:04:40,886 --> 0:04:43,396
实现的用例

117
00:04:43,396 --> 0:04:44,936
创建全新的体验

118
00:04:45,056 --> 0:04:47,646
感谢 ARKit 3 带来的

119
00:04:47,756 --> 0:04:49,706
许多新功能

120
00:04:50,796 --> 0:04:53,116
如人物遮挡 动作

121
00:04:53,116 --> 0:04:55,286
捕捉 协作会话

122
00:04:55,826 --> 0:04:57,396
同时使用前

123
00:04:57,396 --> 0:04:59,246
后摄像头 追踪

124
00:04:59,246 --> 0:05:00,836
多个人脸等等

125
00:04:59,246 --> 0:05:00,836
多个人脸等等

126
00:05:01,616 --> 0:05:02,966
我们要涉及很多内容

127
00:05:02,966 --> 0:05:05,586
所以让我们直接进入

128
00:05:05,586 --> 0:05:07,986
从人物遮挡开始

129
00:05:07,986 --> 0:05:12,066
让我们看看这里的场景

130
00:05:13,356 --> 0:05:14,406
因此为了创造一个

131
00:05:14,406 --> 0:05:16,846
令人信服的 AR 体验

132
00:05:16,846 --> 0:05:18,586
准确定位

133
00:05:18,736 --> 0:05:21,226
虚拟内容以及

134
00:05:21,226 --> 0:05:22,536
匹配真实世界中的灯光很重要

135
00:05:23,626 --> 0:05:25,076
所以让我们把一个虚拟的

136
00:05:25,406 --> 0:05:27,746
浓缩咖啡机放在这 把它放在桌子上

137
00:05:29,146 --> 0:05:31,066
但是等一下 当人们进入

138
00:05:31,176 --> 0:05:32,616
框架 就像在这个例子中一样

139
00:05:32,646 --> 0:05:34,356
它很容易打破幻觉

140
00:05:34,996 --> 0:05:35,956
因为你会期望

141
00:05:36,086 --> 0:05:37,996
前面的人实际上

142
00:05:38,136 --> 0:05:39,356
覆盖浓缩咖啡机

143
00:05:41,266 --> 0:05:43,186
所以有了 ARKit 3 和 

144
00:05:43,306 --> 0:05:45,996
人物遮挡 你就可以解决这个问题

145
00:05:47,106 --> 0:05:49,106
[掌声]

146
00:05:49,236 --> 0:05:49,526
谢谢

147
00:05:51,766 --> 0:05:53,666
所以让我们来看看这是如何完成的

148
00:05:55,546 --> 0:05:57,356
默认情况下虚拟内容

149
00:05:57,356 --> 0:05:59,736
呈现在相机图像的顶部

150
00:05:59,826 --> 0:06:04,726
如你所见 对于纯桌面体验

151
00:05:59,826 --> 0:06:04,726
如你所见 对于纯桌面体验

152
00:06:04,726 --> 0:06:06,296
这很好 但如果

153
00:06:06,296 --> 0:06:07,626
框架中的任何人

154
00:06:07,626 --> 0:06:09,806
在该对象前面

155
00:06:09,806 --> 0:06:10,816
增强则看起来不再准确

156
00:06:12,046 --> 0:06:14,566
所以 ARKit 3 现在为你做的是

157
00:06:15,456 --> 0:06:17,396
由于机器学习

158
00:06:17,676 --> 0:06:19,316
识别出框架中存在的人

159
00:06:19,316 --> 0:06:21,266
创建一个单独的图层

160
00:06:21,266 --> 0:06:23,506
只有包含这些

161
00:06:24,026 --> 0:06:25,176
人的像素

162
00:06:25,556 --> 0:06:26,976
我们称之为分割

163
00:06:28,286 --> 0:06:31,216
然后我们就可以在其他层之上渲染该层

164
00:06:31,456 --> 0:06:34,776
让我们来看看合成图像

165
00:06:35,416 --> 0:06:36,696
现在看起来好多了

166
00:06:37,676 --> 0:06:38,786
如果你凑近看

167
00:06:38,786 --> 0:06:39,996
其实仍然不够正确

168
00:06:41,206 --> 0:06:42,836
所以现在前面的人

169
00:06:43,146 --> 0:06:45,106
堵住了浓缩咖啡机

170
00:06:46,246 --> 0:06:47,506
但是这里如果拉近镜头

171
00:06:47,506 --> 0:06:49,086
你可以看到后面的人

172
00:06:49,086 --> 0:06:51,066
也被渲染到

173
00:06:51,066 --> 0:06:53,206
虚拟对象的顶部

174
00:06:53,306 --> 0:06:55,376
虽然她实际上站在桌子后面

175
00:06:55,856 --> 0:06:57,616
因此在这种情况下 虚拟模型

176
00:06:57,706 --> 0:07:00,406
应该挡住她 而不是反过来这种情况

177
00:06:57,706 --> 0:07:00,406
应该挡住她 而不是反过来这种情况

178
00:07:00,406 --> 0:07:04,446
现在 发生这种情况是因为

179
00:07:04,446 --> 0:07:06,396
我们没有考虑到

180
00:07:06,516 --> 0:07:08,316
人们与摄像机的距离

181
00:07:10,786 --> 0:07:12,816
当 ARKit 3 使用高级的

182
00:07:12,816 --> 0:07:14,246
机器学习来执行

183
00:07:14,246 --> 0:07:15,716
额外的深度估算

184
00:07:15,716 --> 0:07:18,446
步骤 用此估计

185
00:07:18,446 --> 0:07:19,886
分割的人离

186
00:07:19,886 --> 0:07:22,306
相机多远

187
00:07:22,346 --> 0:07:23,786
我们现在可以更正渲染顺序

188
00:07:23,786 --> 0:07:26,016
保证人在前面时进行渲染

189
00:07:26,016 --> 0:07:28,586
如果他们实际上离相机更近的话

190
00:07:28,746 --> 0:07:31,286
并且由于 Apple

191
00:07:31,286 --> 0:07:33,606
神经网络引擎我们能够

192
00:07:33,606 --> 0:07:36,206
在每个框架上进行实时操作

193
00:07:36,206 --> 0:07:40,736
现在让我们来看看

194
00:07:40,736 --> 0:07:41,876
合成图像

195
00:07:42,636 --> 0:07:43,436
正如你所期待的那样

196
00:07:43,536 --> 0:07:44,946
你看到人物

197
00:07:45,246 --> 0:07:47,186
遮挡的虚拟内容

198
00:07:50,356 --> 0:07:52,356
[掌声]

199
00:07:52,696 --> 0:07:53,976
这真的很棒 谢谢你们

200
00:07:58,146 --> 0:08:00,946
因此人物遮挡可以让

201
00:07:58,146 --> 0:08:00,946
因此人物遮挡可以让

202
00:08:00,946 --> 0:08:03,186
虚拟内容呈现在人物后面

203
00:08:04,296 --> 0:08:05,856
它也适用于场景中

204
00:08:05,906 --> 0:08:08,276
的多个人 甚至

205
00:08:08,276 --> 0:08:09,726
可以在人们只是

206
00:08:09,796 --> 0:08:11,936
部分可见的情况下就像

207
00:08:11,936 --> 0:08:13,696
在示例之前 

208
00:08:13,696 --> 0:08:15,766
桌子后面的女人实际上

209
00:08:15,766 --> 0:08:18,726
看不到整个身体 但是它仍然有效

210
00:08:19,816 --> 0:08:21,196
现在 这非常重要

211
00:08:21,196 --> 0:08:23,166
因为它不仅使

212
00:08:23,166 --> 0:08:25,226
你的体验看起来

213
00:08:25,226 --> 0:08:27,686
比以前更真实 而且

214
00:08:27,976 --> 0:08:29,356
对你来说也意味着你现在可以

215
00:08:29,356 --> 0:08:32,556
创建以前不可能的体验

216
00:08:33,446 --> 0:08:34,566
例如 考虑到一个

217
00:08:34,566 --> 0:08:36,655
多人游戏 在这个游戏中

218
00:08:36,966 --> 0:08:38,556
框架中有人和你的

219
00:08:38,856 --> 0:08:40,666
虚拟内容

220
00:08:42,976 --> 0:08:44,786
人物遮挡也

221
00:08:44,876 --> 0:08:48,396
集成在 ARView 和 ARSCNView 中

222
00:08:48,396 --> 0:08:51,616
而且由于深度估算

223
00:08:51,736 --> 0:08:53,326
我们可以为你提供一个

224
00:08:53,326 --> 0:08:55,456
关于相机监测到的

225
00:08:55,516 --> 0:09:00,436
人物距离的相似值

226
00:08:55,516 --> 0:09:00,436
人物距离的相似值

227
00:09:00,436 --> 0:09:03,266
我们正在用 Apple 神经网络引擎做此工作

228
00:09:03,796 --> 0:09:05,236
因此人物遮挡将在

229
00:09:05,236 --> 0:09:08,346
A12 处理器或者更高版本的设备上工作

230
00:09:08,346 --> 0:09:12,706
所以让我们来看看如何

231
00:09:12,706 --> 0:09:13,806
在 API 中打开它

232
00:09:14,716 --> 0:09:16,996
我们在 ARConfiguration 上

233
00:09:17,146 --> 0:09:20,106
有一个名为 FrameSemantics 的新性能

234
00:09:21,256 --> 0:09:23,086
这将为你提供

235
00:09:27,686 --> 0:09:29,416
你还可以使用

236
00:09:29,416 --> 0:09:31,336
ARConfiguration 上的其他方法

237
00:09:31,516 --> 0:09:33,616
检查特定设备或配置上

238
00:09:34,246 --> 0:09:37,176
是否有特定的语义可用

239
00:09:38,646 --> 0:09:40,306
尤其对于人物遮挡来说

240
00:09:40,426 --> 0:09:46,096
这有两种方法可用

241
00:09:46,316 --> 0:09:48,476
一种方法是人物分割

242
00:09:49,186 --> 0:09:51,096
这将帮助你只提供

243
00:09:51,356 --> 0:09:55,606
摄像头图像上呈现的人物分割

244
00:09:56,826 --> 0:09:58,096
如果你知道

245
00:09:58,096 --> 0:10:01,736
人们总是站在最前面 而你的虚拟内容

246
00:09:58,096 --> 0:10:01,736
人们总是站在最前面 而你的虚拟内容

247
00:10:05,476 --> 0:10:07,386
例如 绿屏用例

248
00:10:07,386 --> 0:10:10,346
就是你现在不需要绿屏了

249
00:10:10,616 --> 0:10:15,316
另一种选择是具有深度的人物分割

250
00:10:16,016 --> 0:10:17,266
这将为你提供

251
00:10:17,406 --> 0:10:22,326
这些人与相机距离的额外深度估计

252
00:10:23,266 --> 0:10:24,866
如果人们可以在

253
00:10:24,866 --> 0:10:26,546
内容之后或之前

254
00:10:26,586 --> 0:10:29,236
与虚拟内容一起显示

255
00:10:29,236 --> 0:10:33,286
那么这就是最佳选择

256
00:10:33,386 --> 0:10:34,876
如果你使用 Metal

257
00:10:35,046 --> 0:10:37,506
或者高级用例进行自己的渲染

258
00:10:37,506 --> 0:10:39,616
你还可以使用

259
00:10:39,616 --> 0:10:43,576
ARFrame 上的分段和估算的深度数据

260
00:10:43,576 --> 0:10:46,516
直接访问像素缓冲区 

261
00:10:47,756 --> 0:10:50,446
现在 让我向你展示

262
00:10:56,516 --> 0:11:02,056
[掌声]

263
00:10:56,516 --> 0:11:02,056
[掌声]

264
00:11:02,556 --> 0:11:04,356
所以在 Xcode 中 我有一个 

265
00:11:04,356 --> 0:11:07,656
使用新的 RealityKit API 的示例项目 

266
00:11:08,816 --> 0:11:11,456
让我快速带你了解一下它的作用

267
00:11:12,806 --> 0:11:15,296
因此在我们的 viewDidLoad 方法中

268
00:11:15,636 --> 0:11:17,666
我们正在创建一个 AnchorEntity

269
00:11:18,456 --> 0:11:19,926
用来查找水平面

270
00:11:20,486 --> 0:11:25,146
并将这个锚实体添加到场景中

271
00:11:25,146 --> 0:11:27,386
然后 我们检索一个

272
00:11:27,386 --> 0:11:30,816
模型的 URL 

273
00:11:31,186 --> 0:11:33,136
使用 ModelEntity 的异步模式

274
00:11:33,136 --> 0:11:34,536
加载 API 来载入它

275
00:11:34,536 --> 0:11:38,386
我们将实体添加为

276
00:11:38,386 --> 0:11:42,466
锚的子项 并且安装

277
00:11:42,466 --> 0:11:44,216
手势 以便我可以在

278
00:11:44,266 --> 0:11:46,736
平面上拖动对象

279
00:11:47,976 --> 0:11:49,996
因此由于 RealityKit  

280
00:11:49,996 --> 0:11:51,886
它所做的是自动

281
00:11:52,006 --> 0:11:53,126
设置世界追踪

282
00:11:53,126 --> 0:11:55,406
配置 因为我们知道

283
00:11:55,406 --> 0:11:58,536
我们需要使用世界追踪来进行平面估算

284
00:11:59,116 --> 0:12:00,916
然后 一旦检测到

285
00:11:59,116 --> 0:12:00,916
然后 一旦检测到

286
00:12:04,636 --> 0:12:08,296
现在还没有使用人物遮挡

287
00:12:09,516 --> 0:12:12,106
但是我已经停下来打开它了

288
00:12:12,866 --> 0:12:14,986
这叫做 togglePeopleOcclusion 

289
00:12:14,986 --> 0:12:16,556
我想要执行的是一种方法

290
00:12:16,786 --> 0:12:18,326
当用户点击屏幕时

291
00:12:18,326 --> 0:12:21,136
它允许我打开和关闭人物遮挡

292
00:12:22,256 --> 0:12:24,376
现在让我们继续实施吧

293
00:12:25,046 --> 0:12:28,306
所以我要做的第一件事是

294
00:12:28,436 --> 0:12:31,756
检查我的世界追踪

295
00:12:31,756 --> 0:12:34,076
配置是否支持具有

296
00:12:34,656 --> 0:12:37,296
深度框架语义的人物分割

297
00:12:38,176 --> 0:12:39,406
建议你始终

298
00:12:39,406 --> 0:12:42,776
这样做 因为如果代码

299
00:12:42,776 --> 0:12:47,166
在没有 Apple 神经网络引擎的设备上运行

300
00:12:47,286 --> 0:12:48,426
且不支持此框架语义

301
00:12:48,426 --> 0:12:53,396
我们希望可以从容地处理此事

302
00:12:53,606 --> 0:12:56,066
因此如果是这样的话

303
00:12:56,066 --> 0:12:57,186
我们将向用户展示一条信息

304
00:12:57,186 --> 0:13:00,496
即人物遮挡在该设备上不可用

305
00:12:57,186 --> 0:13:00,496
即人物遮挡在该设备上不可用

306
00:13:03,456 --> 0:13:06,826
让我们继续执行切换

307
00:13:08,056 --> 0:13:09,386
我将在这里

308
00:13:09,386 --> 0:13:11,436
对配置的 frameSemantics 属性

309
00:13:11,506 --> 0:13:13,276
执行 switch 语句

310
00:13:13,916 --> 0:13:16,586
如果框架语义中

311
00:13:16,586 --> 0:13:19,036
包含 personSegmentationWithDepth  

312
00:13:19,036 --> 0:13:22,116
我们将删除它并告诉用户

313
00:13:22,116 --> 0:13:23,786
人物遮挡已关闭

314
00:13:24,446 --> 0:13:29,436
我只需执行另一个案例

315
00:13:30,046 --> 0:13:30,986
如果你没有启用

316
00:13:30,986 --> 0:13:32,756
人物分割 那么我们

317
00:13:32,756 --> 0:13:35,416
将 frameSemantics 插入到

318
00:13:36,356 --> 0:13:38,096
不同的语义属性中

319
00:13:38,096 --> 0:13:39,696
并显示一条消息

320
00:13:39,786 --> 0:13:42,766
我们现在开启人物遮挡

321
00:13:43,346 --> 0:13:45,856
现在 我需要在会话中

322
00:13:45,886 --> 0:13:47,326
重新运行更新的配置

323
00:13:48,516 --> 0:13:51,086
因此让我从 ARView 中 

324
00:13:51,806 --> 0:13:55,136
检索会话 并使用

325
00:13:55,676 --> 0:13:58,826
我刚才更新的配置调用运行

326
00:14:00,036 --> 0:14:01,786
所以现在 我的

327
00:14:01,786 --> 0:14:03,936
togglePeopleOcclusion 方法已经完成 

328
00:14:03,936 --> 0:14:06,446
我现在需要确保

329
00:14:06,446 --> 0:14:08,116
用户点击屏幕时

330
00:14:08,776 --> 0:14:10,146
它实际上是被调用的

331
00:14:11,336 --> 0:14:12,696
我已经安装了一个轻点

332
00:14:12,696 --> 0:14:14,786
手势识别器 在我的

333
00:14:14,786 --> 0:14:20,736
onTap 方法中 我只调用 

334
00:14:21,396 --> 0:14:24,266
这就是我需要做的全部事情

335
00:14:25,376 --> 0:14:27,236
现在 让我继续

336
00:14:28,556 --> 0:14:33,086
构建代码并在我的设备上运行它

337
00:14:34,346 --> 0:14:36,876
我们已经看到

338
00:14:36,926 --> 0:14:40,066
平面被检测到 内容被放置

339
00:14:40,066 --> 0:14:41,856
我可以移动它

340
00:14:41,976 --> 0:14:44,226
由于我添加了手势

341
00:14:45,026 --> 0:14:46,886
你可以看到 RealityKit 已经

342
00:14:46,886 --> 0:14:48,966
增加了一个很好的接地阴影

343
00:14:49,726 --> 0:14:53,806
现在 让我们来看看人物遮挡

344
00:14:53,946 --> 0:14:55,176
现在 它仍然是关闭的

345
00:14:55,176 --> 0:14:57,366
所以如果我用手

346
00:14:57,486 --> 0:14:58,676
你会发现内容总是

347
00:14:58,676 --> 0:15:00,106
放在最上面的

348
00:14:58,676 --> 0:15:00,106
放在最上面的

349
00:15:00,426 --> 0:15:04,616
这是你从 ARKit 2 中所知道的行为

350
00:15:04,856 --> 0:15:07,736
现在 让我打开它 再把我的手拿回来

351
00:15:07,736 --> 0:15:10,156
现在你会看到

352
00:15:10,206 --> 0:15:12,866
[掌声] 虚拟物体实际上被覆盖了

353
00:15:14,516 --> 0:15:18,466
[掌声]

354
00:15:18,966 --> 0:15:22,236
这就是 ARKit 3 里的人体遮挡

355
00:15:27,786 --> 0:15:32,276
[掌声] 谢谢

356
00:15:32,276 --> 0:15:34,656
那么 让我们来谈一谈 ARKit 3 的

357
00:15:34,656 --> 0:15:38,616
另一个令人兴奋的功能 即动作捕捉

358
00:15:39,746 --> 0:15:42,736
通过动作捕捉

359
00:15:42,786 --> 0:15:45,056
你可以追踪人物的身体

360
00:15:45,056 --> 0:15:46,696
然后可以实时

361
00:15:46,696 --> 0:15:49,046
将其映射到虚拟角色

362
00:15:49,966 --> 0:15:51,156
现在 这只能通过

363
00:15:51,156 --> 0:15:53,776
外部设置和特殊装备来完成

364
00:15:54,466 --> 0:15:56,636
现在有了 ARKit 3 它只

365
00:15:56,706 --> 0:15:58,566
需要几行代码

366
00:15:58,566 --> 0:16:00,806
就可以在 iPad 和 iPhone 上正常工作

367
00:15:58,566 --> 0:16:00,806
就可以在 iPad 和 iPhone 上正常工作

368
00:16:03,476 --> 0:16:06,086
现在 动作捕捉让你在

369
00:16:06,126 --> 0:16:10,026
2D 和 3D 中追踪人体

370
00:16:10,586 --> 0:16:12,976
它为你提供了

371
00:16:12,976 --> 0:16:15,676
该人的骨架表现

372
00:16:16,286 --> 0:16:21,256
例如 这可以驱动虚拟角色

373
00:16:22,036 --> 0:16:23,796
这是通过在

374
00:16:23,796 --> 0:16:24,856
Apple 神经网络引擎上运行的

375
00:16:24,856 --> 0:16:27,426
高级机器学习算法实现的

376
00:16:28,116 --> 0:16:29,586
因此它可以在

377
00:16:29,586 --> 0:16:31,496
A12 或更高版本处理器的设备上使用

378
00:16:32,656 --> 0:16:35,076
让我们先来看看 2D 人体检测

379
00:16:35,676 --> 0:16:39,286
怎么打开呢

380
00:16:40,336 --> 0:16:42,186
我们有一个名为

381
00:16:42,186 --> 0:16:43,866
bodyDetection 的新框架语义选项

382
00:16:44,746 --> 0:16:46,396
世界追踪配置

383
00:16:46,396 --> 0:16:48,666
以及图像和方向追踪配置

384
00:16:48,666 --> 0:16:51,416
也支持此功能

385
00:16:52,276 --> 0:16:53,506
所以你只需将它添加到

386
00:16:53,506 --> 0:16:56,036
你的框架语义中并在会话中调用运行

387
00:16:57,636 --> 0:16:58,486
现在 让我们来看看

388
00:16:58,486 --> 0:17:02,696
我们将要得到的数据

389
00:16:58,486 --> 0:17:02,696
我们将要得到的数据

390
00:17:02,696 --> 0:17:05,116
如果检测到某人

391
00:17:05,165 --> 0:17:07,945
则每个 ARFrame 在 detectedBody 属性中

392
00:17:07,945 --> 0:17:12,506
提供 ARBody2D 类型的对象

393
00:17:13,376 --> 0:17:15,996
该对象包含 2D 骨架

394
00:17:17,455 --> 0:17:19,106
骨架为你提供

395
00:17:19,106 --> 0:17:21,016
标准化图像空间中的

396
00:17:21,076 --> 0:17:22,896
所有关节标志

397
00:17:23,876 --> 0:17:24,896
它们以数组的

398
00:17:24,896 --> 0:17:26,896
平面层次结构返回

399
00:17:27,026 --> 0:17:28,246
因为这是最有效率的处理方法

400
00:17:29,846 --> 0:17:31,386
但是你将获得一个骨架定义

401
00:17:33,406 --> 0:17:34,706
由于骨架定义

402
00:17:34,706 --> 0:17:36,416
你拥有如何解释

403
00:17:36,416 --> 0:17:39,656
骨架数据的所有信息

404
00:17:40,446 --> 0:17:43,356
特别是 它包含有关关节层次结构的

405
00:17:43,356 --> 0:17:45,426
信息 比如 实际上

406
00:17:45,676 --> 0:17:48,386
手关节是肘关节的子关节

407
00:17:49,786 --> 0:17:51,176
并且还为你提供关节

408
00:17:51,266 --> 0:17:55,266
的名称 然后来方便访问

409
00:17:55,936 --> 0:17:58,256
所以让我们来看看这个是什么样子的

410
00:17:59,056 --> 0:18:01,006
这是我们在框架中检测到的人

411
00:17:59,056 --> 0:18:01,006
这是我们在框架中检测到的人

412
00:18:01,926 --> 0:18:03,526
这就是 ARKit 提供的

413
00:18:03,526 --> 0:18:05,426
2D 骨架

414
00:18:06,016 --> 0:18:10,056
如前所述 重要关节的命名是为了

415
00:18:10,056 --> 0:18:11,986
便于你找到

416
00:18:11,986 --> 0:18:13,036
你感兴趣的

417
00:18:13,036 --> 0:18:17,446
特定关节的位置 比如头部和右手

418
00:18:18,636 --> 0:18:19,996
所以这是 2D

419
00:18:20,626 --> 0:18:24,166
现在 让我们来看看 3D 运动捕捉

420
00:18:25,216 --> 0:18:26,766
3D 动作捕捉让你

421
00:18:26,766 --> 0:18:29,286
在 3D 空间中追踪人体

422
00:18:29,936 --> 0:18:31,046
并为你提供

423
00:18:34,656 --> 0:18:36,496
它还为你提供比例估计

424
00:18:36,496 --> 0:18:38,516
以便你确定

425
00:18:38,516 --> 0:18:40,696
正在跟踪的人的大小

426
00:18:41,746 --> 0:18:45,256
3D 骨架定位在世界坐标系中

427
00:18:46,516 --> 0:18:48,496
让我们看看在 API 中如何使用这个

428
00:18:52,006 --> 0:18:53,276
我们正在引入一种

429
00:18:53,336 --> 0:18:56,806
名为 ARBodyTrackingConfiguration 的新配置

430
00:18:58,426 --> 0:19:00,346
这可以让你使用 3D 身体

431
00:18:58,426 --> 0:19:00,346
这可以让你使用 3D 身体

432
00:19:00,346 --> 0:19:02,856
追踪 但它也提供了

433
00:19:03,186 --> 0:19:06,476
我们之前看到的 2D 身体检测

434
00:19:06,916 --> 0:19:09,346
因此在该配置中

435
00:19:09,346 --> 0:19:11,486
框架语义是默认打开的

436
00:19:12,686 --> 0:19:14,626
因此 该配置

437
00:19:14,686 --> 0:19:16,236
还追踪设备的

438
00:19:16,296 --> 0:19:18,666
位置和方向并

439
00:19:18,666 --> 0:19:20,166
提供选定的世界追踪功能

440
00:19:20,166 --> 0:19:23,926
如平面估算或图像检测

441
00:19:24,746 --> 0:19:26,756
因此有了它 你就可以

442
00:19:26,756 --> 0:19:28,666
在你的 AR App 中

443
00:19:29,316 --> 0:19:32,086
使用身体追踪来做更多的事情

444
00:19:33,516 --> 0:19:35,716
若要对其进行设置 你

445
00:19:35,716 --> 0:19:37,426
只需创建身体追踪

446
00:19:37,426 --> 0:19:40,256
配置并在会话中运行它

447
00:19:41,396 --> 0:19:43,096
注意 我们还有 API 来

448
00:19:43,146 --> 0:19:45,076
检查当前设备

449
00:19:45,076 --> 0:19:49,136
是否支持该配置

450
00:19:49,306 --> 0:19:51,746
所以 现在 当 ARKit 正在运行

451
00:19:52,076 --> 0:19:54,026
并检测到一个人时 它将

452
00:19:54,026 --> 0:19:58,026
添加一种新类型的锚 即 ARBodyAnchor 

453
00:20:00,096 --> 0:20:02,366
这将在调用锚

454
00:20:02,366 --> 0:20:03,546
的会话中提供给你

455
00:20:03,546 --> 0:20:05,186
就像你知道的

456
00:20:05,186 --> 0:20:06,436
其他锚类型一样

457
00:20:07,786 --> 0:20:09,526
和其他锚一样

458
00:20:09,526 --> 0:20:11,796
它也有一个转换

459
00:20:11,796 --> 0:20:13,066
为你提供被检测者

460
00:20:13,066 --> 0:20:15,136
在世界坐标中的

461
00:20:15,206 --> 0:20:17,306
位置和方向

462
00:20:17,856 --> 0:20:19,576
此外 你将获得

463
00:20:19,686 --> 0:20:22,296
比例系数和 3D

464
00:20:22,676 --> 0:20:23,826
骨架的参考

465
00:20:24,446 --> 0:20:29,176
让我们来看看它是怎么样的

466
00:20:29,846 --> 0:20:31,056
你可以看到它比

467
00:20:31,056 --> 0:20:33,406
2D 骨架更加详细

468
00:20:34,436 --> 0:20:35,626
因此黄色关节是

469
00:20:35,626 --> 0:20:37,036
将通过运动捕捉数据

470
00:20:37,036 --> 0:20:39,316
传递给用户的关节

471
00:20:40,396 --> 0:20:42,516
白色的是我们在骨骼中

472
00:20:42,876 --> 0:20:45,046
额外提供的叶关节

473
00:20:46,096 --> 0:20:47,936
这些没有被有效追踪

474
00:20:48,246 --> 0:20:49,806
因此变换对于

475
00:20:49,806 --> 0:20:51,156
被追踪的父项是静态的

476
00:20:51,846 --> 0:20:53,486
但是 当然 你可以直接

477
00:20:53,486 --> 0:20:55,666
访问每个关节并

478
00:20:56,006 --> 0:20:58,766
检索它们的道路坐标

479
00:21:00,016 --> 0:21:01,606
同样 我们有一个

480
00:21:01,606 --> 0:21:04,056
很重要的标签 一个

481
00:21:04,326 --> 0:21:06,066
按名称查询它们的 API 这样

482
00:21:06,066 --> 0:21:08,826
你就可以很容易地找到

483
00:21:08,826 --> 0:21:11,456
你感兴趣的特定关节

484
00:21:12,016 --> 0:21:13,686
现在 我确信你可以

485
00:21:13,686 --> 0:21:15,486
为这个新的 API 提出

486
00:21:15,746 --> 0:21:18,376
很多很好的用例 但是我想

487
00:21:18,376 --> 0:21:20,026
谈谈一个可能对于

488
00:21:20,056 --> 0:21:21,866
你们很多人来说

489
00:21:21,866 --> 0:21:24,256
都很有趣的特殊用例 它就是

490
00:21:24,346 --> 0:21:26,036
动画 3D 角色

491
00:21:27,696 --> 0:21:30,256
通过将 ARKit 和 RealityKit 

492
00:21:30,256 --> 0:21:32,736
结合使用 你可以驱动一个 

493
00:21:32,736 --> 0:21:34,986
基于三维骨架姿势的模型

494
00:21:34,986 --> 0:21:36,836
这真的非常简单

495
00:21:37,936 --> 0:21:41,486
你所需要的只是一个装配好的网格

496
00:21:42,586 --> 0:21:43,916
你可以在我们的示例 App 中

497
00:21:43,916 --> 0:21:45,506
找到一个这样的例子

498
00:21:45,506 --> 0:21:46,646
你可以在会话主页上

499
00:21:46,646 --> 0:21:48,606
下载 当然 你也可以

500
00:21:48,606 --> 0:21:50,586
在自己选择的内容创建工具中

501
00:21:50,586 --> 0:21:52,296
自由创建自己的 App

502
00:21:52,826 --> 0:21:56,576
让我们看看在代码中实现这一点有多容易

503
00:21:56,856 --> 0:21:58,756
它内置于 RealityKit API 中

504
00:21:59,736 --> 0:22:01,036
我们将使用的

505
00:21:59,736 --> 0:22:01,036
我们将使用的

506
00:22:01,036 --> 0:22:03,686
主要类是 BodyTrackedEntity 

507
00:22:09,486 --> 0:22:12,056
你要做的第一件事是

508
00:22:12,056 --> 0:22:14,676
创建一个 AnchorEntity 的

509
00:22:14,736 --> 0:22:17,376
类型主体 并将此锚添加到场景中 

510
00:22:18,786 --> 0:22:21,736
接下来 你将加载模型

511
00:22:22,036 --> 0:22:23,496
在我们的例子中 它被称为机器人

512
00:22:24,606 --> 0:22:26,136
为此 我们使用异步

513
00:22:26,136 --> 0:22:27,386
加载 API   

514
00:22:27,826 --> 0:22:29,076
在完成处理器中

515
00:22:29,516 --> 0:22:32,806
你将获得 BodyTrackedEntity 我们现在

516
00:22:32,866 --> 0:22:35,906
只需将其添加为 bodyAnchor 的子项

517
00:22:39,666 --> 0:22:42,516
因此只要 ARKit 现在将

518
00:22:42,516 --> 0:22:44,536
AR 主体锚点添加到会话中

519
00:22:45,846 --> 0:22:48,176
骨架的 3D 姿势就会

520
00:22:48,176 --> 0:22:50,386
自动实时应用于

521
00:22:50,386 --> 0:22:52,276
虚拟模型

522
00:22:53,466 --> 0:22:54,916
这就是使用 ARKit 3 进行

523
00:22:54,916 --> 0:22:56,896
动作捕捉的简单方法

524
00:22:58,316 --> 0:23:03,936
[掌声] 谢谢

525
00:22:58,316 --> 0:23:03,936
[掌声] 谢谢

526
00:23:03,936 --> 0:23:06,436
那么 现在让我们谈谈

527
00:23:06,436 --> 0:23:10,526
同步的前后摄像头

528
00:23:10,526 --> 0:23:12,166
ARKit 让你使用后置摄像头来                                      

529
00:23:12,166 --> 0:23:14,316
进行世界追踪 

530
00:23:14,316 --> 0:23:16,056
并且前面使用 2Depth

531
00:23:16,056 --> 0:23:17,326
摄像头系统来进行人脸追踪

532
00:23:18,176 --> 0:23:20,116
现在 你非常需要的一个

533
00:23:20,116 --> 0:23:22,386
功能是能够

534
00:23:22,386 --> 0:23:24,896
把前置和后置摄像头的

535
00:23:24,896 --> 0:23:26,456
用户体验结合到一起

536
00:23:27,316 --> 0:23:30,666
现在 有了 ARKit 3 你现在可以做这个了

537
00:23:31,956 --> 0:23:33,426
因此有了这个新功能 你

538
00:23:33,426 --> 0:23:35,596
可以同时使用两个摄像头

539
00:23:35,676 --> 0:23:37,696
来构建 AR 体验

540
00:23:37,996 --> 0:23:39,336
这就意味着 

541
00:23:39,866 --> 0:23:41,666
你现在可以构建两种新

542
00:23:41,716 --> 0:23:42,866
类型的用例

543
00:23:43,896 --> 0:23:47,106
首先 你可以创建世界追踪体验

544
00:23:47,106 --> 0:23:48,816
因此 使用后置摄像头

545
00:23:48,886 --> 0:23:51,156
还可以从前置摄像头

546
00:23:51,156 --> 0:23:54,096
捕获的面部数据中受益

547
00:23:55,106 --> 0:23:56,746
你可以创建面部追踪

548
00:23:56,746 --> 0:23:59,086
体验 利用

549
00:23:59,216 --> 0:24:00,826
全方位设备定位和

550
00:23:59,216 --> 0:24:00,826
全方位设备定位和

551
00:24:00,826 --> 0:24:02,956
6 度自由度的位置

552
00:24:03,406 --> 0:24:06,896
所有这些都支持 A12

553
00:24:06,896 --> 0:24:08,546
设备和更高版本

554
00:24:08,806 --> 0:24:10,956
让我们来看一个例子

555
00:24:11,426 --> 0:24:12,586
在这里 我们使用

556
00:24:12,586 --> 0:24:14,196
平面估算来进行世界追踪

557
00:24:14,806 --> 0:24:16,836
但是我们也在平面顶部

558
00:24:16,836 --> 0:24:18,656
放置了一个人脸网格

559
00:24:18,656 --> 0:24:20,586
并通过前置摄像头

560
00:24:20,636 --> 0:24:23,186
捕捉到的面部表情

561
00:24:23,186 --> 0:24:24,216
实时更新它

562
00:24:24,216 --> 0:24:27,436
所以让我们看看如何

563
00:24:27,516 --> 0:24:31,336
在 API 中使用并存的前后摄像头

564
00:24:31,546 --> 0:24:34,066
首先 让我们创建一个世界追踪配置

565
00:24:34,416 --> 0:24:36,416
现在 我选择的配置

566
00:24:36,506 --> 0:24:38,706
决定了屏幕上实际显示的

567
00:24:38,706 --> 0:24:40,926
是哪个摄像头流

568
00:24:41,206 --> 0:24:44,256
因此在这种情况下 是后置摄像头

569
00:24:45,666 --> 0:24:47,756
现在我打开新的

570
00:24:47,916 --> 0:24:50,386
userFaceTrackingEnabled 属性

571
00:24:50,586 --> 0:24:52,046
并运行会话

572
00:24:52,366 --> 0:24:58,156
这将导致我收到面部锚点

573
00:24:58,276 --> 0:25:01,326
然后我可以使用

574
00:24:58,276 --> 0:25:01,326
然后我可以使用

575
00:25:01,326 --> 0:25:02,876
来自锚点的任何信息

576
00:25:03,066 --> 0:25:05,496
如面部网格 地形

577
00:25:05,926 --> 0:25:08,666
或者锚点自身的变换

578
00:25:09,236 --> 0:25:11,496
现在 请注意 由于我们

579
00:25:11,666 --> 0:25:13,646
在这处理世界坐标

580
00:25:13,646 --> 0:25:15,256
用户面部传输将被

581
00:25:15,256 --> 0:25:17,646
放置在摄像机后面

582
00:25:17,646 --> 0:25:19,196
这意味着为了能看见面部

583
00:25:19,256 --> 0:25:20,486
你需要

584
00:25:20,616 --> 0:25:22,156
将其转换到

585
00:25:22,156 --> 0:25:26,366
相机前面的某个位置

586
00:25:26,366 --> 0:25:29,486
现在 让我们看看人脸追踪配置

587
00:25:30,366 --> 0:25:31,746
你可以像往常一样

588
00:25:31,746 --> 0:25:33,646
创建面部追踪配置

589
00:25:33,686 --> 0:25:37,496
并将 worldTrackingEnabled 设置成 true

590
00:25:38,226 --> 0:25:42,876
然后 在运行配置后 你可以

591
00:25:42,876 --> 0:25:45,216
在每个帧中访问 例如

592
00:25:45,216 --> 0:25:46,696
在更新帧的会话中

593
00:25:46,856 --> 0:25:50,386
回调当前

594
00:25:50,386 --> 0:25:51,586
相机位置的转换

595
00:25:52,216 --> 0:25:54,506
然后你就可以把它用于

596
00:25:54,616 --> 0:25:56,116
任何你想到的用例

597
00:25:56,326 --> 0:25:58,626
这就是 ARKit 3 里面

598
00:25:58,626 --> 0:26:00,076
同步的前后摄像头

599
00:25:58,626 --> 0:26:00,076
同步的前后摄像头

600
00:26:01,036 --> 0:26:02,376
我们认为你能够

601
00:26:02,526 --> 0:26:04,516
使用这个新的 API 

602
00:26:04,516 --> 0:26:06,126
来做出许多出色的用例

603
00:26:07,281 --> 0:26:09,281
[掌声]

604
00:26:09,546 --> 0:26:12,421
谢谢 [掌声]

605
00:26:12,826 --> 0:26:14,446
现在 让我把它交给

606
00:26:14,446 --> 0:26:16,506
Thomas 他将会告诉你所有 

607
00:26:16,506 --> 0:26:17,816
关于协作会议的内容

608
00:26:20,516 --> 0:26:22,636
[掌声]

609
00:26:23,136 --> 0:26:23,426
&gt;&gt; 谢谢

610
00:26:24,366 --> 0:26:25,056
谢谢你 Andreas 

611
00:26:25,416 --> 0:26:26,386
大家下午好

612
00:26:26,736 --> 0:26:28,876
我的名字是 Thomas 来自 ARKit 团队

613
00:26:29,506 --> 0:26:31,686
让我们谈谈协作会议

614
00:26:32,216 --> 0:26:35,276
在 ARKit 2 中 你可以创建

615
00:26:35,666 --> 0:26:37,786
具有保存和加载世界地图

616
00:26:37,786 --> 0:26:40,086
功能的多用户体验

617
00:26:40,726 --> 0:26:42,046
你必须在一台设备上保存地图

618
00:26:42,046 --> 0:26:43,686
并将其发送到另一台

619
00:26:43,686 --> 0:26:45,756
设备 以便你的用户再次

620
00:26:45,756 --> 0:26:47,276
获得相同的体验

621
00:26:48,586 --> 0:26:50,556
这是一个单一的地图 一次性的

622
00:26:50,556 --> 0:26:52,026
地图共享体验 在这

623
00:26:52,026 --> 0:26:54,306
之后 大多数用户

624
00:26:54,306 --> 0:26:55,186
将不会相同

625
00:26:55,296 --> 0:26:57,506
不再共享相同的信息

626
00:26:58,006 --> 0:27:02,816
好吧 通过在 ARKit 3 上的协作会话 我们现在

627
00:26:58,006 --> 0:27:02,816
好吧 通过在 ARKit 3 上的协作会话 我们现在

628
00:27:02,816 --> 0:27:04,416
可以在整个网络上

629
00:27:04,416 --> 0:27:05,886
持续共享你的映射信息

630
00:27:07,226 --> 0:27:09,016
这可以使你创建临时的

631
00:27:09,016 --> 0:27:11,366
多用户体验 你的

632
00:27:11,366 --> 0:27:14,686
用户可以更轻松地访问同一会话

633
00:27:15,896 --> 0:27:18,076
此外 我们还允许你

634
00:27:18,826 --> 0:27:20,626
在所有设备上

635
00:27:20,626 --> 0:27:22,556
分享或者我们实际共享 ARAnchors 

636
00:27:23,186 --> 0:27:24,226
所有这些锚点

637
00:27:24,226 --> 0:27:26,016
都可以通过锚点的会话 ID 

638
00:27:26,016 --> 0:27:28,216
识别出来 

639
00:27:29,536 --> 0:27:31,386
注意 在这一点上 大多数

640
00:27:31,386 --> 0:27:32,846
所有的坐标系都是

641
00:27:32,846 --> 0:27:34,016
相互独立的

642
00:27:34,016 --> 0:27:35,116
即使我们仍然在幕后

643
00:27:35,116 --> 0:27:37,206
共享信息

644
00:27:37,406 --> 0:27:38,976
让我给你展示一下这是如何运作的

645
00:27:41,566 --> 0:27:44,956
所以在这个视频中 我们可以看到两个用户

646
00:27:45,016 --> 0:27:46,246
注意颜色

647
00:27:46,246 --> 0:27:47,656
一个用户将以绿色

648
00:27:47,656 --> 0:27:49,826
显示功能点 另一个用户

649
00:27:49,826 --> 0:27:52,896
将以红色显示功能点

650
00:27:53,086 --> 0:27:53,976
当他们在环境中移动时

651
00:27:53,976 --> 0:27:56,976
他们开始

652
00:27:56,976 --> 0:28:00,986
映射环境并添加更多的特征点

653
00:27:56,976 --> 0:28:00,986
映射环境并添加更多的特征点

654
00:28:01,976 --> 0:28:05,316
此时 这是他们内部地图的

655
00:28:05,316 --> 0:28:07,186
内部展示 他们不知道

656
00:28:07,186 --> 0:28:08,246
彼此的地图

657
00:28:08,506 --> 0:28:14,576
当他们四处移动时 会收集更多的特征点

658
00:28:17,796 --> 0:28:19,026
当他们在场景中

659
00:28:19,026 --> 0:28:20,776
收集到更多的特征点 你可以

660
00:28:20,776 --> 0:28:21,876
看到内部地图 

661
00:28:21,876 --> 0:28:23,426
注意颜色和它们的

662
00:28:23,426 --> 0:28:25,416
最终匹配点 然后这些

663
00:28:25,416 --> 0:28:28,806
内部地图将合并到一起

664
00:28:28,806 --> 0:28:30,716
只形成一个地图 这意味着

665
00:28:30,716 --> 0:28:32,366
每个用户现在都

666
00:28:32,366 --> 0:28:36,056
了解彼此 也了解场景理解

667
00:28:36,056 --> 0:28:40,206
当他们四处移动时

668
00:28:40,206 --> 0:28:41,346
会提供更多信息

669
00:28:42,606 --> 0:28:43,786
他们又持续在

670
00:28:43,786 --> 0:28:45,106
幕后共享信息

671
00:28:46,316 --> 0:28:48,766
此外 ARKit 3 还为你提供了

672
00:28:48,766 --> 0:28:51,016
类似 AR 的参与者锚点

673
00:28:51,016 --> 0:28:54,786
使你可以了解其他用户

674
00:28:54,786 --> 0:28:56,386
在你的环境中的实时位置

675
00:28:57,226 --> 0:28:59,106
如果你想展示

676
00:28:59,106 --> 0:29:00,646
一个图标或者其他代表该用户的东西

677
00:28:59,106 --> 0:29:00,646
一个图标或者其他代表该用户的东西

678
00:29:00,646 --> 0:29:02,066
这会非常方便

679
00:29:02,616 --> 0:29:07,436
如前所述 ARKit 3

680
00:29:07,436 --> 0:29:09,076
也会在幕后共享 ARAnchors 

681
00:29:09,076 --> 0:29:11,656
这意味着如果你在

682
00:29:11,656 --> 0:29:13,406
一台设备上共享或添加一个锚点

683
00:29:13,406 --> 0:29:15,696
它将自动显示在另一台设备上

684
00:29:17,106 --> 0:29:18,846
让我们看看它在代码中是如何工作的

685
00:29:20,576 --> 0:29:22,066
正如 Andreas 之前提到的

686
00:29:22,096 --> 0:29:24,406
ARKit 和 RealityKit 完全融为一体

687
00:29:25,386 --> 0:29:26,876
如果你想启用

688
00:29:26,876 --> 0:29:27,916
与 RealityKit 的协作会话

689
00:29:27,916 --> 0:29:29,366
这将会非常简单

690
00:29:30,276 --> 0:29:33,126
你首先需要设置你的

691
00:29:33,506 --> 0:29:36,326
Multipeer Connectivity 框架是

692
00:29:36,326 --> 0:29:37,346
一个 Apple 框架

693
00:29:37,346 --> 0:29:38,676
允许你进行发现和

694
00:29:38,676 --> 0:29:40,196
点对点连接

695
00:29:40,196 --> 0:29:43,126
然后 你需要将这个

696
00:29:43,126 --> 0:29:45,116
多点连接会话传递

697
00:29:45,116 --> 0:29:48,826
给 AR 场景视图同步服务

698
00:29:49,316 --> 0:29:53,806
最后 正如每个 ARKit

699
00:29:53,806 --> 0:29:55,456
体验一样 你必须设置

700
00:29:55,506 --> 0:29:57,286
你的 ARWorldTrackingConfiguration 

701
00:29:57,436 --> 0:29:59,576
将 isCollaborationEnabled

702
00:29:59,576 --> 0:30:01,566
标志设置为 true 并在

703
00:29:59,576 --> 0:30:01,566
标志设置为 true 并在

704
00:30:01,566 --> 0:30:02,706
会话中运行该配置

705
00:30:03,236 --> 0:30:08,196
就是这样 接下来会发生什么

706
00:30:09,416 --> 0:30:11,046
因此 ARKit 将

707
00:30:11,046 --> 0:30:13,286
isCollaborationEnabled 标志 

708
00:30:13,796 --> 0:30:16,336
设置成 true 时 基本上将

709
00:30:16,336 --> 0:30:17,466
并且在会话上运行该配置

710
00:30:17,466 --> 0:30:18,836
基本上是将在

711
00:30:18,836 --> 0:30:22,506
ARSessionDelegate 上创建

712
00:30:22,506 --> 0:30:24,006
一个新方法 

713
00:30:24,006 --> 0:30:24,936
以便你传输该数据

714
00:30:25,576 --> 0:30:27,016
在 RealityKit 用例中

715
00:30:27,016 --> 0:30:28,686
我们将处理它 但是如果

716
00:30:28,686 --> 0:30:29,906
你在另一个渲染器中使用 ARKit 

717
00:30:29,906 --> 0:30:31,646
那么我们将

718
00:30:31,646 --> 0:30:33,596
你将必须通过网络发送该数据

719
00:30:35,576 --> 0:30:37,756
此数据成为 AR 协作数据

720
00:30:39,006 --> 0:30:41,036
ARKit 可以在任何时间点

721
00:30:41,036 --> 0:30:43,226
创建 AR 协作数据包

722
00:30:43,226 --> 0:30:45,106
然后你必须再次

723
00:30:45,106 --> 0:30:47,526
转发给其他用户

724
00:30:47,886 --> 0:30:49,886
这不仅限于两个用户

725
00:30:49,886 --> 0:30:51,916
你可以在该会话中

726
00:30:51,916 --> 0:30:53,456
拥有大量用户

727
00:30:54,676 --> 0:30:57,676
在此过程中 ARKit 将

728
00:30:57,676 --> 0:30:59,196
生成额外的 AR  

729
00:30:59,196 --> 0:31:00,746
协作数据 你必须

730
00:30:59,196 --> 0:31:00,746
协作数据 你必须

731
00:31:00,746 --> 0:31:02,776
将这些数据转发到其他设备

732
00:31:02,776 --> 0:31:03,816
并广播这些数据

733
00:31:07,876 --> 0:31:10,006
让我们看看这在代码中是如何工作的

734
00:31:11,296 --> 0:31:13,236
因此 在本例中

735
00:31:13,236 --> 0:31:14,736
你首先需要设置你的多点连接

736
00:31:14,736 --> 0:31:17,236
或者你也可以设置

737
00:31:17,236 --> 0:31:19,716
任何框架 你选择的任何网络框架

738
00:31:19,716 --> 0:31:20,856
并确保你的设备

739
00:31:20,856 --> 0:31:21,866
共享相同的会话

740
00:31:22,336 --> 0:31:25,676
当它们执行时 你需要

741
00:31:25,676 --> 0:31:27,826
启用 ARWorldTrackingConfiguration

742
00:31:27,826 --> 0:31:30,396
并将 isCollaborationEnabled

743
00:31:31,706 --> 0:31:33,466
在这种情况下 你需要 

744
00:31:36,296 --> 0:31:40,196
此时 在代理下将为你提供一个新方法

745
00:31:40,196 --> 0:31:41,806
你将在其中

746
00:31:41,806 --> 0:31:44,296
接收一些协作数据

747
00:31:44,776 --> 0:31:49,066
收到这些数据后

748
00:31:49,066 --> 0:31:50,296
你需要确保将其在

749
00:31:50,296 --> 0:31:52,406
网络上广播给同时

750
00:31:52,726 --> 0:31:54,496
处于协作会话中的其他用户

751
00:31:54,976 --> 0:31:58,376
在其他设备上

752
00:31:58,826 --> 0:32:00,516
接收这些数据后 你需要

753
00:31:58,826 --> 0:32:00,516
接收这些数据后 你需要

754
00:32:00,516 --> 0:32:02,936
更新 URL 会话 以便它

755
00:32:02,936 --> 0:32:04,016
了解这些新数据

756
00:32:04,406 --> 0:32:05,566
就是这样的

757
00:32:07,436 --> 0:32:09,266
此协作数据

758
00:32:10,276 --> 0:32:12,456
自动交换所有

759
00:32:12,456 --> 0:32:14,226
用户创建的 ARAnchors

760
00:32:15,566 --> 0:32:18,126
每个锚点都可以

761
00:32:18,126 --> 0:32:20,506
通过会话 ID 识别 这样你就可以

762
00:32:20,506 --> 0:32:21,706
确保了解

763
00:32:21,706 --> 0:32:25,316
锚点来自哪个设备或者哪个 AR 会话

764
00:32:25,836 --> 0:32:28,426
如前所述

765
00:32:28,426 --> 0:32:30,386
ARParticipantAnchor 实时地

766
00:32:30,386 --> 0:32:32,626
展示参与者的

767
00:32:32,626 --> 0:32:34,586
位置 这在你的某些用例中

768
00:32:34,586 --> 0:32:35,706
非常方便

769
00:32:39,296 --> 0:32:48,546
这就是你创建协作会话的方式 [掌声]

770
00:32:49,046 --> 0:32:50,906
现在让我们来谈谈指导

771
00:32:51,636 --> 0:32:53,316
当你创建一种体验

772
00:32:53,556 --> 0:32:56,436
一种 AR 体验 指导非常重要 

773
00:32:57,056 --> 0:32:58,056
你真的想引导你的

774
00:32:58,056 --> 0:33:00,336
用户 不管他们是新用户

775
00:32:58,056 --> 0:33:00,336
用户 不管他们是新用户

776
00:33:00,336 --> 0:33:03,176
还是返回到你的 AR 体验中的用户

777
00:33:04,036 --> 0:33:05,256
这不是一个简单的过程

778
00:33:05,656 --> 0:33:07,216
有时候 对于你来说

779
00:33:07,216 --> 0:33:09,256
理解甚至指导用户获得新体验

780
00:33:09,256 --> 0:33:11,766
是很难的

781
00:33:13,136 --> 0:33:14,626
在整个过程中

782
00:33:14,626 --> 0:33:16,656
你必须对某些追踪事件做出反应

783
00:33:16,846 --> 0:33:19,216
有时追踪会受限 因为用户移动得

784
00:33:19,216 --> 0:33:21,836
过于快速

785
00:33:21,836 --> 0:33:23,266
到目前为止 我们一直在为你

786
00:33:23,266 --> 0:33:25,336
提供 Human Interface Guideline

787
00:33:25,976 --> 0:33:28,276
这允许你为用户引导体验

788
00:33:28,276 --> 0:33:30,166
提供一些指导

789
00:33:31,736 --> 0:33:33,346
今年 我们将它嵌入到

790
00:33:33,346 --> 0:33:34,836
UI 视图中

791
00:33:38,206 --> 0:33:39,906
我们将它称为 AR Coaching View

792
00:33:40,406 --> 0:33:43,746
这是一个内置的叠加层

793
00:33:43,746 --> 0:33:46,106
你可以直接嵌入到 AR App 中

794
00:33:46,746 --> 0:33:48,386
它可以引导你的用户

795
00:33:48,386 --> 0:33:49,646
获得非常好的追踪体验

796
00:33:50,176 --> 0:33:53,146
它为你的 App 提供一个

797
00:33:53,676 --> 0:33:55,096
连续的设计

798
00:33:55,096 --> 0:33:57,846
便于你的用户能够非常熟悉它

799
00:33:58,496 --> 0:34:00,726
实际上你可能之前已经看过这种设计

800
00:33:58,496 --> 0:34:00,726
实际上你可能之前已经看过这种设计

801
00:34:00,946 --> 0:34:03,496
我们有 AR 快速查看和测量

802
00:34:05,296 --> 0:34:07,906
这个新的 UI 叠加层

803
00:34:07,906 --> 0:34:09,906
会根据不同的追踪事件

804
00:34:09,906 --> 0:34:12,246
自动激活和停用

805
00:34:13,036 --> 0:34:15,576
而且你可以调整某些指导的目标

806
00:34:16,255 --> 0:34:17,686
让我们来看看其中的一些叠加层

807
00:34:17,686 --> 0:34:21,235
所以在 AR Coaching View 中 我们

808
00:34:21,985 --> 0:34:23,206
有多个叠加层

809
00:34:24,036 --> 0:34:25,846
用户引导 UI 使

810
00:34:25,846 --> 0:34:28,065
用户能够理解

811
00:34:28,065 --> 0:34:29,235
你正在寻找的内容 在

812
00:34:29,235 --> 0:34:30,246
本例中 是表面

813
00:34:30,835 --> 0:34:32,106
大多数情况下 你的体验

814
00:34:32,106 --> 0:34:34,235
需要表面来放置内容

815
00:34:34,235 --> 0:34:36,556
因此如果在你的配置中

816
00:34:36,556 --> 0:34:37,956
启用平面检测 则会

817
00:34:37,956 --> 0:34:39,946
自动显示此叠加层

818
00:34:41,335 --> 0:34:42,846
其次 我们有另外一个

819
00:34:42,846 --> 0:34:46,936
叠加层 使用户能够理解

820
00:34:46,936 --> 0:34:48,005
他们必须更多地移动

821
00:34:48,005 --> 0:34:50,746
以收集其他功能

822
00:34:50,746 --> 0:34:52,565
以便更好地进行追踪

823
00:34:53,216 --> 0:34:55,426
最后 我们还有

824
00:34:55,426 --> 0:34:57,536
另外一个叠加层 来帮助你的

825
00:34:57,536 --> 0:34:59,156
用户在某些特定环境下

826
00:34:59,156 --> 0:35:00,766
重新定位 以防你跟丢

827
00:34:59,156 --> 0:35:00,766
重新定位 以防你跟丢

828
00:35:00,766 --> 0:35:04,446
例如 如果 App 进入后台

829
00:35:05,066 --> 0:35:08,306
让我们来看一个例子

830
00:35:11,346 --> 0:35:12,876
因此 在这个例子中

831
00:35:12,876 --> 0:35:14,646
我们要求用户移动设备

832
00:35:14,646 --> 0:35:16,466
来找到一个新的平面

833
00:35:16,466 --> 0:35:17,886
一旦用户移动

834
00:35:17,886 --> 0:35:19,566
并收集更多的

835
00:35:19,566 --> 0:35:21,636
功能 那么内容就会

836
00:35:21,706 --> 0:35:24,116
被放置 视图就会自动关闭

837
00:35:24,596 --> 0:35:26,086
所以你不用做任何事情

838
00:35:26,086 --> 0:35:32,966
一切都是自动处理 [掌声]

839
00:35:33,466 --> 0:35:36,646
让我们来看看如何设置

840
00:35:37,376 --> 0:35:38,886
同样这很容易

841
00:35:39,636 --> 0:35:41,686
因为它是一个简单的 UI 视图

842
00:35:41,686 --> 0:35:43,306
所以你必须将其设置为另一个

843
00:35:43,346 --> 0:35:44,506
UI 视图的子视图

844
00:35:44,766 --> 0:35:46,386
理想情况下 你将其设置为

845
00:35:46,386 --> 0:35:47,826
AR 视图的子级

846
00:35:48,436 --> 0:35:50,676
然后 你需要将此会话

847
00:35:50,676 --> 0:35:53,036
连接到指导视图

848
00:35:53,036 --> 0:35:54,076
以便指导视图知道要对

849
00:35:54,076 --> 0:35:57,386
哪些事情做出反应

850
00:35:57,386 --> 0:35:58,296
或者如果你正在使用

851
00:35:58,296 --> 0:36:00,866
故事板 则需要将教指导

852
00:35:58,296 --> 0:36:00,866
故事板 则需要将教指导

853
00:36:00,866 --> 0:36:02,016
视图的会话提供程序

854
00:36:02,016 --> 0:36:04,326
出口连接到

855
00:36:04,326 --> 0:36:07,556
会话提供程序本身

856
00:36:07,626 --> 0:36:09,416
或者 如果你想

857
00:36:09,416 --> 0:36:10,556
对视图提供给你的某些事件做出反应

858
00:36:10,556 --> 0:36:13,156
可以设置一组委托

859
00:36:13,686 --> 0:36:18,146
最后 如果你想

860
00:36:18,146 --> 0:36:19,706
禁用某些功能

861
00:36:19,706 --> 0:36:21,606
还可以提供一组

862
00:36:21,606 --> 0:36:23,166
特定的指导目标

863
00:36:23,686 --> 0:36:26,976
让我们看看其中的一些委托

864
00:36:30,296 --> 0:36:32,666
因此我们在 AR Coaching View 方面 

865
00:36:32,666 --> 0:36:35,346
有三种新方法 CoachingOverlayViewDelegate

866
00:36:35,826 --> 0:36:37,146
其中两个可以对

867
00:36:37,146 --> 0:36:38,516
激活和停用做出反应

868
00:36:38,516 --> 0:36:40,476
因此你可以选择

869
00:36:40,476 --> 0:36:41,636
在整个体验过程中

870
00:36:41,636 --> 0:36:43,016
是否仍需要启用该功能

871
00:36:43,016 --> 0:36:45,376
或者你认为 例如 如果该用户曾经

872
00:36:45,466 --> 0:36:47,696
用过此功能 则不需要再了解此功能

873
00:36:49,106 --> 0:36:50,916
此外 你可以自动设置

874
00:36:50,916 --> 0:36:52,866
UI 视图 对某些重新定位中止

875
00:36:52,866 --> 0:36:56,946
请求做出响应

876
00:36:56,946 --> 0:36:58,506
指导视图实际上

877
00:36:58,506 --> 0:37:01,106
会为你和你的用户提供一个

878
00:36:58,506 --> 0:37:01,106
会为你和你的用户提供一个

879
00:37:01,106 --> 0:37:02,946
新的 UI 界面 在这里他们   

880
00:37:03,066 --> 0:37:04,266
可以重新定位并且

881
00:37:04,266 --> 0:37:05,636
重新启动会话或者

882
00:37:05,636 --> 0:37:06,786
重置追踪

883
00:37:07,806 --> 0:37:09,676
因此这个新的视图

884
00:37:09,756 --> 0:37:11,746
对于你的 App 来说非常方便 

885
00:37:11,746 --> 0:37:12,596
这样你可以确保获得

886
00:37:12,596 --> 0:37:14,676
一致的设计并帮助你的用户 

887
00:37:15,306 --> 0:37:18,596
现在我们来谈谈面部追踪

888
00:37:19,316 --> 0:37:21,996
在 ARKit 1 中 我们启用面部

889
00:37:21,996 --> 0:37:24,386
追踪功能 可以来追踪一张脸

890
00:37:25,396 --> 0:37:27,456
在 ARKit 2 中 我们能够

891
00:37:27,456 --> 0:37:28,786
追踪多张脸

892
00:37:28,786 --> 0:37:31,516
能够同时追踪三张脸

893
00:37:32,536 --> 0:37:36,476
此外 你还可以

894
00:37:36,476 --> 0:37:38,196
确保在此人离开框架

895
00:37:38,196 --> 0:37:39,576
并再次返回时

896
00:37:39,576 --> 0:37:41,466
识别出他

897
00:37:41,466 --> 0:37:52,546
并再次为你提供相同的面部锚点 ID [掌声]

898
00:37:53,046 --> 0:37:55,796
因此多人面部追踪可以

899
00:37:55,796 --> 0:37:59,036
同时追踪三张脸

900
00:37:59,186 --> 0:38:00,826
并为你提供一个持久的

901
00:37:59,186 --> 0:38:00,826
并为你提供一个持久的

902
00:38:00,826 --> 0:38:02,986
面部锚点 ID 这样你就

903
00:38:02,986 --> 0:38:04,496
可以确保在整个会话中

904
00:38:04,496 --> 0:38:05,376
识别出一个用户

905
00:38:06,016 --> 0:38:07,616
如果你重新启动一个会话

906
00:38:08,436 --> 0:38:09,846
那么这个 ID 就会消失

907
00:38:09,846 --> 0:38:11,206
并出现一个新的会话 

908
00:38:12,536 --> 0:38:14,776
要实现这一点 其实非常简单

909
00:38:15,446 --> 0:38:17,036
我们在 ARFaceTrackingConfiguration 方面  

910
00:38:17,036 --> 0:38:18,856
有两个新属性

911
00:38:20,206 --> 0:38:21,386
第一个允许你

912
00:38:21,386 --> 0:38:23,756
查询在该特定设备上的

913
00:38:23,756 --> 0:38:25,386
一个会话中可以同时

914
00:38:25,386 --> 0:38:27,286
追踪多少面孔

915
00:38:27,846 --> 0:38:29,416
另一个允许你

916
00:38:29,416 --> 0:38:31,046
设置你想要同时追踪的

917
00:38:31,096 --> 0:38:32,356
追踪面数

918
00:38:32,856 --> 0:38:40,956
这就是多面部追踪 [掌声]

919
00:38:41,456 --> 0:38:43,096
让我们来讨论一种新的

920
00:38:43,246 --> 0:38:45,306
追踪配置 我们

921
00:38:45,806 --> 0:38:48,556
称它为 ARPositional

922
00:38:49,486 --> 0:38:50,606
所以这个新的追踪

923
00:38:50,606 --> 0:38:53,416
配置只用于

924
00:38:53,416 --> 0:38:54,906
追踪用例

925
00:38:55,846 --> 0:38:58,096
比如你经常有这样

926
00:38:58,096 --> 0:39:00,066
一个用例 它实际上不需要

927
00:38:58,096 --> 0:39:00,066
一个用例 它实际上不需要

928
00:39:00,096 --> 0:39:01,846
渲染相机背景

929
00:39:03,206 --> 0:39:05,146
嗯 这是针对该用例制作的

930
00:39:06,376 --> 0:39:07,816
我们可以通过将

931
00:39:07,816 --> 0:39:09,956
渲染速率保持在 60 赫兹 

932
00:39:09,956 --> 0:39:13,006
来降低捕获帧速率和

933
00:39:13,006 --> 0:39:15,426
相机分辨率

934
00:39:15,426 --> 0:39:18,546
从而实现低能耗

935
00:39:21,956 --> 0:39:24,536
接下来 让我们谈谈

936
00:39:24,566 --> 0:39:25,376
我们今年所做的

937
00:39:25,376 --> 0:39:28,636
一些场景理解方面的改进

938
00:39:29,966 --> 0:39:31,166
图像检测和图像

939
00:39:31,166 --> 0:39:32,216
追踪已经存在了

940
00:39:32,216 --> 0:39:33,366
一段时间

941
00:39:34,086 --> 0:39:35,816
今年我们可以同时

942
00:39:35,816 --> 0:39:38,056
检测多达 100 个图像  

943
00:39:38,616 --> 0:39:42,226
例如 我们还为你提供

944
00:39:42,316 --> 0:39:44,316
检测打印

945
00:39:44,316 --> 0:39:46,306
图像比例的功能

946
00:39:47,386 --> 0:39:49,016
通常 当新的 App  

947
00:39:49,016 --> 0:39:51,346
要求用户使用图像

948
00:39:51,346 --> 0:39:52,696
来放置内容并

949
00:39:52,696 --> 0:39:55,216
相应地缩放内容时 图像

950
00:39:55,216 --> 0:39:57,456
可能会以不同的大小或者

951
00:39:57,456 --> 0:39:58,726
不同的纸张大小进行打印

952
00:39:59,426 --> 0:40:01,086
通过这种自动比例

953
00:39:59,426 --> 0:40:01,086
通过这种自动比例

954
00:40:01,086 --> 0:40:03,516
估算 你现在可以

955
00:40:03,516 --> 0:40:06,736
检测物理尺寸并相应地调整比例

956
00:40:06,736 --> 0:40:10,786
当你想要创建一个

957
00:40:10,786 --> 0:40:14,356
新的 AR 参考图像时

958
00:40:14,356 --> 0:40:15,666
我们还能够在

959
00:40:15,666 --> 0:40:17,196
运行时查询你传递给 ARKit 的  

960
00:40:17,196 --> 0:40:19,096
图像的质量

961
00:40:21,516 --> 0:40:23,376
我们还改进了

962
00:40:23,376 --> 0:40:24,806
我们的目标检测算法

963
00:40:25,966 --> 0:40:27,756
通过机器学习 我们可以

964
00:40:27,756 --> 0:40:29,536
提高目标检测算法

965
00:40:29,536 --> 0:40:34,896
为你提供更快的识别 以及

966
00:40:34,896 --> 0:40:36,626
更强大的环境

967
00:40:36,626 --> 0:40:38,006
在更多不同的环境中

968
00:40:38,326 --> 0:40:39,976
通常 你必须扫描

969
00:40:40,286 --> 0:40:42,346
环境中的特定对象 以便它在

970
00:40:42,346 --> 0:40:43,456
另一个环境中完美工作

971
00:40:44,226 --> 0:40:45,586
现在 这些都更灵活了

972
00:40:48,456 --> 0:40:51,166
最后 场景理解的

973
00:40:51,166 --> 0:40:52,246
另一个重要方面是

974
00:40:52,246 --> 0:40:54,616
平面估算

975
00:40:55,376 --> 0:40:56,536
通常 你需要平面

976
00:40:56,536 --> 0:40:58,706
估算来放置内容

977
00:40:58,856 --> 0:40:59,926
通过机器学习

978
00:41:00,476 --> 0:41:01,686
我们实际上使它

979
00:41:01,686 --> 0:41:04,006
更精确 我们使它

980
00:41:04,006 --> 0:41:05,956
更强大 使它更快地

981
00:41:05,956 --> 0:41:06,996
检测到平面

982
00:41:07,566 --> 0:41:10,356
让我们来看一个例子

983
00:41:10,986 --> 0:41:16,406
通过机器学习

984
00:41:16,406 --> 0:41:18,756
我们不仅在检测到

985
00:41:18,756 --> 0:41:19,956
这些特征时在地面上

986
00:41:19,956 --> 0:41:21,886
扩展这些平面 而且实际上

987
00:41:21,886 --> 0:41:23,466
流动的范围更广

988
00:41:23,746 --> 0:41:26,056
但是我们也有

989
00:41:26,056 --> 0:41:27,526
检测的能力

990
00:41:27,526 --> 0:41:29,676
当我们现在找不到特征时

991
00:41:30,026 --> 0:41:31,676
我们也有检测侧壁的能力

992
00:41:32,166 --> 0:41:41,376
这些都是机器学习的功劳 [掌声]

993
00:41:41,876 --> 0:41:44,056
正如你在这里看到的

994
00:41:44,056 --> 0:41:45,146
正如可以在上一个视频中看到的

995
00:41:45,146 --> 0:41:46,646
我们在平面上

996
00:41:46,696 --> 0:41:48,326
有好多种分类

997
00:41:49,026 --> 0:41:50,886
好吧 这又是通过机器学习来完成的

998
00:41:50,996 --> 0:41:52,336
去年 我们推出了

999
00:41:52,816 --> 0:41:54,446
五种不同的分类

1000
00:41:55,136 --> 0:41:57,366
墙壁 地板 天花板 桌子和座椅

1001
00:41:58,466 --> 0:41:59,806
今年 我们又推出了

1002
00:41:59,936 --> 0:42:01,026
额外两种

1003
00:41:59,936 --> 0:42:01,026
额外两种

1004
00:42:01,406 --> 0:42:03,106
我们如今增加了

1005
00:42:03,106 --> 0:42:05,886
检测门和窗的能力

1006
00:42:07,296 --> 0:42:10,676
如前所述 平面分类真的非常重要

1007
00:42:10,676 --> 0:42:12,526
或者平面估算

1008
00:42:12,526 --> 0:42:13,566
对于在世界上放置内容

1009
00:42:13,566 --> 0:42:14,696
非常重要

1010
00:42:14,696 --> 0:42:17,256
这实际上是目标放置的理想选择

1011
00:42:17,526 --> 0:42:19,566
你总是将你的对象放在一个表面上

1012
00:42:20,896 --> 0:42:21,916
今年有了新的

1013
00:42:21,916 --> 0:42:25,126
光线投射 API 你甚至可以

1014
00:42:25,126 --> 0:42:26,556
更容易地将你的内容

1015
00:42:26,746 --> 0:42:29,506
放置地更加准确 更加灵活

1016
00:42:30,866 --> 0:42:32,956
它支持任何形式的表明对齐

1017
00:42:33,286 --> 0:42:34,566
所以你不必再局限于

1018
00:42:34,566 --> 0:42:36,116
垂直和水平方向了

1019
00:42:38,156 --> 0:42:40,966
而且 你可以随时追踪你的光线投射

1020
00:42:42,716 --> 0:42:44,466
这就意味着 当 ARKit 或者当

1021
00:42:44,466 --> 0:42:45,486
你移动你的设备时

1022
00:42:45,486 --> 0:42:47,066
ARKit 会检测到更多

1023
00:42:47,066 --> 0:42:49,456
关于环境的信息 它可以

1024
00:42:49,666 --> 0:42:51,486
准确地把你的物体

1025
00:42:51,486 --> 0:42:52,986
放到物理表面的上边

1026
00:42:52,986 --> 0:42:54,026
比如那些平面正在展开

1027
00:42:54,546 --> 0:42:57,806
让我们来看看如何在 ARKit 中启用它

1028
00:43:00,336 --> 0:43:02,816
听起来我们正在创建一个光线投射查询

1029
00:43:03,526 --> 0:43:06,466
一个光线投射查询有三个参数

1030
00:43:06,596 --> 0:43:08,096
第一个决定

1031
00:43:08,096 --> 0:43:09,536
你想要进行光线投射的位置

1032
00:43:10,106 --> 0:43:11,246
在此示例中 我们从屏幕中心

1033
00:43:11,246 --> 0:43:12,266
开始执行此操作的

1034
00:43:12,266 --> 0:43:15,896
然后你需要告诉它

1035
00:43:16,126 --> 0:43:18,686
你想要允许的内容

1036
00:43:18,686 --> 0:43:21,366
以便放置该内容或恢复转换

1037
00:43:21,936 --> 0:43:25,406
此外 你还需要告诉它

1038
00:43:25,406 --> 0:43:26,566
你想要哪种对齐方式

1039
00:43:26,746 --> 0:43:30,686
它可以是水平的 垂直的或者任何

1040
00:43:32,616 --> 0:43:34,756
然后你需要在你的

1041
00:43:35,076 --> 0:43:36,616
AR 会话中将该查询传递给 

1042
00:43:36,616 --> 0:43:38,206
trackedRaycast 方法

1043
00:43:40,056 --> 0:43:41,976
这个方法有一个回调

1044
00:43:41,976 --> 0:43:43,836
它允许你对

1045
00:43:43,836 --> 0:43:45,406
新的转换做出反应 并使用

1046
00:43:45,406 --> 0:43:47,746
该光线投射给你结果 这样

1047
00:43:47,746 --> 0:43:49,996
你就可以相应地调整内容或者锚点

1048
00:43:50,456 --> 0:43:55,696
最后 当你完成这个光线投射 你就可以停下

1049
00:43:56,346 --> 0:44:00,236
这些是一些场景 这些是今年

1050
00:43:56,346 --> 0:44:00,236
这些是一些场景 这些是今年

1051
00:44:00,236 --> 0:44:02,466
我们在光线投射方面所做的改进

1052
00:44:07,256 --> 0:44:08,666
让我们继续我们

1053
00:44:08,666 --> 0:44:11,346
在视觉连贯性方面的增强

1054
00:44:12,216 --> 0:44:16,456
所以今年 我们有了这个新的

1055
00:44:16,456 --> 0:44:18,696
ARView 允许你 

1056
00:44:18,696 --> 0:44:19,986
根据需要激活和停用

1057
00:44:20,216 --> 0:44:22,456
不同的渲染选项

1058
00:44:23,396 --> 0:44:26,876
它还可以根据你的设备功能

1059
00:44:26,876 --> 0:44:29,226
自动停用和激活

1060
00:44:30,076 --> 0:44:31,936
让我们看一下这个例子

1061
00:44:32,266 --> 0:44:35,496
你可能之前已经看过这个视频

1062
00:44:35,496 --> 0:44:38,426
看看飞行器是如何移动的

1063
00:44:38,426 --> 0:44:39,646
物体是如何在表面上

1064
00:44:39,646 --> 0:44:42,066
移动的 所有这些看起来

1065
00:44:42,066 --> 0:44:43,646
都很真实

1066
00:44:44,276 --> 0:44:45,716
当一切都消失 你

1067
00:44:47,256 --> 0:44:49,156
实际上甚至没有意识到

1068
00:44:49,156 --> 0:44:50,176
那些物体是虚拟的

1069
00:44:50,666 --> 0:44:52,726
让我们看一下

1070
00:44:52,726 --> 0:44:55,196
我们所做的一些视觉连贯性的增强

1071
00:44:55,736 --> 0:44:57,956
让我们再看看

1072
00:44:57,956 --> 0:44:59,396
这个视频的开头 

1073
00:44:59,396 --> 0:45:00,936
让这些球在桌子上滚动时

1074
00:44:59,396 --> 0:45:00,936
让这些球在桌子上滚动时

1075
00:45:00,936 --> 0:45:02,986
暂停一下

1076
00:45:04,376 --> 0:45:05,926
这里你可以看到

1077
00:45:06,006 --> 0:45:08,256
景深效果

1078
00:45:08,256 --> 0:45:09,736
这是 RealityKit 的一个新功能

1079
00:45:11,226 --> 0:45:13,216
你的 AR 体验专为

1080
00:45:13,216 --> 0:45:15,746
大型和小型房间而设计

1081
00:45:16,316 --> 0:45:19,196
你 iOS 设备上的相机

1082
00:45:19,196 --> 0:45:20,696
总是根据环境调整焦距

1083
00:45:20,696 --> 0:45:23,266
而景深功能

1084
00:45:23,266 --> 0:45:25,436
允许调整对

1085
00:45:25,436 --> 0:45:26,776
虚拟内容的焦距

1086
00:45:26,776 --> 0:45:29,596
使其与你的物理内容完美匹配

1087
00:45:29,596 --> 0:45:31,626
从而使对象

1088
00:45:31,676 --> 0:45:36,306
在环境中完美融合

1089
00:45:36,306 --> 0:45:37,416
此外 当你快速

1090
00:45:37,416 --> 0:45:39,026
移动相机或物体

1091
00:45:39,026 --> 0:45:41,566
快速移动时 你可以看到

1092
00:45:41,566 --> 0:45:43,356
会出现模糊

1093
00:45:44,496 --> 0:45:46,516
好吧 大部分时间

1094
00:45:46,516 --> 0:45:48,626
你有一个常规渲染器

1095
00:45:48,626 --> 0:45:50,986
并没有运动模糊效果

1096
00:45:50,986 --> 0:45:52,896
那么你的虚拟内容就会很突出

1097
00:45:54,346 --> 0:45:56,436
它并没有真正融入环境

1098
00:45:57,096 --> 0:45:58,356
由于 VIO 相机

1099
00:45:58,356 --> 0:46:01,226
运动和参数感

1100
00:45:58,356 --> 0:46:01,226
运动和参数感

1101
00:46:01,226 --> 0:46:06,116
我们可以合成运动模糊

1102
00:46:06,436 --> 0:46:07,746
我们可以将它应用于

1103
00:46:07,746 --> 0:46:10,506
可视对象 以便它在你的

1104
00:46:10,586 --> 0:46:11,696
环境中完美融合

1105
00:46:11,696 --> 0:46:15,506
这是 ARSCNView 和

1106
00:46:15,506 --> 0:46:16,596
ARView 上的一个变量

1107
00:46:16,716 --> 0:46:20,446
让我们再看一下这个例子

1108
00:46:20,446 --> 0:46:21,876
这里的一切看起来都很好

1109
00:46:55,616 --> 0:46:58,646
今年我们为增强视觉一致性而

1110
00:46:58,646 --> 0:47:00,306
提供的另外两个 API 是

1111
00:46:58,646 --> 0:47:00,306
提供的另外两个 API 是

1112
00:47:00,616 --> 0:47:03,756
HDR 环境纹理和相机颗粒

1113
00:47:06,286 --> 0:47:07,996
放置内容时

1114
00:47:07,996 --> 0:47:08,956
你真的希望该内容能够

1115
00:47:08,956 --> 0:47:10,026
反映现实世界

1116
00:47:11,296 --> 0:47:13,206
有了高动态范围 

1117
00:47:13,986 --> 0:47:16,226
即使在明亮的

1118
00:47:16,226 --> 0:47:17,236
光线环境中

1119
00:47:17,356 --> 0:47:18,746
你也可以捕捉那些突出的或者更高的高光

1120
00:47:19,376 --> 0:47:21,076
使你的内容更有活力

1121
00:47:22,626 --> 0:47:23,846
今年使用 ARKit 我们

1122
00:47:23,846 --> 0:47:25,386
实际上可以请求这些

1123
00:47:25,386 --> 0:47:26,666
HDR 环境纹理

1124
00:47:26,666 --> 0:47:28,576
以便你的内容看起来更好 

1125
00:47:29,896 --> 0:47:32,846
此外 我们还有一个相机颗粒 API

1126
00:47:33,786 --> 0:47:36,236
你可能会注意到

1127
00:47:36,236 --> 0:47:38,026
当你在一个光线很暗的环境中

1128
00:47:38,026 --> 0:47:39,506
有一个 AR 体验

1129
00:47:39,806 --> 0:47:41,856
与相机相比 

1130
00:47:42,176 --> 0:47:43,046
其他内容看起来非常闪亮

1131
00:47:44,906 --> 0:47:46,476
每个相机都会产生一些颗粒

1132
00:47:46,786 --> 0:47:48,056
尤其在很暗的环境下

1133
00:47:48,126 --> 0:47:51,216
颗粒感看起来更重

1134
00:47:51,856 --> 0:47:53,336
使用这种新的相机颗粒

1135
00:47:53,336 --> 0:47:55,906
API 我们可以确保 

1136
00:47:56,716 --> 0:47:58,686
在你的虚拟内容上

1137
00:47:58,686 --> 0:48:00,576
应用相同的颗粒图案 以便它

1138
00:47:58,686 --> 0:48:00,576
应用相同的颗粒图案 以便它

1139
00:48:00,576 --> 0:48:03,386
很好地融合而且不会非常突出

1140
00:48:04,516 --> 0:48:06,296
所以这些是今年

1141
00:48:07,916 --> 0:48:10,966
但是我们并没有就此止步

1142
00:48:11,776 --> 0:48:13,156
我认为很多人都想要

1143
00:48:13,156 --> 0:48:14,456
这样一个功能

1144
00:48:16,146 --> 0:48:17,076
当你开发一种 AR

1145
00:48:17,076 --> 0:48:19,076
体验时 你总有

1146
00:48:19,116 --> 0:48:21,506
或者经常为了原型

1147
00:48:21,506 --> 0:48:23,236
或者测试你的体验去

1148
00:48:23,236 --> 0:48:24,266
一个特定的地方

1149
00:48:26,356 --> 0:48:28,386
大多数时候 当你去那里的时候

1150
00:48:28,386 --> 0:48:29,386
你会回到你的

1151
00:48:29,386 --> 0:48:30,186
桌子 你开发你的体验

1152
00:48:30,186 --> 0:48:31,696
你想要再回来

1153
00:48:31,866 --> 0:48:34,216
今年使用 Reality

1154
00:48:34,216 --> 0:48:36,876
Composer App 你现在可以记录

1155
00:48:36,966 --> 0:48:38,976
体验或者顺序

1156
00:48:40,076 --> 0:48:41,216
这意味着你可以去

1157
00:48:41,266 --> 0:48:42,936
你最喜欢的地方 在那里

1158
00:48:43,006 --> 0:48:44,556
你将通过捕捉环境

1159
00:48:44,556 --> 0:48:48,206
来获得体验

1160
00:48:48,706 --> 0:48:50,376
ARKit 将确保将 

1161
00:48:50,376 --> 0:48:52,596
传感器数据和视频流一起

1162
00:48:52,596 --> 0:48:56,436
保存到电影文件

1163
00:48:56,436 --> 0:48:58,906
容器中 这样你就可以

1164
00:48:58,996 --> 0:49:02,946
随身携带并将其放入 Xcode

1165
00:48:58,996 --> 0:49:02,946
随身携带并将其放入 Xcode

1166
00:49:03,076 --> 0:49:06,256
此时 Xcode 方案设置

1167
00:49:06,256 --> 0:49:08,156
像一个新的

1168
00:49:08,156 --> 0:49:10,126
附加功能或一个

1169
00:49:10,226 --> 0:49:13,456
允许你选择该文件的新字段

1170
00:49:16,106 --> 0:49:18,336
选择该文件

1171
00:49:18,336 --> 0:49:19,976
然后在连接到 Xcode 的

1172
00:49:19,976 --> 0:49:23,906
设备上按运行 然后

1173
00:49:24,046 --> 0:49:26,316
你可以在桌面上重播该体验

1174
00:49:26,936 --> 0:49:28,636
这是原型设计的理想选择

1175
00:49:29,256 --> 0:49:31,336
更适合使用

1176
00:49:31,336 --> 0:49:32,736
不同参数调整

1177
00:49:32,736 --> 0:49:34,136
AR 配置 并尝试

1178
00:49:34,136 --> 0:49:35,526
查看体验的外观

1179
00:49:36,076 --> 0:49:46,456
你甚至可以对某些追踪做出反应 [掌声]

1180
00:49:46,956 --> 0:49:48,376
这很棒

1181
00:49:48,376 --> 0:49:50,136
我想今年在 ARKit 3 中

1182
00:49:50,136 --> 0:49:51,776
你将会有很多不同的工具

1183
00:49:51,776 --> 0:49:54,456
你可以通过协作会话 

1184
00:49:54,456 --> 0:49:56,296
多面部追踪

1185
00:49:56,356 --> 0:49:59,246
来增强你的多用户体验

1186
00:50:00,096 --> 0:50:02,356
你可以通过

1187
00:50:02,406 --> 0:50:03,986
RealityKit 的新的一致性效果

1188
00:50:03,986 --> 0:50:06,746
以及 ARWorldTrackingConfiguration   

1189
00:50:06,746 --> 0:50:08,676
上的新视觉效果来增强

1190
00:50:08,676 --> 0:50:10,926
所有 AR App 的真实感

1191
00:50:12,396 --> 0:50:13,916
你可以通过新的运动捕捉

1192
00:50:14,996 --> 0:50:18,086
以及同时使用前置和后置摄像头来

1193
00:50:18,086 --> 0:50:20,396
启用新的用例

1194
00:50:21,536 --> 0:50:23,396
当然 在现有功能的基础上

1195
00:50:23,396 --> 0:50:26,416
还有很多改进

1196
00:50:26,756 --> 0:50:28,526
例如 使用目标

1197
00:50:28,526 --> 0:50:29,676
检测和机器学习

1198
00:50:30,266 --> 0:50:33,456
至少 最后但并非最不重要的是

1199
00:50:33,456 --> 0:50:36,676
我认为记录和重播

1200
00:50:36,676 --> 0:50:38,446
工作流程将使

1201
00:50:38,446 --> 0:50:39,966
你的设计和原型

1202
00:50:39,966 --> 0:50:41,716
体验设计比以前更好

1203
00:50:41,716 --> 0:50:44,956
所以我真的很期待

1204
00:50:44,956 --> 0:50:46,296
你能去网站上

1205
00:50:46,296 --> 0:50:47,526
下载我们的样品

1206
00:50:49,036 --> 0:50:50,556
我们还有几个实验室

1207
00:50:50,946 --> 0:50:52,476
一个在明天

1208
00:50:52,576 --> 0:50:54,416
一个在周四 我希望

1209
00:50:54,416 --> 0:50:55,466
你能来 跟我们说说

1210
00:50:55,526 --> 0:50:58,016
你有啥问题 或者只是单纯聊聊天 

1211
00:50:59,166 --> 0:51:01,136
然后我们还有两个

1212
00:50:59,166 --> 0:51:01,136
然后我们还有两个

1213
00:51:01,136 --> 0:51:03,016
深度会议 一个是关于

1214
00:51:03,016 --> 0:51:04,856
将人们带入到

1215
00:51:04,856 --> 0:51:07,046
AR 中 我们将更多的讨论 

1216
00:51:07,046 --> 0:51:08,166
人物遮挡和运动捕捉

1217
00:51:09,086 --> 0:51:12,146
第二个是关于协作 AR 体验

1218
00:51:12,806 --> 0:51:15,946
我希望你能享受会议的剩下部分

1219
00:51:17,336 --> 0:51:18,096
祝你有美好的一天

1220
00:51:18,416 --> 0:51:22,500
再见 [掌声]
