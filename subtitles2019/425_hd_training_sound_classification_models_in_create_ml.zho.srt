1
00:00:06,640 --> 0:00:11,044
（在Create ML中

2
00:00:13,881 --> 0:00:14,715
早上好

3
00:00:15,549 --> 0:00:16,950
我是Dan Klingler

4
00:00:17,217 --> 0:00:20,254
我是Apple音效团队的一名

5
00:00:20,554 --> 0:00:22,990
今天 我很高兴有机会能和你们聊聊

6
00:00:23,056 --> 0:00:26,627
在Create ML中

7
00:00:28,795 --> 0:00:30,330
在我们开始之前

8
00:00:30,664 --> 0:00:34,334
你可能想知道什么是声音分类

9
00:00:34,535 --> 0:00:37,271
在你的app中有什么用处？

10
00:00:39,039 --> 0:00:42,609
声音分类是一项录制一段声音

11
00:00:42,676 --> 0:00:45,345
然后把它分类到不同的类别的技术

12
00:00:46,547 --> 0:00:47,781
但仔细想一想

13
00:00:47,848 --> 0:00:50,851
我们有很多种分类声音的方式

14
00:00:52,486 --> 0:00:55,956
第一种是发声体

15
00:00:56,023 --> 0:00:57,391
举个例子

16
00:00:57,457 --> 0:01:00,460
我们有吉他或鼓的声音

17
00:00:57,457 --> 0:01:00,460
我们有吉他或鼓的声音

18
00:01:01,028 --> 0:01:04,364
不同的物件有不一样的声学特性

19
00:01:04,431 --> 0:01:07,668
我们人类因此能分辨不同的声音

20
00:01:09,469 --> 0:01:12,472
第二种声音分类的方式

21
00:01:12,940 --> 0:01:14,808
是声音的源头

22
00:01:15,142 --> 0:01:16,977
如果你有徒步过

23
00:01:17,044 --> 0:01:18,779
或者在繁华城市中心

24
00:01:19,613 --> 0:01:22,816
你能分辨出你周围声音的纹理

25
00:01:22,883 --> 0:01:24,084
是非常不一样的

26
00:01:24,585 --> 0:01:26,587
即使没有任何

27
00:01:26,653 --> 0:01:28,222
脱颖而出的声音

28
00:01:31,425 --> 0:01:34,261
第三种声音分类的方式

29
00:01:34,895 --> 0:01:36,930
是声音的属性

30
00:01:36,997 --> 0:01:38,866
或是声音的特性

31
00:01:39,132 --> 0:01:43,804
举个例子

32
00:01:44,071 --> 0:01:46,139
都来自相同的源头

33
00:01:46,406 --> 0:01:48,675
但是声音的特性却非常不同

34
00:01:48,742 --> 0:01:52,112
它允许我们分辨出这些声音的不同

35
00:01:54,448 --> 0:01:56,216
现在 作为app的开发者

36
00:01:56,550 --> 0:01:58,151
你有不同的app

37
00:01:58,218 --> 0:02:01,755
对于声音分类 你可能有

38
00:01:58,218 --> 0:02:01,755
对于声音分类 你可能有

39
00:02:02,856 --> 0:02:06,326
如果你能训练你自己的模型

40
00:02:06,393 --> 0:02:09,463
不是很棒吗？

41
00:02:11,632 --> 0:02:13,300
现在使用Xcode中

42
00:02:13,367 --> 0:02:16,703
的Create ML app

43
00:02:17,137 --> 0:02:21,141
这是训练声音分类模型最简单的方式

44
00:02:23,510 --> 0:02:25,546
创建一个声音分类器

45
00:02:25,612 --> 0:02:29,516
你需要为Create ML

46
00:02:29,583 --> 0:02:31,518
音频文件

47
00:02:33,086 --> 0:02:36,723
接下来Create ML

48
00:02:36,790 --> 0:02:38,492
开始训练声音分类器模型

49
00:02:40,060 --> 0:02:42,596
然后 你就能在你的app中

50
00:02:42,663 --> 0:02:44,731
使用这个声音分类器

51
00:02:45,832 --> 0:02:49,102
今天 我来用一个例子

52
00:02:49,169 --> 0:02:50,370
向你展示如何实现

53
00:03:00,747 --> 0:03:04,084
首先 我将打开

54
00:03:04,151 --> 0:03:07,054
它是Xcode中附带安装的

55
00:03:09,289 --> 0:03:11,325
我们将会创建一个新的文件

56
00:03:12,326 --> 0:03:14,661
从模板选择中

57
00:03:18,198 --> 0:03:19,466
点击“下一步”

58
00:03:19,933 --> 0:03:22,870
将我们的项目取名为

59
00:03:24,304 --> 0:03:26,874
保存这个项目

60
00:03:28,675 --> 0:03:30,410
当Create ML app

61
00:03:30,477 --> 0:03:32,112
你会看到主界面

62
00:03:32,446 --> 0:03:34,882
左侧的输入标签被选中

63
00:03:36,083 --> 0:03:39,453
为了训练我们自定义的模型

64
00:03:39,520 --> 0:03:41,522
Create ML app的入口

65
00:03:43,323 --> 0:03:46,460
你可以看到上方还有其他的标签

66
00:03:46,527 --> 0:03:49,062
比如训练验证和测试

67
00:03:49,429 --> 0:03:51,665
在训练的各个阶段

68
00:03:51,732 --> 0:03:55,235
它们将为我们的模型

69
00:03:56,570 --> 0:03:57,638
最后

70
00:03:57,704 --> 0:04:00,274
我们可以在输出标签里

71
00:03:57,704 --> 0:04:00,274
我们可以在输出标签里

72
00:04:00,340 --> 0:04:01,742
模型

73
00:04:02,476 --> 0:04:05,312
我们可以实时查看我们的模型

74
00:04:06,780 --> 0:04:10,450
现在 我将开始训练一个乐器分类器

75
00:04:10,851 --> 0:04:13,487
我带来了一些乐器

76
00:04:14,821 --> 0:04:18,058
我有一个TrainingData文件夹

77
00:04:18,125 --> 0:04:20,827
这是我收集的一些声音文件

78
00:04:22,396 --> 0:04:27,034
举个例子 它们包括

79
00:04:27,367 --> 0:04:30,037
吉他 牛铃和振动筛

80
00:04:32,005 --> 0:04:33,340
为了训练我们的模型

81
00:04:33,407 --> 0:04:34,608
我们只需

82
00:04:34,675 --> 0:04:37,978
将文件夹直接拖到

83
00:04:40,013 --> 0:04:41,548
Create ML检测出

84
00:04:41,615 --> 0:04:45,152
我们今天将会使用的声音文件

85
00:04:45,485 --> 0:04:48,422
有7种不同的类别

86
00:04:50,624 --> 0:04:53,160
我们点击“训练”按钮

87
00:04:53,227 --> 0:04:54,995
我们的模型开始训练了

88
00:04:55,796 --> 0:04:58,432
Create ML在训练模型时

89
00:04:58,498 --> 0:05:00,000
首先会做的事情是

90
00:05:00,067 --> 0:05:03,136
扫描我们上传的每个音频文件

91
00:05:03,203 --> 0:05:07,174
检测出每个文件的声音特性

92
00:05:07,708 --> 0:05:10,344
当它收集完所有的声音特性后

93
00:05:10,410 --> 0:05:12,713
它会开始处理我们现在看到的

94
00:05:12,779 --> 0:05:15,749
不断更新迭代模型的权重

95
00:05:17,317 --> 0:05:19,186
模型的权重不断更新

96
00:05:19,253 --> 0:05:21,555
你会看到性能表现不断被优化

97
00:05:21,989 --> 0:05:24,992
精准度达到了100%

98
00:05:25,058 --> 0:05:27,895
这是个好兆头 我们的模型正在收敛

99
00:05:28,929 --> 0:05:31,732
我们今天收集的音频文件

100
00:05:31,798 --> 0:05:33,133
特征都相当明显

101
00:05:33,200 --> 0:05:36,370
比如牛铃和木吉他

102
00:05:36,436 --> 0:05:38,272
所以我们训练的这个特殊模型

103
00:05:38,338 --> 0:05:40,307
处理声音非常的成功

104
00:05:40,374 --> 0:05:41,575
如你所见

105
00:05:41,875 --> 0:05:44,778
训练和验证集合

106
00:05:46,613 --> 0:05:50,517
测试面板是提供大数据集的好地方

107
00:05:50,584 --> 0:05:53,086
你可以拿它作为基准参考

108
00:05:53,887 --> 0:05:58,659
Create ML app

109
00:05:58,959 --> 0:06:01,662
它同时提供了多个数据集

110
00:05:58,959 --> 0:06:01,662
它同时提供了多个数据集

111
00:06:02,129 --> 0:06:06,567
如果你想为你训练的

112
00:06:06,633 --> 0:06:09,369
你可以使用测试面板

113
00:06:10,938 --> 0:06:13,473
最后 我们来看Output标签

114
00:06:13,774 --> 0:06:17,377
我们在用户界面能与我们的模型互动

115
00:06:18,412 --> 0:06:20,180
现在 我有一个

116
00:06:20,247 --> 0:06:22,549
在训练集没有的文件

117
00:06:23,517 --> 0:06:26,386
我将它放到

118
00:06:27,254 --> 0:06:29,623
当我将文件夹拖入用户界面时

119
00:06:29,690 --> 0:06:33,327
你能看到它扫描了这个文件

120
00:06:35,229 --> 0:06:37,264
当我们滚动这个文件

121
00:06:37,331 --> 0:06:38,932
Create ML

122
00:06:38,999 --> 0:06:43,103
将第一秒识别为了背景噪音

123
00:06:43,904 --> 0:06:46,340
接下来的几秒是讲话声

124
00:06:46,840 --> 0:06:48,609
最后是振动筛

125
00:06:50,544 --> 0:06:54,281
现在我们来看这个分类是否准确

126
00:06:54,348 --> 0:06:57,150
我们能在这个界面听一下

127
00:06:58,852 --> 0:07:00,888
测试 一 二 三

128
00:06:58,852 --> 0:07:00,888
测试 一 二 三

129
00:07:10,831 --> 0:07:14,067
至少 它能准确的识别

130
00:07:14,134 --> 0:07:16,937
我们上传的这个文件

131
00:07:17,304 --> 0:07:19,506
现在 更棒的是

132
00:07:19,773 --> 0:07:22,176
我们来用这个模型进行实时互动

133
00:07:22,643 --> 0:07:23,810
为了实现目标

134
00:07:23,877 --> 0:07:27,214
我们添加了一个按钮“录制麦克风”

135
00:07:28,849 --> 0:07:30,384
只要我开始录制

136
00:07:30,684 --> 0:07:33,754
我的Mac就会开始向模型

137
00:07:33,820 --> 0:07:35,522
传入麦克风录制的声音数据

138
00:07:43,931 --> 0:07:46,733
所以 你看 只要我一说话

139
00:07:46,800 --> 0:07:49,803
模型就能高效的识别出说话声

140
00:07:50,070 --> 0:07:51,405
当我安静时

141
00:07:51,471 --> 0:07:54,675
模型则变为背景音状态

142
00:07:57,144 --> 0:07:58,979
我带来了一些乐器

143
00:07:59,046 --> 0:08:01,615
我现在弹奏它们 看模型能否识别

144
00:07:59,046 --> 0:08:01,615
我现在弹奏它们 看模型能否识别

145
00:08:02,583 --> 0:08:04,051
首先是振动筛

146
00:08:12,726 --> 0:08:14,728
接下来是牛铃

147
00:08:20,100 --> 0:08:22,402
更多牛铃

148
00:08:22,703 --> 0:08:25,506
好吧 你们还想再听牛铃

149
00:08:30,978 --> 0:08:33,813
接下来 是我的木吉他

150
00:08:34,147 --> 0:08:36,149
我们同样来试一下

151
00:08:38,719 --> 0:08:40,988
以单音节来开始

152
00:08:47,127 --> 0:08:49,263
下面 我来试一下和弦

153
00:09:06,313 --> 0:09:07,714
识别的非常完美

154
00:09:07,781 --> 0:09:09,750
我觉得它很完美

155
00:09:10,250 --> 0:09:12,419
现在我停止录制

156
00:09:12,953 --> 0:09:14,321
在Create ML app里

157
00:09:14,388 --> 0:09:16,957
我能回滚这段录音

158
00:09:17,024 --> 0:09:20,427
查看之前已被识别的片段

159
00:09:21,128 --> 0:09:24,331
我们来看一下是否有不准确的地方

160
00:09:24,398 --> 0:09:26,300
或有误的地方

161
00:09:26,366 --> 0:09:28,302
我们也可以在文件中截取片段

162
00:09:28,368 --> 0:09:30,337
用作测试数据

163
00:09:30,404 --> 0:09:32,439
来提高我们的模型

164
00:09:33,407 --> 0:09:34,575
最后

165
00:09:34,641 --> 0:09:37,477
我们很高兴看到模型优秀的表现

166
00:09:37,778 --> 0:09:40,380
我们将模型拖到桌面

167
00:09:40,447 --> 0:09:42,649
集成到我们的app里面

168
00:09:43,584 --> 0:09:47,254
以上就是如何在Create ML app

169
00:09:47,321 --> 0:09:49,957
不用一分钟 一行代码

170
00:09:59,132 --> 0:10:01,134
在演示app里

171
00:09:59,132 --> 0:10:01,134
在演示app里

172
00:10:01,735 --> 0:10:04,938
当你收集你的训练数据时

173
00:10:05,839 --> 0:10:07,307
第一个你需要注意的地方是

174
00:10:07,374 --> 0:10:10,010
我如何在文件夹里收集数据

175
00:10:11,712 --> 0:10:13,881
所有吉他的声音

176
00:10:13,947 --> 0:10:16,083
都了放在Guitar的目录里

177
00:10:16,483 --> 0:10:20,621
其他文件也是一样

178
00:10:21,722 --> 0:10:24,291
现在 我们来聊一下背景音类别

179
00:10:25,959 --> 0:10:28,629
即便我们训练了乐器分类器

180
00:10:28,862 --> 0:10:31,131
你还是需要留意

181
00:10:31,198 --> 0:10:33,834
当没有乐器声音时

182
00:10:34,401 --> 0:10:37,471
如果你只让你的模型识别乐器

183
00:10:37,771 --> 0:10:39,673
但如果你给模型识别背景音

184
00:10:39,740 --> 0:10:41,775
它并不知道有这种声音

185
00:10:41,842 --> 0:10:45,012
所以 当你在训练一个声音分类器时

186
00:10:45,078 --> 0:10:47,614
你需要为你的模型考虑多种状况

187
00:10:48,148 --> 0:10:49,750
你需要将

188
00:10:50,083 --> 0:10:52,586
背景音单独划分为一类

189
00:10:55,923 --> 0:10:59,359
现在假设你有一个声音文件

190
00:10:59,726 --> 0:11:03,063
这个文件最开始是鼓声

191
00:10:59,726 --> 0:11:03,063
这个文件最开始是鼓声

192
00:11:03,463 --> 0:11:05,766
接着是背景噪音

193
00:11:06,133 --> 0:11:08,902
随后是吉他的声音

194
00:11:09,937 --> 0:11:11,505
如果你直接将它拖到

195
00:11:12,072 --> 0:11:15,008
Create ML app里

196
00:11:15,075 --> 0:11:17,344
它并不会是合适的训练文件

197
00:11:17,411 --> 0:11:18,312
那是因为

198
00:11:18,378 --> 0:11:21,849
这个文件包含了多个音乐类别

199
00:11:23,350 --> 0:11:24,651
请记住

200
00:11:24,718 --> 0:11:27,821
你必须使用标记的文件夹来训练模型

201
00:11:27,888 --> 0:11:30,591
所以现在我们要做的事

202
00:11:30,657 --> 0:11:32,593
将这个文件截取为三个文件

203
00:11:32,659 --> 0:11:35,529
将它们命名为鼓 吉他和背景音

204
00:11:36,763 --> 0:11:38,932
这样模型才会有良好的表现

205
00:11:38,999 --> 0:11:42,002
当你训练模型时 你要这样分开它们

206
00:11:44,972 --> 0:11:48,308
在收集音频数据时 你还需要注意

207
00:11:49,543 --> 0:11:53,046
第一 你需要保证你收集的音频是

208
00:11:53,113 --> 0:11:55,616
在真实世界存在的

209
00:11:57,251 --> 0:12:01,388
你的app将在很多不同房间

210
00:11:57,251 --> 0:12:01,388
你的app将在很多不同房间

211
00:12:01,455 --> 0:12:03,023
和声音场景工作

212
00:12:03,090 --> 0:12:06,226
你可以使用卷积技术

213
00:12:06,293 --> 0:12:08,629
模拟不同的声音场景

214
00:12:08,695 --> 0:12:10,731
或是不同的房间

215
00:12:13,800 --> 0:12:15,602
另外一个重要的点是

216
00:12:15,669 --> 0:12:17,804
是设备上的录音处理

217
00:12:19,173 --> 0:12:21,875
你可能会检查AV音频会话模式

218
00:12:22,409 --> 0:12:24,077
来为你的app的

219
00:12:24,144 --> 0:12:26,480
录音处理

220
00:12:26,547 --> 0:12:29,082
选择你app最匹配的模式

221
00:12:29,349 --> 0:12:32,319
或是符合你收集的训练数据

222
00:12:34,521 --> 0:12:35,689
最后一点

223
00:12:36,023 --> 0:12:38,859
是要留意你的模型结构

224
00:12:39,259 --> 0:12:41,328
这个声音分类器模型

225
00:12:41,395 --> 0:12:45,365
它能很好地区分不同类别的声音

226
00:12:45,632 --> 0:12:48,368
但它并不是很适合

227
00:12:48,435 --> 0:12:50,637
用来训练所有的会话内容

228
00:12:50,704 --> 0:12:52,506
有更好的工具来做这个任务

229
00:12:52,573 --> 0:12:55,309
所以 你要确保选择了正确的工具

230
00:12:57,644 --> 0:13:00,681
现在你有了ML模型

231
00:12:57,644 --> 0:13:00,681
现在你有了ML模型

232
00:13:00,747 --> 0:13:02,416
我们来讲一下如何

233
00:13:02,482 --> 0:13:03,884
将它集成到你的app

234
00:13:06,019 --> 0:13:08,121
让模型能尽可能简单的

235
00:13:08,188 --> 0:13:10,858
在你的app中发挥作用

236
00:13:10,924 --> 0:13:14,795
我们发布了一个新框架

237
00:13:16,096 --> 0:13:17,564
SoundAnalysis

238
00:13:17,631 --> 0:13:21,001
是一个用来分析声音的高级框架

239
00:13:22,069 --> 0:13:24,338
它使用了Core ML模型

240
00:13:25,339 --> 0:13:28,375
它能在内部进行一些常用的声音操作

241
00:13:28,442 --> 0:13:33,046
比如通道映射

242
00:13:35,249 --> 0:13:36,917
让我们在后台看一下

243
00:13:36,984 --> 0:13:39,453
SoundAnalysis

244
00:13:39,853 --> 0:13:42,890
现在 上面的部分是你的app

245
00:13:43,190 --> 0:13:44,491
下面是

246
00:13:44,558 --> 0:13:47,361
SoundAnalysis

247
00:13:48,629 --> 0:13:50,197
你要做的第一件事是

248
00:13:50,264 --> 0:13:53,267
给SoundAnalysis框架

249
00:13:53,534 --> 0:13:55,235
训练后的模型

250
00:13:57,538 --> 0:14:01,241
接着 你的app会提供

251
00:13:57,538 --> 0:14:01,241
接着 你的app会提供

252
00:14:01,308 --> 0:14:02,843
需要被识别一些音频

253
00:14:04,878 --> 0:14:08,182
这个音频会先被通道映射处理

254
00:14:08,248 --> 0:14:12,319
它能保证你的模型将会收到

255
00:14:12,386 --> 0:14:13,754
就像我们之前那样

256
00:14:14,087 --> 0:14:16,089
传给模型一个音频

257
00:14:16,156 --> 0:14:17,424
甚至在一个客户端

258
00:14:17,491 --> 0:14:20,360
比如 你在上传一个立体音频数据

259
00:14:22,529 --> 0:14:25,699
下一步是

260
00:14:26,400 --> 0:14:27,634
我们训练的模型

261
00:14:27,701 --> 0:14:31,238
适合处理16赫兹音频

262
00:14:31,538 --> 0:14:34,007
这样能保证你提供的音频

263
00:14:34,074 --> 0:14:37,511
会转化成模型想要的速率

264
00:14:41,381 --> 0:14:43,784
最后一步是

265
00:14:43,851 --> 0:14:45,752
这是一个音频缓冲的操作

266
00:14:46,820 --> 0:14:48,822
我们现在使用的大多数模型

267
00:14:49,389 --> 0:14:53,694
需要一定量的音频数据来处理分析块

268
00:14:54,061 --> 0:14:58,332
通常情况下 客户端里的音频

269
00:14:58,398 --> 0:15:01,335
通常都会流进任意大小的缓冲区

270
00:14:58,398 --> 0:15:01,335
通常都会流进任意大小的缓冲区

271
00:15:01,401 --> 0:15:04,905
要实现一个有效的循环缓冲区

272
00:15:04,972 --> 0:15:08,108
来确保向你的模型传入

273
00:15:08,175 --> 0:15:09,309
一个正确大小的音频块

274
00:15:09,843 --> 0:15:11,912
如果你的模型需要大约一分钟左右的

275
00:15:11,979 --> 0:15:14,781
这一步能保证

276
00:15:14,848 --> 0:15:17,918
传给模型的文件都是合规的

277
00:15:18,752 --> 0:15:22,523
最后 当数据传给模型后

278
00:15:22,589 --> 0:15:24,391
你的app将会收到包含关于音频

279
00:15:24,458 --> 0:15:28,562
分类结果一个反馈

280
00:15:29,663 --> 0:15:31,164
现在 好消息是

281
00:15:31,231 --> 0:15:33,634
你并不需要知道所有的这些

282
00:15:33,901 --> 0:15:36,436
你只需将音频提供给

283
00:15:36,503 --> 0:15:38,505
SoundAnalysis框架

284
00:15:38,572 --> 0:15:41,108
接着在你的app里

285
00:15:43,710 --> 0:15:46,346
那么 我们来聊一下

286
00:15:46,413 --> 0:15:48,949
你从SoundAnalysis

287
00:15:49,650 --> 0:15:51,251
音频是一个流媒体

288
00:15:51,318 --> 0:15:53,587
它并不像图片一样

289
00:15:53,654 --> 0:15:54,888
有开始和结尾

290
00:15:55,355 --> 0:15:56,723
因此

291
00:15:56,957 --> 0:15:59,960
这个结果可能和我们预想的有点不同

292
00:16:01,094 --> 0:16:03,497
你的结果包含了一段时间

293
00:16:03,564 --> 0:16:07,868
它和分析结果的音频时间相对应

294
00:16:08,702 --> 0:16:10,103
在这个例子中

295
00:16:10,170 --> 0:16:12,639
训练模型的块大小是被指定的

296
00:16:12,706 --> 0:16:15,309
你能看到大约是一分钟

297
00:16:17,077 --> 0:16:19,880
当你不断为模型提供音频时

298
00:16:19,947 --> 0:16:21,815
你会不断收到包含

299
00:16:21,882 --> 0:16:26,386
你分析的音频块的顶级分类结果

300
00:16:27,688 --> 0:16:29,189
现在你可能留意到

301
00:16:29,256 --> 0:16:32,025
这一秒的结果覆盖了之前的

302
00:16:32,092 --> 0:16:33,560
大约50%

303
00:16:33,894 --> 0:16:35,662
我们就是这样设计的

304
00:16:36,663 --> 0:16:40,033
你要确定你提供的每个音频片段

305
00:16:40,100 --> 0:16:43,770
都能出现在分析视图的中间

306
00:16:44,037 --> 0:16:47,441
否则它会出现在分析视图左右两边

307
00:16:47,508 --> 0:16:50,511
模型就不会正常发挥作用

308
00:16:51,311 --> 0:16:55,048
所以 分析结果时默认是50%重合

309
00:16:55,115 --> 0:16:57,017
但如果你有相应的使用场景时

310
00:16:57,084 --> 0:16:59,620
在API中是可配置的

311
00:17:00,921 --> 0:17:03,457
当你持续提供音频数据时

312
00:17:04,090 --> 0:17:06,359
你会一直收到结果

313
00:17:07,294 --> 0:17:10,063
只要音频流是有效的

314
00:17:10,130 --> 0:17:12,465
你就能一直上传数据 然后得到结果

315
00:17:15,935 --> 0:17:18,105
现在 我们来快速过一下

316
00:17:18,172 --> 0:17:21,608
SoundAnalysis框架

317
00:17:23,410 --> 0:17:25,512
假设我们有一个音频文件

318
00:17:25,579 --> 0:17:26,813
我们想要用今天

319
00:17:26,880 --> 0:17:29,550
训练的模型来分析它

320
00:17:30,551 --> 0:17:31,618
首先

321
00:17:31,685 --> 0:17:33,954
我们创建了一个音频文件分析器

322
00:17:34,021 --> 0:17:37,291
将文件的URL提供给分析器

323
00:17:38,825 --> 0:17:42,462
接着 我们创建了一个

324
00:17:42,529 --> 0:17:45,132
接着实例化一个模型

325
00:17:45,199 --> 0:17:47,067
MySoundClassifier

326
00:17:49,469 --> 0:17:52,906
接着 我们向分析器提出请求

327
00:17:53,273 --> 0:17:54,675
提供一个观察器

328
00:17:54,741 --> 0:17:57,911
它会处理我们模型产出的结果

329
00:17:59,746 --> 0:18:02,783
最后 我们来分析文件

330
00:17:59,746 --> 0:18:02,783
最后 我们来分析文件

331
00:18:02,850 --> 0:18:06,486
它会先扫描文件然后处理结果

332
00:18:09,156 --> 0:18:10,958
现在 在你的app那一侧

333
00:18:11,325 --> 0:18:13,627
你需要确保你的一个类

334
00:18:13,694 --> 0:18:17,097
符合SNResultsObserving协议

335
00:18:17,865 --> 0:18:20,701
这就是你会从框架收到的结果

336
00:18:22,169 --> 0:18:26,373
你要实现的第一个方法是

337
00:18:27,574 --> 0:18:30,444
这个方法可能会被调用很多次

338
00:18:30,711 --> 0:18:34,147
只要当新的监测可用时

339
00:18:35,916 --> 0:18:39,453
你就有可能获取到

340
00:18:39,520 --> 0:18:41,355
和与之相关的时间范围

341
00:18:41,421 --> 0:18:44,224
这就是你app中处理声音分类

342
00:18:44,291 --> 0:18:47,561
事件时运行的逻辑

343
00:18:49,630 --> 0:18:51,565
另外一个你可能感兴趣的方法是

344
00:18:51,632 --> 0:18:53,934
请求

345
00:18:54,234 --> 0:18:56,370
如果由于某种原因分析失败了

346
00:18:56,436 --> 0:18:57,971
这个方法就会被调用

347
00:18:58,238 --> 0:19:02,109
接下来 你将不会从

348
00:18:58,238 --> 0:19:02,109
接下来 你将不会从

349
00:19:02,843 --> 0:19:06,146
或者这个流媒体被成功处理了

350
00:19:06,213 --> 0:19:08,081
比如 在文件的最后

351
00:19:08,415 --> 0:19:11,084
你会收到

352
00:19:13,554 --> 0:19:16,390
下面 我们来做一下今天的总结

353
00:19:17,691 --> 0:19:21,161
你知道了如何在Create ML

354
00:19:21,361 --> 0:19:23,163
来训练声音分类器

355
00:19:25,799 --> 0:19:28,735
以及 在设备中

356
00:19:28,802 --> 0:19:30,804
框架来运行模型

357
00:19:33,040 --> 0:19:34,508
了解更多信息

358
00:19:34,575 --> 0:19:38,979
可以在developer.apple.com

359
00:19:39,546 --> 0:19:41,148
你会在文章中找到一个

360
00:19:41,215 --> 0:19:45,219
使用设备内置的麦克风

361
00:19:45,285 --> 0:19:47,020
来进行声音分类的例子

362
00:19:47,387 --> 0:19:50,457
和之前大家看到的乐器分类

363
00:19:52,993 --> 0:19:54,428
感谢聆听

364
00:19:54,494 --> 0:19:58,765
希望大家都能在app中
