1
00:00:00,506 --> 0:00:05,500
[ Music ]

2
00:00:09,516 --> 0:00:12,500
[ Applause ]

3
00:00:14,096 --> 0:00:14,596
&gt;&gt; Good morning.

4
00:00:15,486 --> 0:00:16,606
My name's Dan Klingler.

5
00:00:17,156 --> 0:00:18,656
And, I'm a software engineer on

6
00:00:18,656 --> 0:00:19,926
the audio team here at Apple.

7
00:00:19,926 --> 0:00:22,266
And, today I'm really excited to

8
00:00:22,266 --> 0:00:24,016
talk to you about training sound

9
00:00:24,016 --> 0:00:26,036
classification models in Create

10
00:00:26,036 --> 0:00:26,276
ML.

11
00:00:28,766 --> 0:00:30,906
So, before we start, we might

12
00:00:30,906 --> 0:00:32,766
want to ask the question what is

13
00:00:32,766 --> 0:00:34,806
sound classification, and how

14
00:00:34,806 --> 0:00:36,266
might it be useful in your

15
00:00:36,266 --> 0:00:36,876
application?

16
00:00:38,086 --> 0:00:41,036
Sound classification is the task

17
00:00:41,036 --> 0:00:43,166
of taking a sound, and placing

18
00:00:43,166 --> 0:00:44,966
it into one of many categories.

19
00:00:45,846 --> 0:00:47,386
But, if you think about it,

20
00:00:47,806 --> 0:00:49,056
there are many different ways

21
00:00:49,056 --> 0:00:50,416
that we can categorize sound.

22
00:00:51,616 --> 0:00:53,956
One way might be to think about

23
00:00:53,956 --> 0:00:55,486
the object that makes the sound.

24
00:00:55,996 --> 0:00:57,686
So, in this example, we have the

25
00:00:57,686 --> 0:00:59,456
sound of guitar, or the sound of

26
00:00:59,456 --> 0:01:00,056
drums.

27
00:00:59,456 --> 0:01:00,056
drums.

28
00:01:00,336 --> 0:01:02,106
And, it's really the acoustical

29
00:01:02,106 --> 0:01:03,496
properties of that object which

30
00:01:03,496 --> 0:01:05,135
are different, and allow us as

31
00:01:05,135 --> 0:01:06,686
humans to disambiguate these

32
00:01:06,686 --> 0:01:07,266
sounds.

33
00:01:09,236 --> 0:01:10,766
But, a second way we could think

34
00:01:10,766 --> 0:01:13,156
about sound classification is

35
00:01:13,156 --> 0:01:15,216
where the sound comes from, and

36
00:01:15,216 --> 0:01:16,436
if you've ever gone on a hike,

37
00:01:16,996 --> 0:01:18,456
or been in the middle of a busy

38
00:01:18,456 --> 0:01:20,986
city, you'll understand that the

39
00:01:20,986 --> 0:01:22,566
texture of the sound around you

40
00:01:22,846 --> 0:01:25,026
is very different, even though

41
00:01:25,026 --> 0:01:26,616
there's not one particular sound

42
00:01:26,616 --> 0:01:27,916
that stands out, necessarily.

43
00:01:31,306 --> 0:01:32,386
And, a third way we could think

44
00:01:32,386 --> 0:01:34,996
about sound classification, is

45
00:01:34,996 --> 0:01:36,526
by looking at the attributes of

46
00:01:36,526 --> 0:01:37,906
the sound, or the properties of

47
00:01:37,906 --> 0:01:38,376
the sound.

48
00:01:38,926 --> 0:01:41,516
And so in this example, a baby's

49
00:01:41,516 --> 0:01:43,976
laugh versus a baby's cry.

50
00:01:44,026 --> 0:01:45,756
Both come from the same source,

51
00:01:46,346 --> 0:01:47,606
but the properties of the sound

52
00:01:47,606 --> 0:01:48,486
are very different.

53
00:01:48,656 --> 0:01:50,406
And, this allows us to tell the

54
00:01:50,406 --> 0:01:51,726
difference between these sounds.

55
00:01:53,336 --> 0:01:56,856
Now, as app developers, you all

56
00:01:56,856 --> 0:01:58,406
have different apps, and you

57
00:01:58,406 --> 0:01:59,686
might have your different use

58
00:01:59,686 --> 0:02:01,386
case for sound classification.

59
00:01:59,686 --> 0:02:01,386
case for sound classification.

60
00:02:02,716 --> 0:02:04,456
And, wouldn't it be great if you

61
00:02:04,456 --> 0:02:05,946
could train your own model,

62
00:02:06,346 --> 0:02:07,696
tailor-made for your own

63
00:02:07,746 --> 0:02:08,976
application's use case?

64
00:02:11,486 --> 0:02:13,626
And, you can do that today using

65
00:02:13,626 --> 0:02:15,176
the Create ML app, which is

66
00:02:15,176 --> 0:02:16,366
built right into Xcode.

67
00:02:16,856 --> 0:02:18,806
This is the simplest way to

68
00:02:18,806 --> 0:02:20,696
train a sound classifier model.

69
00:02:22,556 --> 0:02:24,846
To train a sound classifier,

70
00:02:25,596 --> 0:02:27,386
you'll first provide labeled

71
00:02:27,386 --> 0:02:29,726
audio data to Create ML, in the

72
00:02:29,726 --> 0:02:31,056
form of audio files.

73
00:02:32,736 --> 0:02:35,236
Then, Create ML will train a

74
00:02:35,236 --> 0:02:37,096
sound classifier model on your

75
00:02:37,096 --> 0:02:38,616
custom data.

76
00:02:39,356 --> 0:02:41,236
And then, you can take that

77
00:02:41,236 --> 0:02:43,126
sound classifier, and use it

78
00:02:43,226 --> 0:02:44,376
right in your application.

79
00:02:45,256 --> 0:02:46,926
And, I'd love to show you this

80
00:02:46,926 --> 0:02:49,336
process in action today, with a

81
00:02:49,336 --> 0:02:49,736
demo.

82
00:02:52,516 --> 0:02:55,500
[ Applause ]

83
00:03:00,376 --> 0:03:02,226
So, to start, I'm going to

84
00:03:02,226 --> 0:03:04,236
launch the Create ML app, which

85
00:03:04,236 --> 0:03:05,556
you can find bundled with your

86
00:03:05,556 --> 0:03:06,546
Xcode installation.

87
00:03:08,936 --> 0:03:10,196
We're going to be creating a new

88
00:03:10,196 --> 0:03:13,366
document, and select Sound from

89
00:03:13,366 --> 0:03:14,246
the template chooser.

90
00:03:17,816 --> 0:03:20,316
We'll click Next, and name our

91
00:03:20,316 --> 0:03:22,636
project MySoundClassifier.

92
00:03:23,006 --> 0:03:25,406
Let's save this project to our

93
00:03:25,406 --> 0:03:26,436
Documents directory.

94
00:03:27,696 --> 0:03:30,236
Once the Create ML app launches,

95
00:03:30,446 --> 0:03:32,576
you'll see this home screen, and

96
00:03:32,576 --> 0:03:33,946
the Input tab is selected on the

97
00:03:33,946 --> 0:03:34,336
left.

98
00:03:36,036 --> 0:03:37,106
This is where we'll provide our

99
00:03:37,106 --> 0:03:38,846
training data to the Create ML

100
00:03:39,326 --> 0:03:40,346
app, in order to train our

101
00:03:40,346 --> 0:03:41,096
custom model.

102
00:03:43,236 --> 0:03:45,086
You'll see some other tabs

103
00:03:45,086 --> 0:03:45,996
across the top here.

104
00:03:46,476 --> 0:03:48,156
Like Training, Validation, and

105
00:03:48,156 --> 0:03:48,636
Testing.

106
00:03:49,126 --> 0:03:50,656
And, these allow us to view some

107
00:03:50,656 --> 0:03:52,516
statistics on the accuracy of

108
00:03:52,516 --> 0:03:53,886
our model for each of these

109
00:03:53,886 --> 0:03:54,776
stages of training.

110
00:03:55,996 --> 0:03:58,726
And, finally, the Output tab is

111
00:03:58,726 --> 0:03:59,846
where we'll expect to find our

112
00:03:59,846 --> 0:04:01,296
model after it's been trained.

113
00:03:59,846 --> 0:04:01,296
model after it's been trained.

114
00:04:02,046 --> 0:04:03,606
And, we can also interact with

115
00:04:03,606 --> 0:04:04,916
our model in real time.

116
00:04:06,356 --> 0:04:07,866
Now, today, I'm going to be

117
00:04:07,866 --> 0:04:09,246
training a musical instrument

118
00:04:09,246 --> 0:04:09,956
classifier.

119
00:04:09,956 --> 0:04:11,376
And, I've brought some

120
00:04:11,376 --> 0:04:13,246
instruments with me that we can

121
00:04:13,696 --> 0:04:13,806
try.

122
00:04:14,476 --> 0:04:16,245
I have a TrainingData directory

123
00:04:16,805 --> 0:04:18,406
that we can open, and take a

124
00:04:18,406 --> 0:04:19,706
look at some of the sound files

125
00:04:19,706 --> 0:04:21,046
I've collected.

126
00:04:22,055 --> 0:04:25,056
These contain recordings from an

127
00:04:25,056 --> 0:04:27,526
acoustic guitar, for example, or

128
00:04:27,526 --> 0:04:29,376
cowbell, or shaker.

129
00:04:30,056 --> 0:04:33,756
To train our model, all we need

130
00:04:33,756 --> 0:04:35,886
to do is drag this directory

131
00:04:36,266 --> 0:04:37,456
straight into Create ML.

132
00:04:38,136 --> 0:04:41,636
Create ML has figured out that

133
00:04:41,636 --> 0:04:43,386
we have a total of 49 sound

134
00:04:43,386 --> 0:04:44,346
files we're going to be using

135
00:04:44,346 --> 0:04:44,696
today.

136
00:04:44,696 --> 0:04:47,286
And, that spans 7 different

137
00:04:47,286 --> 0:04:47,876
classes.

138
00:04:50,276 --> 0:04:52,086
All we need to do is press the

139
00:04:52,086 --> 0:04:53,816
Train button, and our model will

140
00:04:53,816 --> 0:04:54,516
begin training.

141
00:04:55,766 --> 0:04:57,696
Now, the first thing Create ML

142
00:04:57,696 --> 0:04:58,596
is going to be doing when

143
00:04:58,596 --> 0:05:00,586
training this model is walking

144
00:04:58,596 --> 0:05:00,586
training this model is walking

145
00:05:00,586 --> 0:05:02,036
through each of the sound files

146
00:05:02,036 --> 0:05:03,916
we provided, and extracting

147
00:05:03,916 --> 0:05:06,176
audio features across the entire

148
00:05:06,176 --> 0:05:07,006
file.

149
00:05:07,376 --> 0:05:08,776
And, once it's collected all

150
00:05:08,776 --> 0:05:10,526
these audio features, it will

151
00:05:10,526 --> 0:05:11,766
begin the process you're seeing

152
00:05:11,766 --> 0:05:13,706
now, which is where the model

153
00:05:13,706 --> 0:05:14,616
weights are updating

154
00:05:14,616 --> 0:05:15,236
iteratively.

155
00:05:17,266 --> 0:05:18,206
As the model weights are

156
00:05:18,206 --> 0:05:19,896
updating, you can see that the

157
00:05:19,896 --> 0:05:22,126
performance is increasing, and

158
00:05:22,126 --> 0:05:24,276
our accuracy is moving towards

159
00:05:24,276 --> 0:05:26,356
100%, which is a good sign that

160
00:05:26,356 --> 0:05:27,416
our model is converging.

161
00:05:28,906 --> 0:05:30,026
Now, if you think about the

162
00:05:30,026 --> 0:05:31,356
sounds we've collected today,

163
00:05:31,396 --> 0:05:33,456
they're fairly distinct, like

164
00:05:33,456 --> 0:05:35,176
cowbell and acoustic guitar

165
00:05:35,176 --> 0:05:36,086
sound fairly different.

166
00:05:36,316 --> 0:05:37,616
And so, this particular model

167
00:05:37,616 --> 0:05:38,976
we've trained is able to do a

168
00:05:38,976 --> 0:05:40,356
really good job with the sounds,

169
00:05:40,356 --> 0:05:42,296
as you can see, on both the

170
00:05:42,296 --> 0:05:43,906
training and the validation

171
00:05:43,906 --> 0:05:44,396
sets.

172
00:05:44,936 --> 0:05:48,736
The testing pane is a good place

173
00:05:48,796 --> 0:05:50,726
to provide a large data set that

174
00:05:50,726 --> 0:05:52,676
you might have for benchmarking.

175
00:05:53,866 --> 0:05:55,976
The Create ML app allows you to

176
00:05:55,976 --> 0:05:57,556
train multiple models at the

177
00:05:57,556 --> 0:05:58,286
same time.

178
00:05:58,916 --> 0:06:00,256
And, potentially provide

179
00:05:58,916 --> 0:06:00,256
And, potentially provide

180
00:06:00,316 --> 0:06:01,636
different sets of training data.

181
00:06:01,716 --> 0:06:03,626
And so, the testing pane is a

182
00:06:03,626 --> 0:06:05,176
great place if you want to

183
00:06:05,176 --> 0:06:06,776
provide a common benchmark for

184
00:06:06,776 --> 0:06:07,706
all those different model

185
00:06:07,706 --> 0:06:08,876
configurations you're training.

186
00:06:10,636 --> 0:06:12,186
Finally, as we make our way to

187
00:06:12,186 --> 0:06:14,346
the Output tab, you'll see a UI

188
00:06:14,446 --> 0:06:16,196
that shows how we can interact

189
00:06:16,196 --> 0:06:16,926
with our model.

190
00:06:17,846 --> 0:06:19,556
Now, I've collected one other

191
00:06:19,556 --> 0:06:21,376
file that I didn't include in my

192
00:06:21,376 --> 0:06:22,066
training set.

193
00:06:22,066 --> 0:06:24,606
And, I've placed this in the

194
00:06:24,606 --> 0:06:25,866
TestingData directory.

195
00:06:25,866 --> 0:06:28,376
When I drag that directory into

196
00:06:28,376 --> 0:06:30,206
the UI, you can see it

197
00:06:30,206 --> 0:06:31,706
recognizes a file called

198
00:06:31,706 --> 0:06:33,606
classification test.

199
00:06:34,876 --> 0:06:36,616
As we scroll through this file,

200
00:06:37,276 --> 0:06:39,126
it appears that Create ML has

201
00:06:39,126 --> 0:06:40,806
classified the first second or

202
00:06:40,806 --> 0:06:42,376
so of this file as background

203
00:06:42,376 --> 0:06:42,756
noise.

204
00:06:43,876 --> 0:06:45,276
Then, speech for the next couple

205
00:06:45,276 --> 0:06:48,246
seconds, and finally shaker.

206
00:06:50,506 --> 0:06:52,706
But, let's find out if we agree

207
00:06:52,706 --> 0:06:53,956
with this classification.

208
00:06:54,256 --> 0:06:55,606
And, we can listen to this file

209
00:06:55,606 --> 0:06:58,626
right here in the UI.

210
00:06:58,816 --> 0:06:59,976
Test, 1, 2, 3.

211
00:07:00,516 --> 0:07:04,516
[ Shaker Playing ]

212
00:07:05,516 --> 0:07:10,366
[ Applause ]

213
00:07:10,866 --> 0:07:12,306
So, it seems at least on the

214
00:07:12,306 --> 0:07:14,436
file that we've collected, that

215
00:07:14,436 --> 0:07:15,386
this model seems to be

216
00:07:15,386 --> 0:07:16,516
performing reasonably.

217
00:07:17,286 --> 0:07:19,146
Now, what would be even better,

218
00:07:19,726 --> 0:07:20,756
is if we could interact with

219
00:07:20,756 --> 0:07:21,676
this model live.

220
00:07:22,606 --> 0:07:24,306
And, to do that, we've added a

221
00:07:24,306 --> 0:07:26,806
button here that has Record

222
00:07:26,806 --> 0:07:27,426
Microphone.

223
00:07:28,316 --> 0:07:30,796
And, once I begin recording, my

224
00:07:30,796 --> 0:07:32,176
Mac will begin feeding the

225
00:07:32,176 --> 0:07:34,026
built-in microphone data into

226
00:07:34,026 --> 0:07:34,966
the model we've just trained.

227
00:07:35,516 --> 0:07:43,436
[ Applause ]

228
00:07:43,936 --> 0:07:45,626
So, what you can see is, anytime

229
00:07:45,626 --> 0:07:47,286
I'm speaking, the model's

230
00:07:47,286 --> 0:07:48,796
recognizing speech with high

231
00:07:48,796 --> 0:07:49,496
confidence.

232
00:07:49,976 --> 0:07:51,696
And, as I quiet down, you can

233
00:07:51,696 --> 0:07:53,526
see the model settle back down

234
00:07:53,676 --> 0:07:56,636
into a background state.

235
00:07:57,056 --> 0:07:58,206
And, I brought a few instruments

236
00:07:58,206 --> 0:07:59,716
with me so that we can play

237
00:07:59,716 --> 0:08:00,966
along and see if the model can

238
00:07:59,716 --> 0:08:00,966
along and see if the model can

239
00:08:00,966 --> 0:08:01,676
recognize them.

240
00:08:02,556 --> 0:08:03,726
Let's start with a shaker.

241
00:08:04,516 --> 0:08:06,516
[ Playing Shaker ]

242
00:08:07,516 --> 0:08:11,706
[ Applause ]

243
00:08:12,206 --> 0:08:13,826
I've also brought my trusty

244
00:08:13,826 --> 0:08:13,976
cowbell.

245
00:08:14,516 --> 0:08:19,546
[ Playing Cowbell ]

246
00:08:20,046 --> 0:08:22,266
&gt;&gt; More cowbell!

247
00:08:22,686 --> 0:08:23,836
&gt;&gt; Well, the people have spoken.

248
00:08:23,836 --> 0:08:24,496
More cowbell.

249
00:08:24,496 --> 0:08:24,966
There you have it.

250
00:08:25,016 --> 0:08:27,016
[ Playing Cowbell ]

251
00:08:27,516 --> 0:08:30,096
[ Applause ]

252
00:08:30,596 --> 0:08:32,155
And, I also brought my acoustic

253
00:08:32,155 --> 0:08:34,366
guitar with me here today, so we

254
00:08:34,366 --> 0:08:35,645
can try some of this as well.

255
00:08:38,666 --> 0:08:39,515
I can start with some

256
00:08:39,515 --> 0:08:40,496
single-note lines.

257
00:08:41,515 --> 0:08:46,636
[ Playing Guitar ]

258
00:08:47,136 --> 0:08:48,436
And then, we can try some chords

259
00:08:48,436 --> 0:08:48,976
as well.

260
00:08:49,516 --> 0:08:55,500
[ Playing Guitar ]

261
00:08:57,516 --> 0:09:05,216
[ Applause ]

262
00:08:57,516 --> 0:09:05,216
[ Applause ]

263
00:09:05,716 --> 0:09:07,206
So, that seemed to be working

264
00:09:07,206 --> 0:09:08,246
pretty well, and I think that's

265
00:09:08,246 --> 0:09:09,216
something we can work with.

266
00:09:10,196 --> 0:09:11,556
So, I can stop the recording

267
00:09:11,556 --> 0:09:11,856
now.

268
00:09:12,526 --> 0:09:14,386
And, in the Create ML app, I'm

269
00:09:14,386 --> 0:09:15,956
able to scroll back through this

270
00:09:15,956 --> 0:09:17,596
recording, and take a look at

271
00:09:17,596 --> 0:09:18,726
any of the segments that we've

272
00:09:18,726 --> 0:09:19,926
been analyzing so far.

273
00:09:21,096 --> 0:09:22,386
This might be a great place to

274
00:09:22,386 --> 0:09:23,456
check if there are any

275
00:09:23,456 --> 0:09:24,966
anomalies, or things that it

276
00:09:24,966 --> 0:09:26,836
didn't get correct, and maybe we

277
00:09:26,836 --> 0:09:28,356
can clip some parts of this file

278
00:09:28,356 --> 0:09:29,936
to use, as part of our training

279
00:09:29,936 --> 0:09:31,376
set to improve the performance

280
00:09:31,376 --> 0:09:32,306
of our model.

281
00:09:33,376 --> 0:09:35,136
And, finally, when we're happy

282
00:09:35,136 --> 0:09:36,356
that our model is performing the

283
00:09:36,356 --> 0:09:38,336
way we'd like, we can simply

284
00:09:38,336 --> 0:09:40,126
drag this model to our desktop,

285
00:09:40,416 --> 0:09:41,506
where we can integrate it into

286
00:09:41,506 --> 0:09:42,316
our application.

287
00:09:43,556 --> 0:09:44,796
And, that's training a sound

288
00:09:44,826 --> 0:09:47,316
classifier in the Create ML app

289
00:09:47,316 --> 0:09:48,836
in under a minute, with zero

290
00:09:48,836 --> 0:09:49,416
lines of code.

291
00:09:50,516 --> 0:09:56,500
[ Applause ]

292
00:09:59,106 --> 0:10:00,736
So, you saw during the demo,

293
00:09:59,106 --> 0:10:00,736
So, you saw during the demo,

294
00:10:01,726 --> 0:10:02,936
there's some things to consider

295
00:10:02,996 --> 0:10:04,046
when collecting your training

296
00:10:04,046 --> 0:10:04,386
data.

297
00:10:05,766 --> 0:10:06,486
And, the first thing you'll

298
00:10:06,486 --> 0:10:08,466
notice is how I collected this

299
00:10:08,526 --> 0:10:09,706
data in directories.

300
00:10:11,696 --> 0:10:12,946
All the sounds that come from a

301
00:10:12,946 --> 0:10:15,206
guitar are placed in the Guitar

302
00:10:15,206 --> 0:10:15,746
directory.

303
00:10:16,476 --> 0:10:19,296
And, likewise with a file like

304
00:10:19,296 --> 0:10:20,386
Drums or Background.

305
00:10:21,226 --> 0:10:22,556
Now, let's talk about the

306
00:10:22,556 --> 0:10:23,836
background class for a minute.

307
00:10:25,916 --> 0:10:26,786
Even though we're training a

308
00:10:26,786 --> 0:10:28,406
musical instrument classifier,

309
00:10:28,816 --> 0:10:30,136
you still need to consider what

310
00:10:30,136 --> 0:10:31,976
might happen if there's not any

311
00:10:31,976 --> 0:10:33,056
musical instruments being

312
00:10:33,056 --> 0:10:35,276
played, and if you only trained

313
00:10:35,276 --> 0:10:36,416
your model on musical

314
00:10:36,416 --> 0:10:38,406
instruments, but then fed it

315
00:10:38,406 --> 0:10:39,186
background data.

316
00:10:39,716 --> 0:10:40,876
That's data it's never seen

317
00:10:40,876 --> 0:10:41,306
before.

318
00:10:41,476 --> 0:10:43,276
And so, make sure that when

319
00:10:43,276 --> 0:10:44,066
you're training a sound

320
00:10:44,066 --> 0:10:45,716
classifier, if you expect your

321
00:10:45,716 --> 0:10:47,816
model to work in situations

322
00:10:48,106 --> 0:10:49,436
where there's background noise,

323
00:10:50,046 --> 0:10:51,856
to provide that as part of the

324
00:10:51,856 --> 0:10:52,916
class as well.

325
00:10:55,866 --> 0:10:57,506
Now, suppose you had a file that

326
00:10:57,506 --> 0:10:58,826
was called sounds.

327
00:10:59,686 --> 0:11:01,176
And, the file started at the

328
00:10:59,686 --> 0:11:01,176
And, the file started at the

329
00:11:01,176 --> 0:11:03,686
beginning with drums, and then

330
00:11:03,686 --> 0:11:04,866
transitioned to background

331
00:11:04,866 --> 0:11:07,286
noise, and then finally ended

332
00:11:07,666 --> 0:11:08,626
with guitar.

333
00:11:09,906 --> 0:11:13,236
This file, as is, is not going

334
00:11:13,236 --> 0:11:15,606
to be useful for dragging

335
00:11:15,606 --> 0:11:16,886
directly in the Create ML app.

336
00:11:17,006 --> 0:11:18,896
And, that's because this sound

337
00:11:18,896 --> 0:11:20,676
contains multiple sound classes

338
00:11:20,716 --> 0:11:21,456
in one file.

339
00:11:23,016 --> 0:11:25,246
Remember, you have to use

340
00:11:25,246 --> 0:11:26,756
labeled directories to train

341
00:11:26,756 --> 0:11:28,726
your model, and so the best

342
00:11:28,726 --> 0:11:29,946
thing to do in this situation

343
00:11:29,946 --> 0:11:31,666
would be to split this file into

344
00:11:31,666 --> 0:11:33,776
three, and name them drums,

345
00:11:33,776 --> 0:11:35,116
guitar, and background.

346
00:11:35,736 --> 0:11:38,126
This is going to have a lot

347
00:11:38,126 --> 0:11:39,406
better performance when training

348
00:11:39,406 --> 0:11:40,936
your model, if you split your

349
00:11:40,936 --> 0:11:41,686
files this way.

350
00:11:44,196 --> 0:11:46,856
A few other considerations when

351
00:11:46,856 --> 0:11:47,856
collecting audio data.

352
00:11:49,046 --> 0:11:51,656
First, we want to insure that

353
00:11:51,656 --> 0:11:52,806
the data you're collecting

354
00:11:53,066 --> 0:11:54,536
matches a real-world audio

355
00:11:54,536 --> 0:11:55,216
environment.

356
00:11:55,786 --> 0:11:59,346
So, remember that if your app is

357
00:11:59,346 --> 0:12:01,006
intended to work in a variety of

358
00:11:59,346 --> 0:12:01,006
intended to work in a variety of

359
00:12:01,006 --> 0:12:03,136
rooms or acoustic scenarios, you

360
00:12:03,136 --> 0:12:04,646
can either collect data in those

361
00:12:04,646 --> 0:12:06,986
acoustic scenarios, or consider

362
00:12:06,986 --> 0:12:08,486
even simulating those rooms,

363
00:12:08,666 --> 0:12:09,656
using a technique called

364
00:12:09,746 --> 0:12:10,356
convolution.

365
00:12:12,856 --> 0:12:14,776
Another important thing to

366
00:12:14,776 --> 0:12:16,416
consider is the on-device

367
00:12:16,416 --> 0:12:17,466
microphone processing.

368
00:12:17,996 --> 0:12:20,446
You might check out AV audio

369
00:12:20,626 --> 0:12:22,926
session modes to select

370
00:12:23,156 --> 0:12:24,726
different modes of microphone

371
00:12:24,726 --> 0:12:26,066
processing in your application,

372
00:12:26,516 --> 0:12:27,906
and select the one which is best

373
00:12:27,906 --> 0:12:29,516
suited for your app, or

374
00:12:29,516 --> 0:12:31,066
potentially matches the training

375
00:12:31,066 --> 0:12:33,966
data you've collected.

376
00:12:34,426 --> 0:12:36,406
And a final point, is to be

377
00:12:36,406 --> 0:12:38,516
aware of the model architecture.

378
00:12:39,226 --> 0:12:40,616
So, this is the sound classifier

379
00:12:40,616 --> 0:12:42,876
model, and it can do pretty well

380
00:12:42,876 --> 0:12:44,396
at classifying varieties of

381
00:12:44,396 --> 0:12:45,016
sounds.

382
00:12:45,576 --> 0:12:46,666
But, this is not something that

383
00:12:46,666 --> 0:12:48,136
would be suitable for, say,

384
00:12:48,396 --> 0:12:49,576
training a full-on speech

385
00:12:49,576 --> 0:12:50,226
recognizer.

386
00:12:50,676 --> 0:12:52,096
There are better tools for that

387
00:12:52,096 --> 0:12:52,436
job.

388
00:12:52,526 --> 0:12:53,496
So, make sure you're always

389
00:12:53,496 --> 0:12:54,476
using the right tool for the

390
00:12:54,476 --> 0:12:54,756
job.

391
00:12:57,246 --> 0:13:00,166
So, now you have this ML model,

392
00:12:57,246 --> 0:13:00,166
So, now you have this ML model,

393
00:13:00,166 --> 0:13:01,796
and let's talk about how you can

394
00:13:01,796 --> 0:13:02,846
integrate it into your

395
00:13:02,846 --> 0:13:03,476
application.

396
00:13:05,336 --> 0:13:07,136
And, to make it as easy as

397
00:13:07,136 --> 0:13:08,836
possible to run sound

398
00:13:08,876 --> 0:13:10,056
classification models in your

399
00:13:10,056 --> 0:13:11,926
app, we're also releasing a new

400
00:13:11,926 --> 0:13:14,456
framework called SoundAnalysis.

401
00:13:15,366 --> 0:13:17,996
SoundAnalysis is a new

402
00:13:17,996 --> 0:13:19,536
high-level framework for

403
00:13:19,536 --> 0:13:20,526
analyzing sound.

404
00:13:21,596 --> 0:13:25,576
It uses Core ML models, and it

405
00:13:25,576 --> 0:13:27,376
handles common audio operations

406
00:13:27,376 --> 0:13:29,296
internally, such as channel

407
00:13:29,296 --> 0:13:31,306
mapping, sample rate conversion,

408
00:13:31,716 --> 0:13:32,526
or reblocking.

409
00:13:35,136 --> 0:13:36,306
And, let's take a look under the

410
00:13:36,306 --> 0:13:38,436
hood to see how SoundAnalysis

411
00:13:38,436 --> 0:13:38,946
works.

412
00:13:39,276 --> 0:13:41,716
Now, the top section represents

413
00:13:41,716 --> 0:13:42,566
your application.

414
00:13:43,086 --> 0:13:44,436
And, the bottom represents

415
00:13:44,506 --> 0:13:45,696
what's happening under the hood

416
00:13:45,776 --> 0:13:46,946
in SoundAnalysis.

417
00:13:47,186 --> 0:13:50,346
The first thing you'll do is

418
00:13:50,346 --> 0:13:51,616
provide the model you just

419
00:13:51,616 --> 0:13:53,596
trained using Create ML to

420
00:13:53,596 --> 0:13:54,846
SoundAnalysis framework.

421
00:13:56,656 --> 0:13:59,826
Then, your application will

422
00:13:59,826 --> 0:14:01,756
provide some audio that needs to

423
00:13:59,826 --> 0:14:01,756
provide some audio that needs to

424
00:14:01,756 --> 0:14:02,456
be analyzed.

425
00:14:04,176 --> 0:14:06,596
This audio will first hit a

426
00:14:06,596 --> 0:14:08,356
channel-mapping step, which

427
00:14:08,356 --> 0:14:09,996
ensures that if your model

428
00:14:09,996 --> 0:14:12,236
expects one channel of audio,

429
00:14:12,346 --> 0:14:14,506
like ours did here, that that's

430
00:14:14,506 --> 0:14:15,636
what's delivered to the model,

431
00:14:16,086 --> 0:14:17,736
even as a client, if you're

432
00:14:17,736 --> 0:14:19,576
delivering stereo data, for

433
00:14:19,576 --> 0:14:19,976
example.

434
00:14:22,476 --> 0:14:23,986
The next step that happens is

435
00:14:23,986 --> 0:14:25,296
called sample rate conversion.

436
00:14:26,066 --> 0:14:28,066
The model we trained natively

437
00:14:28,066 --> 0:14:30,456
operates on 16 kilohertz audio

438
00:14:30,456 --> 0:14:30,846
data.

439
00:14:31,216 --> 0:14:33,196
And, this ensures that the audio

440
00:14:33,196 --> 0:14:34,766
that you provide gets converted

441
00:14:35,036 --> 0:14:36,386
to match the rate the model

442
00:14:36,386 --> 0:14:36,976
expects.

443
00:14:41,326 --> 0:14:42,366
The final step that

444
00:14:42,366 --> 0:14:43,986
SoundAnalysis performs is an

445
00:14:43,986 --> 0:14:45,316
audio buffering operation.

446
00:14:46,796 --> 0:14:47,966
Most of the models we're working

447
00:14:47,966 --> 0:14:50,276
with today require a fixed

448
00:14:50,276 --> 0:14:52,236
amount of audio data to process

449
00:14:52,236 --> 0:14:53,106
an analysis chunk.

450
00:14:54,006 --> 0:14:56,916
And, oftentimes, the audio that

451
00:14:56,916 --> 0:14:58,596
you have as a client might be

452
00:14:58,596 --> 0:15:00,516
coming in at arbitrary sized

453
00:14:58,596 --> 0:15:00,516
coming in at arbitrary sized

454
00:15:00,516 --> 0:15:01,006
buffers.

455
00:15:01,136 --> 0:15:02,496
And, it's a lot of work to

456
00:15:02,496 --> 0:15:04,096
implement an efficient ring

457
00:15:04,096 --> 0:15:05,666
buffer that makes sure to

458
00:15:05,666 --> 0:15:07,506
deliver the correct size chunks

459
00:15:07,506 --> 0:15:08,796
of audio to your model.

460
00:15:09,386 --> 0:15:11,966
And so, this step ensures that

461
00:15:11,966 --> 0:15:13,456
if the model expects around 1

462
00:15:13,456 --> 0:15:15,046
second of audio data, that that

463
00:15:15,046 --> 0:15:16,876
will always be what's delivered

464
00:15:16,876 --> 0:15:17,406
to the model.

465
00:15:18,396 --> 0:15:20,726
And then, finally, after the

466
00:15:20,726 --> 0:15:22,306
data's delivered to the model,

467
00:15:22,586 --> 0:15:23,466
your app will receive a

468
00:15:23,466 --> 0:15:25,786
callback, containing the top

469
00:15:25,786 --> 0:15:27,536
classification results for that

470
00:15:27,536 --> 0:15:28,176
piece of audio.

471
00:15:28,996 --> 0:15:31,286
Now, the good thing is, you

472
00:15:31,286 --> 0:15:32,816
don't really have to know any of

473
00:15:32,816 --> 0:15:33,276
this.

474
00:15:33,866 --> 0:15:35,486
Just remember to take your

475
00:15:35,486 --> 0:15:36,996
audio, provide it to

476
00:15:36,996 --> 0:15:38,626
SoundAnalysis framework, and

477
00:15:38,626 --> 0:15:40,166
then handle the results in your

478
00:15:40,166 --> 0:15:40,776
application.

479
00:15:43,696 --> 0:15:45,356
So, let's talk a little bit more

480
00:15:45,356 --> 0:15:46,876
about the results you'll expect

481
00:15:46,876 --> 0:15:48,276
to get from SoundAnalysis.

482
00:15:49,616 --> 0:15:51,496
Audio is a stream, and it

483
00:15:51,496 --> 0:15:53,216
doesn't always have a beginning

484
00:15:53,216 --> 0:15:54,516
and end, like images do.

485
00:15:54,966 --> 0:15:56,986
And, for this reason, the

486
00:15:56,986 --> 0:15:58,206
results that we're working with

487
00:15:58,416 --> 0:15:59,506
might look a little different.

488
00:16:00,196 --> 0:16:02,416
Your results contain a time

489
00:16:02,416 --> 0:16:04,566
range, and this corresponds to

490
00:16:04,606 --> 0:16:05,906
the block of audio that was

491
00:16:05,906 --> 0:16:07,336
analyzed for that result.

492
00:16:08,396 --> 0:16:10,756
In this example, the block size

493
00:16:10,756 --> 0:16:11,716
is specific to model

494
00:16:11,716 --> 0:16:13,506
architecture, and is around 1

495
00:16:13,506 --> 0:16:14,786
second, as you can see.

496
00:16:17,046 --> 0:16:18,496
As you continue providing audio

497
00:16:18,496 --> 0:16:20,016
data to the model, you'll

498
00:16:20,016 --> 0:16:21,566
continue to receive results

499
00:16:21,826 --> 0:16:22,776
containing the top

500
00:16:22,776 --> 0:16:24,516
classifications for that block

501
00:16:24,516 --> 0:16:25,756
of audio that you analyzed.

502
00:16:27,326 --> 0:16:29,586
Now, you might notice that this

503
00:16:29,586 --> 0:16:31,126
second result has overlapped the

504
00:16:31,126 --> 0:16:33,126
previous results by about 50%.

505
00:16:33,726 --> 0:16:35,166
And, this is actually by design.

506
00:16:36,636 --> 0:16:38,066
You want to make sure that every

507
00:16:38,266 --> 0:16:39,196
piece of audio that you're

508
00:16:39,196 --> 0:16:40,956
providing has the opportunity to

509
00:16:40,956 --> 0:16:42,536
fall near the middle of an

510
00:16:42,536 --> 0:16:43,476
analysis window.

511
00:16:43,916 --> 0:16:45,726
Otherwise, it might fall between

512
00:16:45,726 --> 0:16:48,066
two analysis windows, and the

513
00:16:48,066 --> 0:16:49,576
model performance might not be

514
00:16:49,576 --> 0:16:50,096
as good.

515
00:16:51,096 --> 0:16:53,126
And so, the default is 50%

516
00:16:53,126 --> 0:16:54,696
overlap on the analysis,

517
00:16:55,096 --> 0:16:56,476
although it's configurable in

518
00:16:56,506 --> 0:16:57,906
the API, if you have a use case

519
00:16:57,906 --> 0:16:59,146
that requires otherwise.

520
00:17:00,336 --> 0:17:02,216
And, as you continue providing

521
00:17:02,216 --> 0:17:04,715
audio data, you'll continue to

522
00:17:04,715 --> 0:17:05,726
receive results.

523
00:17:05,886 --> 0:17:08,566
And, you can continue pushing

524
00:17:08,566 --> 0:17:09,996
this data, and getting results

525
00:17:09,996 --> 0:17:10,656
for as long as the audio stream

526
00:17:10,656 --> 0:17:10,976
is active.

527
00:17:15,876 --> 0:17:18,175
Now, let's take a quick look at

528
00:17:18,175 --> 0:17:19,766
the API provided by

529
00:17:19,766 --> 0:17:21,016
SoundAnalysis Framework.

530
00:17:23,356 --> 0:17:24,846
Let's say we have an audio file,

531
00:17:24,846 --> 0:17:27,106
and we want to analyze it using

532
00:17:27,106 --> 0:17:28,215
the classifier we've just

533
00:17:28,215 --> 0:17:29,016
trained here today.

534
00:17:30,526 --> 0:17:32,516
To start, we'll create an audio

535
00:17:32,516 --> 0:17:34,736
file analyzer, and provide the

536
00:17:34,736 --> 0:17:36,126
URL to the file we'd like to

537
00:17:36,126 --> 0:17:36,806
analyze.

538
00:17:38,476 --> 0:17:40,506
Then, we'll create a

539
00:17:40,506 --> 0:17:42,716
classifySoundRequest, and then

540
00:17:42,716 --> 0:17:44,756
instantiate MySoundClassifier,

541
00:17:45,136 --> 0:17:46,196
which is the model we trained

542
00:17:46,196 --> 0:17:46,396
here.

543
00:17:49,406 --> 0:17:51,536
Then, we'll add this request to

544
00:17:51,536 --> 0:17:53,796
our analyzer, and provide an

545
00:17:53,796 --> 0:17:55,376
observer, which will handle the

546
00:17:55,376 --> 0:17:56,876
results that the model will

547
00:17:56,876 --> 0:17:57,496
produce.

548
00:17:59,156 --> 0:18:02,366
Finally, we'll analyze the file,

549
00:17:59,156 --> 0:18:02,366
Finally, we'll analyze the file,

550
00:18:02,816 --> 0:18:03,966
which will start scanning

551
00:18:03,966 --> 0:18:05,336
through the file and producing

552
00:18:05,336 --> 0:18:06,016
the results.

553
00:18:09,146 --> 0:18:10,476
Now, on your application side,

554
00:18:11,266 --> 0:18:12,486
you'll need to make sure that

555
00:18:12,486 --> 0:18:14,296
one of your classes implements

556
00:18:14,576 --> 0:18:16,706
the SNResultsObserving protocol.

557
00:18:17,806 --> 0:18:18,866
This is how you'll receive

558
00:18:18,866 --> 0:18:20,106
results from the framework.

559
00:18:21,796 --> 0:18:23,076
The first method you might

560
00:18:23,076 --> 0:18:25,466
implement is request didProduce

561
00:18:25,466 --> 0:18:25,866
result.

562
00:18:26,526 --> 0:18:28,876
This method will be called many

563
00:18:28,876 --> 0:18:29,986
times, potentially.

564
00:18:30,656 --> 0:18:32,976
Once for each new observation

565
00:18:32,976 --> 0:18:33,786
that's available.

566
00:18:34,796 --> 0:18:37,516
You might consider grabbing the

567
00:18:37,666 --> 0:18:39,536
top classification result, and

568
00:18:39,536 --> 0:18:40,886
the time range associated with

569
00:18:40,886 --> 0:18:41,026
it.

570
00:18:41,346 --> 0:18:42,436
And, this is where the logic

571
00:18:42,466 --> 0:18:44,356
would go in your application, to

572
00:18:44,356 --> 0:18:46,766
handle the sound classification

573
00:18:46,766 --> 0:18:46,976
event.

574
00:18:49,596 --> 0:18:50,726
Another method you'll be

575
00:18:50,726 --> 0:18:52,216
interested in is request

576
00:18:52,386 --> 0:18:53,546
didFailWithError.

577
00:18:54,216 --> 0:18:55,586
If analysis fails for any

578
00:18:55,586 --> 0:18:57,106
reason, this method will be

579
00:18:57,106 --> 0:18:57,526
called.

580
00:18:57,526 --> 0:18:59,176
And then, you shouldn't expect

581
00:18:59,176 --> 0:19:00,766
to receive any more results from

582
00:18:59,176 --> 0:19:00,766
to receive any more results from

583
00:19:00,766 --> 0:19:01,496
this analyzer.

584
00:19:02,796 --> 0:19:04,926
Or, if the stream completes

585
00:19:04,926 --> 0:19:06,656
successfully, at the end of the

586
00:19:06,656 --> 0:19:08,486
file, for example, you'll

587
00:19:08,486 --> 0:19:11,246
receive the request didComplete.

588
00:19:13,506 --> 0:19:15,546
So, let's recap what you've seen

589
00:19:15,546 --> 0:19:15,926
today.

590
00:19:17,656 --> 0:19:18,926
You saw how you can train a

591
00:19:18,926 --> 0:19:20,636
sound classifier in Create ML

592
00:19:21,326 --> 0:19:24,596
using your own audio data.

593
00:19:25,656 --> 0:19:27,636
And, take that model and run it

594
00:19:27,636 --> 0:19:29,866
on-device using SoundAnalysis

595
00:19:29,866 --> 0:19:30,216
framework.

596
00:19:32,186 --> 0:19:34,886
For more information, check out

597
00:19:34,886 --> 0:19:36,486
our sound classification article

598
00:19:36,846 --> 0:19:38,496
on developer.apple.com.

599
00:19:39,106 --> 0:19:40,346
And, there you'll find an

600
00:19:40,346 --> 0:19:42,246
example of how to perform sound

601
00:19:42,246 --> 0:19:43,956
classification on your device's

602
00:19:44,006 --> 0:19:45,866
built-in microphone, using AV

603
00:19:45,866 --> 0:19:47,816
Audio Engine, just like the

604
00:19:47,816 --> 0:19:49,466
musical instrument demo you saw

605
00:19:49,466 --> 0:19:49,886
earlier.

606
00:19:53,046 --> 0:19:54,506
Thank you all for listening, and

607
00:19:54,506 --> 0:19:56,186
I can't wait to see how you use

608
00:19:56,186 --> 0:19:57,556
sound classification in your

609
00:19:57,556 --> 0:19:57,976
applications.

610
00:19:58,516 --> 0:20:04,500
[ Applause ]
