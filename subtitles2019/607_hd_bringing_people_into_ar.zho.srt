1
00:00:06,640 --> 0:00:09,643
（带领大家进入AR的世界）

2
00:00:11,578 --> 0:00:12,579
大家好

3
00:00:14,715 --> 0:00:15,949
谢谢你们的到来

4
00:00:18,051 --> 0:00:20,087
这是关于ARKit的一个

5
00:00:20,320 --> 0:00:22,956
我们会向你展示如何带领

6
00:00:24,191 --> 0:00:27,027
我是Adrian 我将会和同事

7
00:00:31,665 --> 0:00:34,902
这周早些时候Apple

8
00:00:35,502 --> 0:00:38,472
这是为渲染逼真内容而

9
00:00:39,139 --> 0:00:42,276
它也是从底层开发用来支持AR

10
00:00:44,645 --> 0:00:48,549
我们也有一个介绍ARKit 3

11
00:00:48,615 --> 0:00:51,251
我们向你展示了今年ARKit

12
00:00:52,019 --> 0:00:55,055
为了深入了解

13
00:00:56,190 --> 0:00:58,492
首先 我将向你展示真人遮挡剔除

14
00:00:58,725 --> 0:01:02,229
接下来 Tanmay会带你了解

15
00:00:58,725 --> 0:01:02,229
接下来 Tanmay会带你了解

16
00:01:02,296 --> 0:01:04,063
动作捕捉是如何工作的

17
00:01:05,432 --> 0:01:06,433
那么 我们开始吧

18
00:01:07,467 --> 0:01:08,802
什么是真人遮挡剔除？

19
00:01:10,604 --> 0:01:15,375
现在使用ARKit 我们已经可以

20
00:01:15,442 --> 0:01:16,443
定位渲染的内容了

21
00:01:16,677 --> 0:01:18,579
不过 如果我们看我身后的视频

22
00:01:18,645 --> 0:01:20,247
我们能看到出现了严重的错误

23
00:01:21,748 --> 0:01:22,783
我们期待看到的是

24
00:01:22,950 --> 0:01:25,853
离相机近的人会挡住

25
00:01:26,086 --> 0:01:28,989
被渲染的内容

26
00:01:30,424 --> 0:01:33,260
使用真人遮挡剔除 我们就能做到了

27
00:01:33,861 --> 0:01:38,665
我们能正确地在场景中处理渲染内容

28
00:01:38,732 --> 0:01:39,733
和人相互遮挡对方

29
00:01:40,167 --> 0:01:43,136
为了了解我们需要做一些什么

30
00:01:43,437 --> 0:01:44,838
我将截取一帧

31
00:01:46,673 --> 0:01:48,509
这里 我们有了照相机的图片

32
00:01:49,009 --> 0:01:51,211
有两个人站在一张桌子周围

33
00:01:51,845 --> 0:01:55,182
我们想要将一个渲染的物件放在

34
00:01:56,617 --> 0:02:01,321
目前为止 在ARKit 2上

35
00:01:56,617 --> 0:02:01,321
目前为止 在ARKit 2上

36
00:02:01,388 --> 0:02:03,690
是简单地将它覆盖在图片上

37
00:02:04,057 --> 0:02:05,392
当我们这样做的时候

38
00:02:05,559 --> 0:02:08,662
我们看到了一个错误的遮挡关系

39
00:02:09,663 --> 0:02:11,365
这不是我们所期待的

40
00:02:11,765 --> 0:02:14,668
我们想要将红色的叉变为一个绿色

41
00:02:15,269 --> 0:02:18,839
为了实现 我们需要明白当有人比

42
00:02:18,906 --> 0:02:20,641
渲染内容要靠近照相机时

43
00:02:21,008 --> 0:02:25,579
我们需要正确地保证渲染物件

44
00:02:27,414 --> 0:02:30,317
这本质上是一个深度排序的问题

45
00:02:31,018 --> 0:02:33,554
为了明白我们会如何解决这个问题

46
00:02:33,620 --> 0:02:35,189
我会来分解这个场景

47
00:02:36,924 --> 0:02:39,793
这是一张相同倾斜角度的图片

48
00:02:40,093 --> 0:02:44,431
我们将场景分割散开

49
00:02:44,498 --> 0:02:45,499
不同的深度基准面

50
00:02:46,033 --> 0:02:49,603
我们在不同的深度基准面上

51
00:02:49,670 --> 0:02:50,671
真实和渲染的物件混合在了一起

52
00:02:51,104 --> 0:02:54,408
每个人在他们自己的深度基准面上

53
00:02:54,708 --> 0:02:57,578
中间是渲染的物件

54
00:03:00,280 --> 0:03:03,617
对于渲染的物件

55
00:03:03,684 --> 0:03:06,787
它的确切位置

56
00:03:07,688 --> 0:03:11,124
我们为了对真实的内容也实现了

57
00:03:11,191 --> 0:03:14,194
我们需要知道人在场景中的位置

58
00:03:14,862 --> 0:03:18,365
为了实现 我们加入了两个新的缓冲

59
00:03:19,900 --> 0:03:24,471
我们加入了段缓冲器

60
00:03:24,538 --> 0:03:25,539
在场景中位置

61
00:03:26,306 --> 0:03:30,177
我们也给了你一个相应的深度缓冲

62
00:03:30,244 --> 0:03:32,779
它为你提供人的深度位置信息

63
00:03:34,815 --> 0:03:39,720
现在 最酷的事情是我们

64
00:03:39,786 --> 0:03:44,992
是由A12芯片

65
00:03:45,058 --> 0:03:48,428
只需相机的照片就可以

66
00:03:49,930 --> 0:03:54,768
我们的这两个新缓冲将作为

67
00:03:54,835 --> 0:03:56,370
两个新属性暴露给ARFrame

68
00:03:56,436 --> 0:03:59,373
segmentationBuffer

69
00:04:03,277 --> 0:04:05,946
因为我们想要使用缓冲来实现

70
00:04:06,213 --> 0:04:10,517
我们也要将它们以相同的频率

71
00:04:11,185 --> 0:04:14,388
所以 当你相机的帧以60帧每秒

72
00:04:15,088 --> 0:04:19,226
我们也能以60帧每秒的频率

73
00:04:21,995 --> 0:04:25,399
我们也想要这些缓冲和相机的图片

74
00:04:25,465 --> 0:04:26,533
分辨率一样

75
00:04:27,601 --> 0:04:31,972
不过 为了实时完成这个功能

76
00:04:32,039 --> 0:04:33,841
神经网络只能检视到一张小的照片

77
00:04:34,575 --> 0:04:37,077
所以 如果你拿到神经网络的输出

78
00:04:37,811 --> 0:04:40,848
我们放大它

79
00:04:40,914 --> 0:04:43,083
神经网络并未检视到

80
00:04:44,484 --> 0:04:49,223
所以为了弥补细节的损耗

81
00:04:50,691 --> 0:04:51,892
我们使用遮片提取

82
00:04:52,793 --> 0:04:58,532
遮片提取做了什么呢 根本上来说

83
00:04:58,999 --> 0:05:00,601
作为一个指导 接着看相机的照片

84
00:04:58,999 --> 0:05:00,601
作为一个指导 接着看相机的照片

85
00:05:00,667 --> 0:05:02,936
为了了解遗失的细节是什么

86
00:05:04,071 --> 0:05:07,708
现在 使用一张遮片提取的照片

87
00:05:07,774 --> 0:05:11,745
和estimatedDepthData

88
00:05:11,979 --> 0:05:14,748
接着 我们能定位他们

89
00:05:15,382 --> 0:05:18,151
最后 我们解决深度排序的问题

90
00:05:18,218 --> 0:05:20,454
接着 我们就能重绘场景了

91
00:05:22,189 --> 0:05:23,924
这使用了很多的技术

92
00:05:24,458 --> 0:05:26,827
我们想要开发者能尽可能简单地

93
00:05:26,894 --> 0:05:28,762
使用它

94
00:05:28,996 --> 0:05:31,832
所以我们提供了三种不同的方式

95
00:05:33,367 --> 0:05:38,338
首先 我们有RealityKit

96
00:05:39,072 --> 0:05:43,377
但如果你已使用SceneKit了

97
00:05:43,443 --> 0:05:46,046
真人遮挡剔除的支持

98
00:05:46,613 --> 0:05:48,582
如果你有了自己的渲染器

99
00:05:48,982 --> 0:05:50,984
或是使用了一个第三方渲染

100
00:05:51,451 --> 0:05:56,123
我们为你提供了建筑块来让你

101
00:05:56,323 --> 0:05:58,458
协助完成真人遮挡剔除

102
00:05:59,760 --> 0:06:01,828
那么 我们来看一下我们如何使用

103
00:05:59,760 --> 0:06:01,828
那么 我们来看一下我们如何使用

104
00:06:03,830 --> 0:06:09,102
如果你要创建一个新的AR app

105
00:06:10,037 --> 0:06:14,641
它有新的UI元素叫做ARView

106
00:06:14,842 --> 0:06:19,279
简单易用的API

107
00:06:19,713 --> 0:06:24,952
更近一步混合了真实

108
00:06:26,920 --> 0:06:29,723
它也在内部支持真人遮挡剔除功能

109
00:06:30,257 --> 0:06:32,159
如果你参加过新功能介绍的演讲

110
00:06:32,226 --> 0:06:36,063
你已看过在一个ARView中启用

111
00:06:36,129 --> 0:06:37,130
真人遮挡剔除功能的一段实时示例

112
00:06:37,464 --> 0:06:40,000
为了深入了解它

113
00:06:40,601 --> 0:06:41,702
我们来看一些代码

114
00:06:43,637 --> 0:06:47,241
这里是我的视图控制器里的

115
00:06:47,608 --> 0:06:49,076
所以 我要做的第一件事是

116
00:06:49,142 --> 0:06:51,245
来确保是否支持这个功能

117
00:06:52,646 --> 0:06:57,351
我通过检查我的配置 就是这个

118
00:06:58,085 --> 0:07:01,288
我需要一个新的属性叫做

119
00:06:58,085 --> 0:07:01,288
我需要一个新的属性叫做

120
00:07:01,588 --> 0:07:04,424
我们将要使用FrameSemantics的

121
00:07:04,491 --> 0:07:06,560
来启用真人遮挡剔除功能

122
00:07:07,728 --> 0:07:11,999
只要我知道它在我的配置中被支持

123
00:07:12,733 --> 0:07:16,904
我只需在我的配置中设置

124
00:07:17,271 --> 0:07:19,506
接下来 当会话开始运行时

125
00:07:19,573 --> 0:07:24,344
ARView会自动地为

126
00:07:24,411 --> 0:07:25,679
启用真人遮挡剔除功能

127
00:07:27,381 --> 0:07:31,451
所以 我们所需做的是

128
00:07:31,518 --> 0:07:34,788
使用我们今年介绍的新属性

129
00:07:35,055 --> 0:07:39,459
就像你在之前的例子中看到的

130
00:07:39,893 --> 0:07:42,029
但我们也可以只使用

131
00:07:42,095 --> 0:07:44,565
当你想要启用一个似气象员的体验

132
00:07:46,166 --> 0:07:50,938
现在 我们推荐使用ARView

133
00:07:51,205 --> 0:07:54,708
原因是因为它内部集成了一个

134
00:07:55,442 --> 0:07:58,612
那意味着整个绘图管线都能意识到

135
00:07:58,679 --> 0:08:00,747
场景中有人

136
00:07:58,679 --> 0:08:00,747
场景中有人

137
00:08:00,981 --> 0:08:04,418
它也因此可以使用这个功能来

138
00:08:04,484 --> 0:08:05,786
处理透明的物件

139
00:08:06,687 --> 0:08:10,357
它也会同时创建最佳的性能体验

140
00:08:11,491 --> 0:08:14,628
如果你担心使用真人遮挡剔除

141
00:08:14,695 --> 0:08:17,331
的用户体验

142
00:08:17,397 --> 0:08:19,499
我想为你带来一部短片

143
00:08:38,018 --> 0:08:39,285
这是Swiftstrike

144
00:08:39,720 --> 0:08:43,056
它是一个很酷的示例

145
00:08:43,390 --> 0:08:46,727
我强烈建议你们中

146
00:08:48,061 --> 0:08:49,830
那么 如果你已经在使用

147
00:08:51,431 --> 0:08:54,301
我们看看我如何在SceneKit

148
00:08:56,703 --> 0:09:00,440
如果你已在使用ARSCNView

149
00:08:56,703 --> 0:09:00,440
如果你已在使用ARSCNView

150
00:09:00,674 --> 0:09:03,310
一样非常相似的方式启用

151
00:09:04,044 --> 0:09:07,981
我们要做是在我们的配置中设置

152
00:09:08,048 --> 0:09:10,617
ARSCNView会自动地

153
00:09:11,752 --> 0:09:14,988
但实现SceneKit和

154
00:09:15,055 --> 0:09:19,993
SceneKit做了一个

155
00:09:20,260 --> 0:09:21,962
那对你具体意味着什么呢

156
00:09:22,029 --> 0:09:24,731
基于你设置的深度 它可能不会

157
00:09:24,798 --> 0:09:26,333
非常好地处理透明的物体

158
00:09:28,502 --> 0:09:31,171
最后 假如我有我

159
00:09:32,239 --> 0:09:36,076
我们想要让你在你自己

160
00:09:36,143 --> 0:09:39,213
实现真人遮挡剔除功能

161
00:09:40,747 --> 0:09:43,884
它能让你完全控制效果图

162
00:09:45,886 --> 0:09:48,722
我们想要给你更多的自由度

163
00:09:48,889 --> 0:09:52,559
为你提供简单易用的API的同时

164
00:09:52,626 --> 0:09:53,627
这个很棒的功能

165
00:09:54,228 --> 0:09:57,364
那么 在我为你展示如何做之前

166
00:09:58,498 --> 0:10:02,836
我们的这个来自神经网络的

167
00:09:58,498 --> 0:10:02,836
我们的这个来自神经网络的

168
00:10:03,070 --> 0:10:05,272
处理一张比较小的图片

169
00:10:05,339 --> 0:10:06,540
接下来 我们介绍了遮片提取

170
00:10:06,607 --> 0:10:08,976
为了修复一些遗失的细节

171
00:10:10,277 --> 0:10:15,249
当我们自定义效果图时

172
00:10:15,315 --> 0:10:18,285
它通过使用Metal

173
00:10:18,485 --> 0:10:22,122
为你提供遮片的纹理

174
00:10:22,656 --> 0:10:25,192
我们来用一个例子来看如何实现

175
00:10:27,194 --> 0:10:29,496
那么 这里是我的自定义构建函数

176
00:10:29,830 --> 0:10:34,468
我要做的第一件事是确认这个功能

177
00:10:35,235 --> 0:10:39,173
只要我实现后 我只需调用

178
00:10:39,239 --> 0:10:41,241
我为它提供frame

179
00:10:41,475 --> 0:10:45,746
它返回给我Metal纹理

180
00:10:45,812 --> 0:10:49,917
我能使用它们 最后

181
00:10:51,585 --> 0:10:55,822
我们为ARKit增加的类叫做

182
00:10:55,889 --> 0:10:59,059
就像你在例子中看到的那样 它获取

183
00:10:59,226 --> 0:11:01,628
返回一张你能用到的纹理

184
00:10:59,226 --> 0:11:01,628
返回一张你能用到的纹理

185
00:11:02,362 --> 0:11:03,830
不过 我们还没做完

186
00:11:05,365 --> 0:11:08,769
和segmentationBuffer类似

187
00:11:09,036 --> 0:11:13,473
estimatedDepthData也低

188
00:11:13,540 --> 0:11:15,642
覆盖在我们的遮片图片上

189
00:11:16,844 --> 0:11:18,846
我们看到它们可能不太一致

190
00:11:19,346 --> 0:11:22,516
我们可以使用遮片的没有阿尔法值的

191
00:11:22,683 --> 0:11:25,586
更重要的是

192
00:11:25,652 --> 0:11:27,554
相应深度值的阿尔法值

193
00:11:31,191 --> 0:11:34,294
现在 虽然遮片已经拥有了一些

194
00:11:34,361 --> 0:11:36,630
我们不能真的修正它自己的阿尔法值

195
00:11:36,697 --> 0:11:39,266
取而代之的是 我们需要修正

196
00:11:41,468 --> 0:11:44,771
那么 我们回到我之前的例子

197
00:11:45,873 --> 0:11:49,376
这里我们有我添加的一行代码

198
00:11:49,443 --> 0:11:51,512
为了让我使用我自定义的效果图

199
00:11:52,045 --> 0:11:56,750
所以我加入了额外的函数

200
00:11:57,084 --> 0:12:00,254
非常相似地 它也获取frame和

201
00:11:57,084 --> 0:12:00,254
非常相似地 它也获取frame和

202
00:12:00,454 --> 0:12:02,022
返回一张纹理图

203
00:12:04,157 --> 0:12:08,128
你看它的API 它和我们生成

204
00:12:08,762 --> 0:12:12,933
返回纹理图 获取frame和

205
00:12:14,001 --> 0:12:18,338
这能保证遮片中

206
00:12:18,405 --> 0:12:20,874
都有一个相应的深度值

207
00:12:20,941 --> 0:12:23,677
我们可以用它来处理最终效果图

208
00:12:25,345 --> 0:12:29,850
利用这个扩大了的深度和遮片

209
00:12:29,917 --> 0:12:30,884
效果图中

210
00:12:32,853 --> 0:12:36,056
效果图通常在片段着色器

211
00:12:36,523 --> 0:12:39,960
那么 我们来看一个示例着色器

212
00:12:41,962 --> 0:12:45,799
我从通常需要怎么

213
00:12:46,033 --> 0:12:49,736
我创建了一个相机图片

214
00:12:50,370 --> 0:12:53,841
因为我们要做遮挡剔除

215
00:12:55,342 --> 0:12:58,278
接下来 我通常会

216
00:12:58,345 --> 0:13:01,648
我在一张真实图片上覆盖

217
00:12:58,345 --> 0:13:01,648
我在一张真实图片上覆盖

218
00:13:02,082 --> 0:13:03,817
传给它渲染阿尔法值

219
00:13:05,219 --> 0:13:08,522
下一个部分 为了实现

220
00:13:08,589 --> 0:13:12,526
我也创建了一个遮片和

221
00:13:13,527 --> 0:13:18,632
接着我确保对比dilatedDepth

222
00:13:18,866 --> 0:13:22,469
如我发现dilatedDepth里的

223
00:13:22,536 --> 0:13:25,339
则意味着那里可能有一个人

224
00:13:25,672 --> 0:13:29,776
接下来 我通过混合了照像机

225
00:13:30,244 --> 0:13:34,348
不过 如果渲染内容离照相机更近

226
00:13:34,414 --> 0:13:38,585
如以往一样 将渲染内容

227
00:13:39,520 --> 0:13:44,458
这样 我们终于在我们自定义

228
00:13:44,525 --> 0:13:46,260
实现了真人遮挡剔除功能

229
00:13:55,169 --> 0:13:59,706
因为这个功能运用了神经网络引擎和

230
00:13:59,773 --> 0:14:02,442
它支持使用了A12及之后的设备

231
00:13:59,773 --> 0:14:02,442
它支持使用了A12及之后的设备

232
00:14:03,810 --> 0:14:06,613
我也在室内环境中运行良好

233
00:14:07,381 --> 0:14:10,784
你看到的这些所有视频中

234
00:14:10,851 --> 0:14:13,787
但这个功能也适用于

235
00:14:15,022 --> 0:14:17,291
它也适用于多人场景

236
00:14:18,358 --> 0:14:21,628
在我邀请Tanmay上台为大家

237
00:14:21,695 --> 0:14:22,863
我们来快速回顾一下

238
00:14:24,598 --> 0:14:29,636
使用真人遮挡剔除功能

239
00:14:29,703 --> 0:14:31,305
和真实内容间正确处理遮挡关系

240
00:14:33,440 --> 0:14:35,843
如果你要创建一个新的app

241
00:14:35,909 --> 0:14:40,848
使用RealityKit和ARView

242
00:14:41,682 --> 0:14:44,184
如果你已经在你的app中使用了

243
00:14:44,251 --> 0:14:47,888
我们也在ARSCNView中

244
00:14:49,223 --> 0:14:52,659
如果你有你自己的渲染器

245
00:14:52,726 --> 0:14:56,463
调用ARMatteGenerator

246
00:14:56,530 --> 0:14:58,632
至你自己的渲染器中

247
00:14:59,333 --> 0:15:02,970
下面 有请Tanmay

248
00:14:59,333 --> 0:15:02,970
下面 有请Tanmay

249
00:15:03,036 --> 0:15:04,037
动作捕捉

250
00:15:06,340 --> 0:15:07,341
（动作捕捉）

251
00:15:10,878 --> 0:15:12,045
谢谢Adrian

252
00:15:12,679 --> 0:15:16,650
大家好 我是Tanmay

253
00:15:16,717 --> 0:15:19,720
我们今年引入的新技术

254
00:15:20,087 --> 0:15:21,388
动作捕捉

255
00:15:25,826 --> 0:15:28,095
那么 什么是动作捕捉呢？

256
00:15:28,729 --> 0:15:31,865
它就是用来捕捉人的动作的

257
00:15:33,200 --> 0:15:36,537
你看到了一个人

258
00:15:36,603 --> 0:15:41,909
你用一个虚拟的角色

259
00:15:41,975 --> 0:15:45,546
使那个角色表现出和你看到的一样的

260
00:15:45,746 --> 0:15:49,283
我们试着能让你的app

261
00:15:50,484 --> 0:15:51,685
现在 我们开始深入了解吧

262
00:15:52,486 --> 0:15:56,156
我们想要这个角色来模仿

263
00:15:56,356 --> 0:16:00,160
但在我们开始前

264
00:15:56,356 --> 0:16:00,160
但在我们开始前

265
00:16:00,227 --> 0:16:03,463
怎样的动画效果

266
00:16:04,631 --> 0:16:07,034
所以 这是一个虚拟角色的示例

267
00:16:07,434 --> 0:16:10,270
我们给它照一张X光片子

268
00:16:12,005 --> 0:16:14,474
你能看到它有两个主要部分

269
00:16:14,708 --> 0:16:17,244
它的外层 叫做一个网格

270
00:16:17,644 --> 0:16:20,781
骨性结构里面

271
00:16:21,782 --> 0:16:25,519
结合这两者就是这个完整的角色

272
00:16:26,787 --> 0:16:30,791
骨架是整个角色背后的驱动力

273
00:16:31,191 --> 0:16:34,695
它包含了我们用来控制它动作的

274
00:16:35,596 --> 0:16:39,833
所以 为了让角色动起来

275
00:16:39,900 --> 0:16:40,968
一样的动作动起来

276
00:16:42,102 --> 0:16:43,804
所以 第一步怎么做呢？

277
00:16:43,971 --> 0:16:48,809
我们有一个人 第一步我们

278
00:16:50,777 --> 0:16:52,346
这就是它的看上去的样子

279
00:16:52,579 --> 0:16:56,550
这要这个骨架动了

280
00:16:57,985 --> 0:17:02,990
整个虚拟角色在自动地模仿你

281
00:16:57,985 --> 0:17:02,990
整个虚拟角色在自动地模仿你

282
00:17:05,325 --> 0:17:06,393
所以 我们如何实现呢？

283
00:17:07,794 --> 0:17:09,396
我们如何让这个骨架动起来呢？

284
00:17:09,762 --> 0:17:13,165
基于这张图片 我们使用机器学习

285
00:17:14,134 --> 0:17:16,936
来首次估量图片中人的姿势

286
00:17:17,304 --> 0:17:22,643
我们使用这个姿势来创建一个完整

287
00:17:24,344 --> 0:17:27,481
最后 我们使用这个骨架

288
00:17:27,548 --> 0:17:29,216
给你最终的角色

289
00:17:30,083 --> 0:17:34,821
我们在ARKit整合了你看到的

290
00:17:36,924 --> 0:17:38,492
为了完整的概括

291
00:17:39,226 --> 0:17:43,096
我们在今年的ARKit加入了

292
00:17:43,764 --> 0:17:48,802
使用它 你能在你的设备中实时

293
00:17:49,636 --> 0:17:54,675
它和RealityKit一起能完美工作

294
00:17:54,942 --> 0:17:58,278
接着渲染在你的屏幕中

295
00:17:58,912 --> 0:18:03,016
它由机器学习驱动

296
00:17:58,912 --> 0:18:03,016
它由机器学习驱动

297
00:18:04,051 --> 0:18:07,888
我们在A12及之后设备支持这功能

298
00:18:09,223 --> 0:18:12,626
那么 现在利用这项技术

299
00:18:12,693 --> 0:18:14,328
你能用它做什么？

300
00:18:15,796 --> 0:18:18,866
对于新手来说 你可以创建

301
00:18:18,932 --> 0:18:20,300
虚拟角色

302
00:18:20,501 --> 0:18:22,369
你可以在AR中拥有一个你自己的

303
00:18:23,136 --> 0:18:25,873
这是可以立刻实现的

304
00:18:26,306 --> 0:18:29,843
除此之外 你可以在其他的

305
00:18:30,043 --> 0:18:31,245
一些使用案例中用到它

306
00:18:32,212 --> 0:18:37,017
比如 你可以创建你自己的监测

307
00:18:37,084 --> 0:18:38,752
人们动作的模型来进一步强化它

308
00:18:40,387 --> 0:18:43,690
你能使用它来创建分析动作的工具

309
00:18:43,757 --> 0:18:47,294
比如高尔夫挥杆动作

310
00:18:47,361 --> 0:18:50,497
或是在锻炼中的姿势是否正确

311
00:18:52,799 --> 0:18:56,670
现在 因为人在场景中有了一个

312
00:18:56,904 --> 0:19:00,741
你可以和任何你喜欢的虚拟物件

313
00:18:56,904 --> 0:19:00,741
你可以和任何你喜欢的虚拟物件

314
00:19:02,242 --> 0:19:06,213
它支持场景中所有的虚拟物件

315
00:19:07,681 --> 0:19:12,186
最后 你也能使用它来分析图片

316
00:19:12,419 --> 0:19:16,857
因为我们也在图像空间

317
00:19:17,024 --> 0:19:21,662
你可以创建它来编辑工具

318
00:19:24,798 --> 0:19:29,703
这甚至不需要涵盖全部的可能性

319
00:19:29,903 --> 0:19:31,972
你可以远程启用它

320
00:19:32,406 --> 0:19:34,675
你还可以用它来做其他很多事情

321
00:19:36,176 --> 0:19:38,879
现在 我来向你展示如何

322
00:19:38,946 --> 0:19:40,013
动作捕捉

323
00:19:40,814 --> 0:19:44,651
基于不同的使用场景

324
00:19:45,786 --> 0:19:49,223
第一种

325
00:19:50,324 --> 0:19:52,860
如果你只是想快速地让一个角色

326
00:19:53,760 --> 0:19:55,996
这个高级的API能帮你

327
00:19:56,630 --> 0:20:02,102
如果你想启用高级的使用场景

328
00:19:56,630 --> 0:20:02,102
如果你想启用高级的使用场景

329
00:20:02,169 --> 0:20:07,774
或在场景中与3D物件交互

330
00:20:07,841 --> 0:20:10,944
来单独取出骨架中的每个元素

331
00:20:12,112 --> 0:20:14,081
它是为你而设计的

332
00:20:15,115 --> 0:20:19,453
最后 如果你的使用场景需要

333
00:20:19,686 --> 0:20:22,856
位于2D版本的图片空间的骨架

334
00:20:22,923 --> 0:20:25,492
或用来编辑工具 或用于其他事情

335
00:20:25,559 --> 0:20:27,728
我们也提供了相关的入口

336
00:20:29,796 --> 0:20:32,799
那么 我们从RealityKit

337
00:20:34,201 --> 0:20:37,204
你知道的 我们今年

338
00:20:37,738 --> 0:20:43,010
使用RealityKit的API

339
00:20:43,076 --> 0:20:46,113
就能追踪一个人

340
00:20:47,514 --> 0:20:51,685
为此我们提供了一个

341
00:20:52,486 --> 0:20:55,289
你也可以加入你自定义的角色

342
00:20:55,455 --> 0:20:58,058
基于我们提供的示例结构

343
00:20:58,525 --> 0:21:02,429
所以 基于我们提供的示例结构

344
00:20:58,525 --> 0:21:02,429
所以 基于我们提供的示例结构

345
00:21:02,496 --> 0:21:07,801
你可以使用任意的网格

346
00:21:07,868 --> 0:21:09,036
你想要的任意的角色

347
00:21:11,238 --> 0:21:15,909
最后 被追踪的人非常容易通过一个

348
00:21:15,976 --> 0:21:20,113
在这里被访问到

349
00:21:20,747 --> 0:21:24,551
它会自动地收集我们在动作捕捉

350
00:21:24,618 --> 0:21:25,619
中需要的所有转换信息

351
00:21:28,522 --> 0:21:29,690
那么 首先

352
00:21:31,825 --> 0:21:36,330
你在ARView获取的

353
00:21:36,597 --> 0:21:37,698
你门知道的

354
00:21:37,764 --> 0:21:42,069
它是结合了AR和RealityKit

355
00:21:42,436 --> 0:21:47,741
它由一个新的配置

356
00:21:48,342 --> 0:21:53,947
只要你启用了它

357
00:21:54,181 --> 0:21:58,385
封装在锚点整体里的一个叫做

358
00:21:58,452 --> 0:22:00,220
的特殊类型

359
00:21:58,452 --> 0:22:00,220
的特殊类型

360
00:22:00,821 --> 0:22:02,322
那么 我来为你简单介绍一下

361
00:22:02,389 --> 0:22:05,259
bodyTrackedEntity

362
00:22:06,627 --> 0:22:09,997
一个身体被追踪的整体代表着

363
00:22:10,731 --> 0:22:14,268
它包含下面的骨骼和它的位置

364
00:22:15,736 --> 0:22:19,606
它被实时跟踪 每帧都会更新

365
00:22:20,174 --> 0:22:25,045
最后 它结合了骨骼来

366
00:22:25,512 --> 0:22:27,381
和为你提供了完整的角色

367
00:22:28,949 --> 0:22:32,119
现在 我们来快速过一遍让角色

368
00:22:32,186 --> 0:22:33,754
你会发现非常简单

369
00:22:33,820 --> 0:22:35,622
你只需跟随这3个步骤

370
00:22:36,590 --> 0:22:38,759
第一步是加载一个角色

371
00:22:40,160 --> 0:22:45,632
为了自动地异步加载一个被追踪的人

372
00:22:45,699 --> 0:22:48,869
entity.loadBodyTrackedAsync

373
00:22:50,003 --> 0:22:55,209
以及你可以使用.sync来捕捉

374
00:22:55,275 --> 0:22:56,877
或者 如果没有任何问题的话

375
00:22:56,944 --> 0:22:59,513
你会在返回值块中获取你的角色

376
00:22:59,980 --> 0:23:03,283
这个角色是

377
00:22:59,980 --> 0:23:03,283
这个角色是

378
00:23:07,087 --> 0:23:11,458
在我们示例代码的包中 我们有一个

379
00:23:11,625 --> 0:23:15,596
如果你为这个文件提供文件机器人

380
00:23:15,662 --> 0:23:19,066
为机器人网格添加骨骼

381
00:23:20,501 --> 0:23:24,571
接下来 第二部是来获取

382
00:23:24,638 --> 0:23:25,739
你想要放置角色的位置信息

383
00:23:27,107 --> 0:23:30,744
比如 如果你想要将角色放在追踪人

384
00:23:30,811 --> 0:23:34,515
你可以通过使用

385
00:23:34,581 --> 0:23:37,184
的参数.body来获取位置信息

386
00:23:37,684 --> 0:23:40,420
请留意这只是一个例子

387
00:23:40,487 --> 0:23:42,890
它也能被放置到其他位置

388
00:23:43,223 --> 0:23:47,661
在地板上 在桌面上或其他任意地方

389
00:23:47,728 --> 0:23:49,997
位置的一个锚点

390
00:23:51,164 --> 0:23:53,467
角色会一直模仿这个人

391
00:23:55,903 --> 0:23:59,940
最后 将你的角色放在这里

392
00:24:01,341 --> 0:24:04,077
你可以驱动你的角色了

393
00:24:08,448 --> 0:24:12,653
那么 你可能会思考

394
00:24:12,719 --> 0:24:14,855
角色来替换这个机器人呢

395
00:24:16,857 --> 0:24:20,928
就像我之前说的 我们有一个

396
00:24:21,195 --> 0:24:24,531
这个usdz文件有一个完整的结构

397
00:24:24,598 --> 0:24:29,236
如果你自定义的模型遵循了和它

398
00:24:30,070 --> 0:24:33,941
另外 我们提供的下面的骨骼

399
00:24:34,274 --> 0:24:38,178
是一个极高保真的包含了91个

400
00:24:38,712 --> 0:24:42,382
这就是你要了解的所有信息了

401
00:24:45,485 --> 0:24:46,486
有很多 对吧？

402
00:24:46,954 --> 0:24:51,425
这些只是一般的关节点

403
00:24:52,759 --> 0:24:54,895
如果你的角色遵循了这个命名方案

404
00:24:54,962 --> 0:24:56,763
你可以直接在RealityKit

405
00:25:00,767 --> 0:25:04,872
这是一个快速简单的方式来载入

406
00:25:05,472 --> 0:25:09,009
现在 我们来看低级别的API

407
00:25:10,410 --> 0:25:14,615
这里 我们提供了一些接触骨骼中

408
00:25:16,250 --> 0:25:19,086
我们通过了一个非常简单易用的

409
00:25:21,054 --> 0:25:23,257
你能启用所有的这些高阶的

410
00:25:23,323 --> 0:25:26,727
我们之前讨论过

411
00:25:26,793 --> 0:25:31,265
或是为你模型提供一个输入

412
00:25:33,967 --> 0:25:38,639
最后 我们提供的骨骼包含了

413
00:25:38,705 --> 0:25:41,408
叫做ARBodyAnchor

414
00:25:42,543 --> 0:25:46,580
ARBodyAnchor

415
00:25:46,780 --> 0:25:47,781
整个数据结构的初始点

416
00:25:48,849 --> 0:25:50,984
这就是数据结构的样子

417
00:25:52,519 --> 0:25:57,424
上面有ARBodyAnchor

418
00:26:00,360 --> 0:26:03,397
那么 我们来看看这个结构

419
00:26:05,799 --> 0:26:08,135
ARBodyAnchor

420
00:26:08,202 --> 0:26:09,870
它包含一个几何物体

421
00:26:10,170 --> 0:26:13,307
现在 这个几何物体本身是一个骨架

422
00:26:13,473 --> 0:26:15,542
这就是骨架的样子

423
00:26:15,976 --> 0:26:19,780
它包含了节点和边界

424
00:26:21,048 --> 0:26:23,050
它也包含一个转换

425
00:26:24,084 --> 0:26:26,854
这个转换只是锚点在

426
00:26:26,920 --> 0:26:29,489
旋转和转换矩阵形式中的位置

427
00:26:30,257 --> 0:26:33,026
在这里 访问骨架是我们主要

428
00:26:33,227 --> 0:26:34,595
那么 我们开始吧

429
00:26:36,096 --> 0:26:38,131
我们展现给你的这个骨架是什么呢？

430
00:26:38,432 --> 0:26:39,466
它是一个由节点组成的几何体

431
00:26:39,666 --> 0:26:43,170
它代表节点

432
00:26:43,637 --> 0:26:46,273
你看到的绿色的点和黄色的点

433
00:26:46,607 --> 0:26:49,276
它包含了表示着骨头的边界

434
00:26:49,343 --> 0:26:50,811
你看到的白色的线

435
00:26:51,245 --> 0:26:53,413
向你展示了节点被链接

436
00:26:55,382 --> 0:26:59,086
只要所有的节点被连接了

437
00:27:00,287 --> 0:27:05,259
我们称这个骨架为ARSkeleton

438
00:27:05,425 --> 0:27:07,661
ARBodyAnchor来访问它

439
00:27:11,798 --> 0:27:15,836
骨架的根节点 几何体最上面的点

440
00:27:15,903 --> 0:27:19,940
在几何体的层级中

441
00:27:21,074 --> 0:27:24,178
那么 我们继续

442
00:27:25,145 --> 0:27:28,148
这里所谓的定义

443
00:27:28,482 --> 0:27:30,083
它包含了两个组件

444
00:27:30,584 --> 0:27:33,487
骨架中展示的所有关节点的名字

445
00:27:33,687 --> 0:27:36,223
和它们之间的连接

446
00:27:36,290 --> 0:27:37,291
它展示了如何将关节连接在一起

447
00:27:37,991 --> 0:27:40,260
那么 我们来看一下这些属性

448
00:27:42,296 --> 0:27:46,166
这里 我们在骨架上标记

449
00:27:46,233 --> 0:27:50,571
这些关节点有具有意义的语义名字

450
00:27:50,637 --> 0:27:54,641
右肩 左手 右手等等 和人类似

451
00:27:55,809 --> 0:27:58,812
这里我想说明一下

452
00:27:58,879 --> 0:28:02,749
它们是你看到的人身上估量出来的

453
00:27:58,879 --> 0:28:02,749
它们是你看到的人身上估量出来的

454
00:28:03,016 --> 0:28:05,152
黄色的是没有被跟随的

455
00:28:05,485 --> 0:28:08,488
它们只是跟随离它们

456
00:28:11,158 --> 0:28:15,429
放大 关注右手

457
00:28:15,495 --> 0:28:17,731
右手 右胳膊肘和右肩

458
00:28:18,432 --> 0:28:20,601
它们遵循了父子关系

459
00:28:21,134 --> 0:28:25,439
你的手是胳膊肘的孩子

460
00:28:25,973 --> 0:28:28,742
这也包含了骨架的剩余部分

461
00:28:29,009 --> 0:28:30,978
以此来为你提供完整的层级

462
00:28:36,350 --> 0:28:39,953
我们现在知道了所有被调用的关节点

463
00:28:40,354 --> 0:28:42,289
但我们如何确定它们的位置？

464
00:28:43,023 --> 0:28:46,293
我们提供了两种方式来访问所有

465
00:28:47,261 --> 0:28:49,830
第一个是和它父辈有关联的

466
00:28:50,397 --> 0:28:53,667
如果你想要你右手的位置关联到

467
00:28:54,001 --> 0:28:57,237
你可以通过调用localTransform

468
00:28:57,304 --> 0:29:00,107
为它提供

469
00:28:57,304 --> 0:29:00,107
为它提供

470
00:29:01,675 --> 0:29:04,945
但如果你想要改变相关的根节点

471
00:29:05,012 --> 0:29:08,248
是髋关节 你可以调用

472
00:29:08,315 --> 0:29:11,685
以及再一次提供相同的

473
00:29:13,520 --> 0:29:17,057
现在 如果你不想单独访问

474
00:29:17,124 --> 0:29:21,962
但你想要一个包含了所有

475
00:29:22,362 --> 0:29:26,466
你一可以通过使用

476
00:29:26,533 --> 0:29:28,402
和modelTransforms

477
00:29:28,769 --> 0:29:33,874
它会返回给你一个包含所有关节点

478
00:29:34,508 --> 0:29:37,311
如果你想要它关联至你的父辈

479
00:29:37,377 --> 0:29:39,046
你可以使用

480
00:29:39,213 --> 0:29:42,349
如你想要它关联至根节点 你可使用

481
00:29:44,618 --> 0:29:46,987
所以 现在我们来仔细观察这个

482
00:29:47,221 --> 0:29:50,791
我们来通过代码了解如何使用

483
00:29:52,492 --> 0:29:55,529
你从遍历场景中所有的锚点开始

484
00:29:56,263 --> 0:29:57,998
只要找到bodyAnchor

485
00:29:59,099 --> 0:30:01,702
只要你有了bodyAnchor

486
00:29:59,099 --> 0:30:01,702
只要你有了bodyAnchor

487
00:30:01,768 --> 0:30:04,004
bodyAnchor位于

488
00:30:05,305 --> 0:30:08,609
你能使用bodyAnchor的

489
00:30:09,943 --> 0:30:13,046
因为在我们的几何体中

490
00:30:14,047 --> 0:30:18,185
所以bodyAnchor.transform会

491
00:30:19,453 --> 0:30:22,523
只要你有了锚点的变换

492
00:30:22,589 --> 0:30:25,792
你可能需要访问几何体的锚点

493
00:30:25,859 --> 0:30:26,860
可以通过使用锚点骨架属性来完成

494
00:30:28,295 --> 0:30:31,265
当你有了这个几何体

495
00:30:31,331 --> 0:30:32,466
你需要所有的节点

496
00:30:32,799 --> 0:30:35,836
来获取所有关节点转换的列表

497
00:30:36,170 --> 0:30:39,506
就是所有关节点的位置列表

498
00:30:39,573 --> 0:30:42,376
jointModelTransforms

499
00:30:43,310 --> 0:30:48,248
在这种情况下 当你有了列表之后

500
00:30:48,315 --> 0:30:51,151
以及访问每个元素或

501
00:30:52,186 --> 0:30:57,024
所以遍历所有的关节点

502
00:30:57,090 --> 0:31:00,661
parentIndices

503
00:30:57,090 --> 0:31:00,661
parentIndices

504
00:31:00,727 --> 0:31:01,828
每个关节点的

505
00:31:02,296 --> 0:31:05,098
只需检查父节点是否为根节点

506
00:31:05,165 --> 0:31:07,968
使整个层级中最顶部的点

507
00:31:08,702 --> 0:31:14,441
所以当父节点不是根节点时 你可以

508
00:31:14,675 --> 0:31:20,547
列表访问父节点的转换 但需要使用

509
00:31:21,114 --> 0:31:22,115
就是这些了

510
00:31:22,316 --> 0:31:26,653
它为你提供整个层级中对应的

511
00:31:26,820 --> 0:31:30,891
当你在层级中有了每个孩子和父亲

512
00:31:30,958 --> 0:31:32,860
你就有了骨架的整个层级

513
00:31:34,194 --> 0:31:36,263
你现在可以随时使用它

514
00:31:36,897 --> 0:31:38,365
那么 我们来运行这段代码

515
00:31:38,432 --> 0:31:43,170
使用它 我们来简单画一下这个骨架

516
00:31:44,905 --> 0:31:46,206
这是它的样子

517
00:31:46,640 --> 0:31:49,209
我们所做的是

518
00:31:49,276 --> 0:31:53,380
我们获取了所有的父亲和孩子的点

519
00:31:53,914 --> 0:31:56,183
它开始自动地模仿人了

520
00:31:56,783 --> 0:32:00,387
这是你能做的最基本的事情

521
00:31:56,783 --> 0:32:00,387
这是你能做的最基本的事情

522
00:32:00,454 --> 0:32:04,358
整个骨架层级

523
00:32:04,424 --> 0:32:05,626
就如我们之前讲的那样

524
00:32:06,360 --> 0:32:10,430
无论你的想象力把你带到哪里

525
00:32:14,368 --> 0:32:19,206
到现在为止 我们只了解了

526
00:32:19,840 --> 0:32:24,344
但 如果你想要平面空间的

527
00:32:24,511 --> 0:32:27,314
我们也有一个API能实现

528
00:32:28,115 --> 0:32:33,720
这里 我们为你提供了在二维空间

529
00:32:35,355 --> 0:32:39,993
我们也在一般的平面空间

530
00:32:41,361 --> 0:32:44,164
使用这个API同样非常简单

531
00:32:45,766 --> 0:32:48,235
你可以使用它做语义图片分析

532
00:32:48,302 --> 0:32:51,839
或者为图片和视屏创建编辑工具

533
00:32:53,774 --> 0:32:57,511
最后 整个结构都连接了一个物体

534
00:32:57,711 --> 0:32:59,713
叫做ARBody2D

535
00:33:00,881 --> 0:33:04,985
这就是视觉上ARBody2D

536
00:33:05,719 --> 0:33:09,790
ARBody2D物件包含了整个

537
00:33:13,861 --> 0:33:15,495
这就是结构的样子

538
00:33:16,129 --> 0:33:18,599
所以你在上面有了这个物件本身

539
00:33:18,799 --> 0:33:22,469
接着 还是和3D响应的

540
00:33:22,536 --> 0:33:23,537
都在物件的下面

541
00:33:24,571 --> 0:33:25,873
我们来看一下这个结构

542
00:33:25,939 --> 0:33:27,941
快速了解这些元素

543
00:33:29,543 --> 0:33:32,012
从上面的ARBody2D物件开始

544
00:33:33,547 --> 0:33:36,016
你有两种访问这个物件的方式

545
00:33:36,850 --> 0:33:40,487
如果你已经在二维空间了

546
00:33:40,554 --> 0:33:41,688
vARFrame

547
00:33:41,955 --> 0:33:47,194
你可以使用ARFrame的

548
00:33:48,495 --> 0:33:51,999
这里 这个人是ARBody2D物件

549
00:33:53,734 --> 0:33:57,204
为了你使用方便

550
00:33:57,404 --> 0:34:01,008
因为某些原因

551
00:33:57,404 --> 0:34:01,008
因为某些原因

552
00:34:01,074 --> 0:34:04,945
你也想要二维空间对应的2D骨架

553
00:34:05,779 --> 0:34:11,185
通过使用ARBodyAnchor的

554
00:34:11,351 --> 0:34:12,585
我们为你提供了一个

555
00:34:13,719 --> 0:34:16,523
它也会为你返回

556
00:34:18,725 --> 0:34:22,862
访问完ARBody2D物件时

557
00:34:22,929 --> 0:34:24,864
从中取出骨架

558
00:34:26,099 --> 0:34:28,902
这就是那个骨架的视觉图

559
00:34:30,070 --> 0:34:34,107
就像我说过的

560
00:34:34,608 --> 0:34:38,745
所以 如果这是你的图像网格

561
00:34:38,812 --> 0:34:40,447
右下是1,1

562
00:34:40,514 --> 0:34:45,018
示例图中的所有位置坐标都

563
00:34:45,085 --> 0:34:47,087
x和y方向都是

564
00:34:49,956 --> 0:34:52,926
你看到的绿色的点被称为标记点

565
00:34:53,293 --> 0:34:56,730
请注意 我们在这里并不称他们为

566
00:34:56,797 --> 0:35:00,200
我们称它们为标记点的原因是

567
00:34:56,797 --> 0:35:00,200
我们称它们为标记点的原因是

568
00:35:03,103 --> 0:35:07,241
和3D版本一样 它包含一个

569
00:35:07,307 --> 0:35:10,577
描述哪一个标记点在骨架中被调用

570
00:35:10,644 --> 0:35:12,546
以及如何连接这些标记点

571
00:35:14,348 --> 0:35:16,750
在这个骨架中 有16个关节点

572
00:35:17,017 --> 0:35:20,621
和3D类似 它们具有语义上有意义

573
00:35:20,687 --> 0:35:24,892
比如左肩 右肩 左手和右手等等

574
00:35:25,192 --> 0:35:29,263
根节点还在髋关节这里

575
00:35:31,965 --> 0:35:34,568
那么 请关注右手

576
00:35:34,635 --> 0:35:37,437
我们看到这只手是右胳膊肘

577
00:35:37,504 --> 0:35:39,573
胳膊肘是右肩的一个孩子

578
00:35:39,806 --> 0:35:43,010
再一次 这和你在3D版本中看到的

579
00:35:43,076 --> 0:35:45,412
父子关系类似

580
00:35:46,079 --> 0:35:48,682
这个类似的层级关系在这里构成

581
00:35:50,951 --> 0:35:54,988
那么 了解了所有的信息

582
00:35:55,055 --> 0:35:56,056
了解一下这个结构

583
00:35:57,057 --> 0:35:59,826
我们从访问ARBody2D物件开始

584
00:36:01,695 --> 0:36:06,066
所以 只要你有了ARFRame

585
00:36:06,133 --> 0:36:07,701
来获取ARBody2D物件

586
00:36:08,168 --> 0:36:10,737
现在 只要你有了ARBody2D物件

587
00:36:10,804 --> 0:36:14,141
你就能访问整个内部的骨架结构

588
00:36:14,842 --> 0:36:18,111
你可以通过使用person.skeleton

589
00:36:18,979 --> 0:36:22,182
现在 person指向了ARBody2D

590
00:36:22,950 --> 0:36:26,987
骨架的definition再一次包含了

591
00:36:27,054 --> 0:36:31,058
和如何连接这些关节点的信息

592
00:36:31,124 --> 0:36:34,761
它在definition中被展示

593
00:36:34,828 --> 0:36:36,830
definition属性来访问它

594
00:36:38,365 --> 0:36:40,000
只要你有了这些信息

595
00:36:40,167 --> 0:36:43,604
你可能需要知道这些标记点

596
00:36:43,871 --> 0:36:47,641
和3D版本类似 我们有一个叫做

597
00:36:47,708 --> 0:36:52,646
它提供了你看到的所有绿色点的

598
00:36:53,547 --> 0:36:56,383
但请注意 这里的绿色的点都是

599
00:36:56,450 --> 0:36:57,718
所以它们处于二维空间

600
00:36:57,784 --> 0:37:00,621
它们是标准的像素坐标

601
00:36:57,784 --> 0:37:00,621
它们是标准的像素坐标

602
00:37:01,688 --> 0:37:05,225
只要你有了那个列表

603
00:37:05,692 --> 0:37:08,428
对于每一个标记点来说

604
00:37:08,795 --> 0:37:11,932
parentIndices属性

605
00:37:12,399 --> 0:37:15,269
再一次 只需检查它的父节点是否为

606
00:37:15,335 --> 0:37:17,704
因为根节点处于点的层级中的最顶层

607
00:37:18,205 --> 0:37:23,010
如果父节点不是根节点 你可以通过

608
00:37:23,076 --> 0:37:26,547
列表访问它的转换

609
00:37:27,314 --> 0:37:29,349
这种方式下

610
00:37:29,583 --> 0:37:34,721
你在骨架层级中

611
00:37:34,788 --> 0:37:36,089
有了每个父子对的转换

612
00:37:36,390 --> 0:37:38,692
如果你想的话

613
00:37:38,759 --> 0:37:41,461
你可以在这里继续完成你想做的事

614
00:37:42,863 --> 0:37:44,731
我们来总结一下

615
00:37:47,668 --> 0:37:51,371
我们介绍了今年AR的动作捕捉

616
00:37:52,873 --> 0:37:56,844
我们提供了实时追踪人的访问

617
00:37:58,912 --> 0:38:04,952
我们同时提供了3D和2D骨架

618
00:37:58,912 --> 0:38:04,952
我们同时提供了3D和2D骨架

619
00:38:05,886 --> 0:38:08,755
这就是我们连接人体姿势的方式

620
00:38:10,591 --> 0:38:13,861
我们支持了立刻同步的角色动画

621
00:38:14,661 --> 0:38:16,964
它在RealityKit中

622
00:38:18,498 --> 0:38:20,367
我们有了一个之前讨论过的

623
00:38:20,634 --> 0:38:24,004
来快速让角色动起来

624
00:38:24,338 --> 0:38:27,808
就像我之前提到的

625
00:38:27,875 --> 0:38:32,412
只要它是基于我们提供的示例结构

626
00:38:34,047 --> 0:38:38,118
我们介绍了一个你可能会想到的

627
00:38:38,185 --> 0:38:42,689
的高阶用法的ARKit API

628
00:38:46,260 --> 0:38:48,061
今天的演讲就要结束了

629
00:38:48,629 --> 0:38:51,198
我们介绍了两个功能

630
00:38:51,431 --> 0:38:53,433
真人遮挡剔除和动作捕捉

631
00:38:53,700 --> 0:38:57,838
我们为这些功能都提供了API

632
00:38:59,072 --> 0:39:03,110
了解今天演讲的更多信息

633
00:38:59,072 --> 0:39:03,110
了解今天演讲的更多信息

634
00:39:03,343 --> 0:39:06,513
请随意下载示例代码并使用它

635
00:39:07,314 --> 0:39:08,782
我们明天会在实验室

636
00:39:08,849 --> 0:39:11,418
你可以带着你的问题来找我们

637
00:39:15,522 --> 0:39:16,523
谢谢
